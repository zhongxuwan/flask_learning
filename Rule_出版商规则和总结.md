# Rule å‡ºç‰ˆå•†è§„åˆ™å’Œæ€»ç»“ï¼ˆè¡¥å……æ–‡æ¡£3ï¼‰

æœ¬æ–‡æ¡£æ˜¯ `Rule_HTMLè§£æè§„åˆ™è¯¦è§£.md` ç³»åˆ—çš„æœ€åä¸€éƒ¨åˆ†ï¼Œè®²è§£å‰©ä½™ä¸‰ä¸ªå‡ºç‰ˆå•†è§„åˆ™å’ŒæŠ€æœ¯æ€»ç»“ã€‚

---

## ğŸ“š å‰©ä½™å‡ºç‰ˆå•†è§£æè§„åˆ™

### 4. Springerè§„åˆ™ï¼ˆç¬¬198-280è¡Œï¼‰â­â­â­â­â­

Springerè§„åˆ™æ˜¯**æœ€å¤æ‚**çš„è§„åˆ™ï¼Œæœ‰æœ€å¤šçš„é€‰æ‹©å™¨å’Œå¤šç§åå¤‡æ–¹æ¡ˆã€‚

```python
@classmethod
def springer_rule(cls, html: str) -> Optional[str]:
    """Springer æ‘˜è¦è§£æè§„åˆ™"""
    try:
        soup = BeautifulSoup(html, 'lxml')
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨
        selectors = [
            "#Abs1-content",                          # æ ‡å‡†æ‘˜è¦é€‰æ‹©å™¨
            "section[data-title='Abstract']",         # æ–°ç‰ˆå¸ƒå±€
            "div[data-test='abstract-content']",      # æµ‹è¯•å±æ€§
            "div.c-article-section__content",         # æ–‡ç« éƒ¨åˆ†å†…å®¹
            'meta[name="description"]',
            'meta[name="citation_abstract"]',
            'meta[property="og:description"]',
            "div#abstract-content",
            "div.abstract-content",
            "section.Abstract",
            "div[class*='abstract']",
            "p.Para",                                 # æ®µè½æ–‡æœ¬
            "div.c-article-body",                     # æ–‡ç« ä¸»ä½“
            "div.article-body"
        ]
```

**Springeré€‰æ‹©å™¨è¯¦è§£**ï¼š

```python
# Springerç‰¹æœ‰é€‰æ‹©å™¨ï¼š

# "#Abs1-content"
# Springerçš„æ ‡å‡†æ‘˜è¦ID
# HTMLç¤ºä¾‹ï¼š
<div id="Abs1-content">
    <p>This paper...</p>
</div>

# "section[data-title='Abstract']"
# ä½¿ç”¨data-titleå±æ€§
# HTMLç¤ºä¾‹ï¼š
<section data-title="Abstract" class="Abs1">
    <div class="Abs1-content">...</div>
</section>

# å±æ€§é€‰æ‹©å™¨è¯­æ³•ï¼š
[data-title='Abstract']  # ç²¾ç¡®åŒ¹é…
[data-test]              # æœ‰æ­¤å±æ€§å³å¯
[class*='abstract']      # åŒ…å«"abstract"

# "div[data-test='abstract-content']"
# æµ‹è¯•å±æ€§ï¼ˆæ–°ç‰ˆSpringerä½¿ç”¨ï¼‰
# HTMLç¤ºä¾‹ï¼š
<div data-test="abstract-content">...</div>

# "div.c-article-section__content"
# BEMå‘½åè§„èŒƒçš„ç±»
# cï¼šç»„ä»¶ï¼ˆcomponentï¼‰
# article-sectionï¼šå—ï¼ˆblockï¼‰
# contentï¼šå…ƒç´ ï¼ˆelementï¼‰

# "p.Para"
# Springerçš„æ®µè½ç±»
# HTMLç¤ºä¾‹ï¼š
<p class="Para">Abstract content...</p>
```

### Springerï¼šéå†å’Œå¤„ç†ï¼ˆç¬¬222-245è¡Œï¼‰

```python
# éå†æ‰€æœ‰é€‰æ‹©å™¨                             # ç¬¬222è¡Œï¼ˆæ³¨é‡Šï¼‰
for selector in selectors:                  # ç¬¬223è¡Œ
    element = soup.select_one(selector)     # ç¬¬224è¡Œ
    if element:                             # ç¬¬225è¡Œ
        # å¯¹äºmetaæ ‡ç­¾                       # ç¬¬226è¡Œï¼ˆæ³¨é‡Šï¼‰
        if element.name == 'meta':          # ç¬¬227è¡Œ
            content = element.get('content', '').strip()  # ç¬¬228è¡Œ
            if content:                     # ç¬¬229è¡Œ
                return content              # ç¬¬230è¡Œ
        # å¯¹äºå…¶ä»–æ ‡ç­¾                       # ç¬¬231è¡Œï¼ˆæ³¨é‡Šï¼‰
        else:                               # ç¬¬232è¡Œ
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´                # ç¬¬233è¡Œï¼ˆæ³¨é‡Šï¼‰
            for unwanted in element.find_all(['button', 'script', 'style']):  # ç¬¬234è¡Œ
                unwanted.decompose()        # ç¬¬235è¡Œ
            
            # è·å–æ–‡æœ¬                       # ç¬¬237è¡Œï¼ˆæ³¨é‡Šï¼‰
            text = element.get_text().strip()  # ç¬¬238è¡Œ
            if text:                        # ç¬¬239è¡Œ
                # æ¸…ç†æ–‡æœ¬                   # ç¬¬240è¡Œï¼ˆæ³¨é‡Šï¼‰
                text = ' '.join(text.split())  # è§„èŒƒåŒ–ç©ºç™½  # ç¬¬241è¡Œ
                text = re.sub(r'^abstract\s*:?\s*', '', text, flags=re.IGNORECASE)  # ç§»é™¤å¼€å¤´çš„"Abstract"å­—æ ·  # ç¬¬242è¡Œ
                text = text.strip()         # ç¬¬243è¡Œ
                if text and len(text) > 50:  # ç¬¬244è¡Œ
                    return text             # ç¬¬245è¡Œ
```

**Springeræ–‡æœ¬æ¸…ç†æµç¨‹**ï¼š

```python
# Springerçš„æ–‡æœ¬æ¸…ç†æ˜¯æœ€å½»åº•çš„

# 1. è§„èŒƒåŒ–ç©ºç™½
text = ' '.join(text.split())

# è¾“å…¥ï¼š
"""
Abstract:
    This  paper
presents...
"""
# è¾“å‡ºï¼š"Abstract: This paper presents..."

# 2. ç§»é™¤"Abstract"å‰ç¼€
text = re.sub(r'^abstract\s*:?\s*', '', text, flags=re.IGNORECASE)

# è¾“å…¥ï¼š"Abstract: This paper..."
# è¾“å‡ºï¼š"This paper..."

# 3. å†æ¬¡å»é™¤ç©ºç™½
text = text.strip()

# 4. éªŒè¯é•¿åº¦
if text and len(text) > 50:
    return text

# å®Œæ•´ç¤ºä¾‹ï¼š
# åŸå§‹ï¼š
"""
<div class="abstract">
    Abstract:
    
    This    paper
    presents...
</div>
"""

# ç»è¿‡æ¸…ç†ï¼š
"This paper presents..."
```

### Springerï¼šJSON-LDå’Œåå¤‡æ–¹æ¡ˆï¼ˆç¬¬247-276è¡Œï¼‰

```python
# å°è¯•ä½¿ç”¨JSON-LDæ•°æ®                       # ç¬¬247è¡Œï¼ˆæ³¨é‡Šï¼‰
script_tags = soup.find_all('script', type='application/ld+json')  # ç¬¬248è¡Œ
for script in script_tags:                  # ç¬¬249è¡Œ
    try:                                    # ç¬¬250è¡Œ
        data = json.loads(script.string)    # ç¬¬251è¡Œ
        if isinstance(data, dict) and 'description' in data:  # ç¬¬252è¡Œ
            abstract = data['description'].strip()  # ç¬¬253è¡Œ
            if abstract and len(abstract) > 50:  # ç¬¬254è¡Œ
                return abstract             # ç¬¬255è¡Œ
        elif isinstance(data, dict) and '@graph' in data:  # ç¬¬256è¡Œ
            for item in data['@graph']:     # ç¬¬257è¡Œ
                if isinstance(item, dict) and 'description' in item:  # ç¬¬258è¡Œ
                    abstract = item['description'].strip()  # ç¬¬259è¡Œ
                    if abstract and len(abstract) > 50:  # ç¬¬260è¡Œ
                        return abstract     # ç¬¬261è¡Œ
    except:                                 # ç¬¬262è¡Œ
        continue                            # ç¬¬263è¡Œ

# å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«"Abstract"çš„æ®µè½  # ç¬¬265è¡Œï¼ˆæ³¨é‡Šï¼‰
abstract_section = None                     # ç¬¬266è¡Œ
for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):  # ç¬¬267è¡Œ
    if 'abstract' in heading.get_text().lower():  # ç¬¬268è¡Œ
        abstract_section = heading.find_next('p')  # ç¬¬269è¡Œ
        break                               # ç¬¬270è¡Œ

if abstract_section:                        # ç¬¬272è¡Œ
    text = abstract_section.get_text().strip()  # ç¬¬273è¡Œ
    if text and len(text) > 50:             # ç¬¬274è¡Œ
        return text                         # ç¬¬275è¡Œ

return None                                 # ç¬¬277è¡Œ
```

**Springeré«˜çº§ç‰¹æ€§**ï¼š

```python
# 1. JSON-LD @graphå¤„ç†
elif isinstance(data, dict) and '@graph' in data:
    for item in data['@graph']:

# @graphè¯¦è§£ï¼š
# JSON-LDä¸­çš„å›¾ç»“æ„
# åŒ…å«å¤šä¸ªèŠ‚ç‚¹

# ç¤ºä¾‹ï¼š
{
    "@context": "https://schema.org",
    "@graph": [
        {
            "@type": "ScholarlyArticle",
            "description": "This paper..."
        },
        {
            "@type": "Person",
            "name": "John Doe"
        }
    ]
}

# éå†æ‰€æœ‰èŠ‚ç‚¹ï¼ŒæŸ¥æ‰¾description

# 2. æ ‡é¢˜åå¤‡æ–¹æ¡ˆ â­â­â­â­
# å¦‚æœæ‰€æœ‰é€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾"Abstract"æ ‡é¢˜

for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):
    if 'abstract' in heading.get_text().lower():
        abstract_section = heading.find_next('p')
        break

# æŸ¥æ‰¾æ‰€æœ‰æ ‡é¢˜æ ‡ç­¾
# soup.find_all(['h1', 'h2', 'h3', 'h4'])
# è¿”å›æ‰€æœ‰h1ã€h2ã€h3ã€h4æ ‡ç­¾

# æ£€æŸ¥æ ‡é¢˜æ–‡æœ¬
if 'abstract' in heading.get_text().lower():

# heading.get_text()ï¼šè·å–æ ‡é¢˜æ–‡æœ¬
# .lower()ï¼šè½¬å°å†™
# 'abstract' in ...ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«"abstract"

# æŸ¥æ‰¾æ ‡é¢˜åçš„ç¬¬ä¸€ä¸ªæ®µè½
abstract_section = heading.find_next('p')

# find_next('p')ï¼š
# æŸ¥æ‰¾å½“å‰å…ƒç´ åçš„ç¬¬ä¸€ä¸ª<p>æ ‡ç­¾

# HTMLç¤ºä¾‹ï¼š
<h2>Abstract</h2>
<p>This paper presents...</p>
<p>Second paragraph...</p>

# heading.find_next('p')
# è¿”å›ç¬¬ä¸€ä¸ª<p>ï¼ˆ"This paper presents..."ï¼‰

# breakï¼š
# æ‰¾åˆ°åç«‹å³åœæ­¢ï¼Œä¸ç»§ç»­æŸ¥æ‰¾
```

---

### 5. Elsevierè§„åˆ™ï¼ˆç¬¬282-349è¡Œï¼‰

```python
@classmethod
def elsevier_rule(cls, html: str) -> Optional[str]:
    """Elsevier (ScienceDirect) æ‘˜è¦è§£æè§„åˆ™"""
    try:
        soup = BeautifulSoup(html, 'html.parser')  # æ³¨æ„ï¼šä½¿ç”¨html.parser
        
        selectors = [
            "div.abstract.author",          # ä¼ ç»Ÿå¸ƒå±€
            "div.abstract",
            "div#abstracts",                # å¤æ•°å½¢å¼
            "div#abstract",
            "section.abstract",
            'meta[name="description"]',
            'meta[name="citation_abstract"]',
            'meta[property="og:description"]',
            "div.Abstracts",
            "div[class*='abstract']",
            "#section-abstract"
        ]
```

**Elsevierç‰¹ç‚¹**ï¼š

```python
# 1. ä½¿ç”¨html.parser
soup = BeautifulSoup(html, 'html.parser')

# ä¸ºä»€ä¹ˆä¸ç”¨lxmlï¼Ÿ
# Elsevierçš„HTMLå¯èƒ½ä¸å®Œå…¨è§„èŒƒ
# html.parseræ›´å®½å®¹

# 2. Elsevierç‰¹æœ‰çš„æ®µè½å¤„ç†
if element:
    if element.name == 'meta':
        abstract = element.get('content', '').strip()
    else:
        # é¦–å…ˆå°è¯•ä»æ®µè½ä¸­æå–æ‘˜è¦
        paragraphs = element.find_all('p')
        if paragraphs:
            abstract_parts = []
            for p in paragraphs:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in p.find_all(['script', 'style', 'button']):
                    unwanted.decompose()
                text = p.get_text().strip()
                if text:
                    abstract_parts.append(text)
            abstract = ' '.join(abstract_parts)
```

**æ®µè½æ‹¼æ¥è¯¦è§£**ï¼š

```python
# ä¸ºä»€ä¹ˆè¦åˆ†æ®µè½å¤„ç†ï¼Ÿ
# Elsevierçš„æ‘˜è¦å¯èƒ½åˆ†å¤šä¸ªæ®µè½

# HTMLç¤ºä¾‹ï¼š
<div class="abstract">
    <p>Background: This paper...</p>
    <p>Methods: We propose...</p>
    <p>Results: The experiments...</p>
</div>

# å¤„ç†æµç¨‹ï¼š

# 1. æŸ¥æ‰¾æ‰€æœ‰æ®µè½
paragraphs = element.find_all('p')
# [<p>Background...</p>, <p>Methods...</p>, <p>Results...</p>]

# 2. åˆå§‹åŒ–åˆ—è¡¨
abstract_parts = []

# 3. éå†æ¯ä¸ªæ®µè½
for p in paragraphs:
    # æ¸…ç†æ®µè½
    for unwanted in p.find_all(['script', 'style', 'button']):
        unwanted.decompose()
    
    # æå–æ–‡æœ¬
    text = p.get_text().strip()
    
    # æ·»åŠ åˆ°åˆ—è¡¨
    if text:
        abstract_parts.append(text)

# abstract_parts = [
#     "Background: This paper...",
#     "Methods: We propose...",
#     "Results: The experiments..."
# ]

# 4. æ‹¼æ¥æ®µè½
abstract = ' '.join(abstract_parts)
# "Background: This paper... Methods: We propose... Results: The experiments..."

# ä¸ºä»€ä¹ˆç”¨ç©ºæ ¼æ‹¼æ¥ï¼Ÿ
# ä¿æŒæ®µè½ä¹‹é—´çš„åˆ†éš”
# é¿å…æ–‡å­—è¿åœ¨ä¸€èµ·
```

---

### 6. RFC Editorè§„åˆ™ï¼ˆç¬¬351-404è¡Œï¼‰â­â­â­â­

RFC Editorè§„åˆ™æ˜¯**æœ€ç‰¹æ®Š**çš„ï¼Œå› ä¸ºRFCæ–‡æ¡£çš„ç»“æ„éå¸¸è§„èŒƒã€‚

```python
@classmethod
def rfc_editor_rule(cls, html: str) -> Optional[str]:
    """RFC Editor æ‘˜è¦è§£æè§„åˆ™"""
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾Abstractæ ‡é¢˜ - è€ƒè™‘å¤šç§å¯èƒ½çš„æ ‡ç­¾å’Œå¤§å°å†™
        abstract_heading = soup.find('h2', string=lambda s: s and s.lower() == 'abstract')
        
        if not abstract_heading:
            # å°è¯•å…¶ä»–å¯èƒ½çš„æ ‡é¢˜æ ¼å¼
            for tag in ['h1', 'h3', 'strong', 'b']:
                abstract_heading = soup.find(tag, string=lambda s: s and s.lower() == 'abstract')
                if abstract_heading:
                    break
        
        if not abstract_heading:
            return None
        
        # æ”¶é›†æ ‡é¢˜åçš„æ‰€æœ‰æ®µè½ï¼Œç›´åˆ°é‡åˆ°æ°´å¹³çº¿æˆ–ä¸‹ä¸€ä¸ªæ ‡é¢˜
        paragraphs: List[str] = []
        current = abstract_heading.next_sibling
        
        while current:
            # é‡åˆ°æ°´å¹³çº¿æˆ–æ–°æ ‡é¢˜æ—¶åœæ­¢
            if current.name == 'hr' or current.name in ['h1', 'h2', 'h3', 'h4']:
                break
            
            # æ”¶é›†æ®µè½æ–‡æœ¬
            if current.name == 'p':
                text = current.get_text().strip()
                if text:
                    paragraphs.append(text)
            
            current = current.next_sibling
```

**RFC Editorç‰¹æ®Šå¤„ç†**ï¼š

```python
# 1. lambdaå‡½æ•°æŸ¥æ‰¾æ ‡é¢˜
abstract_heading = soup.find('h2', string=lambda s: s and s.lower() == 'abstract')

# lambdaè¯¦è§£ â­â­â­â­â­
# lambdaï¼šåŒ¿åå‡½æ•°

# å®Œæ•´å½¢å¼ï¼š
def check_abstract(s):
    return s and s.lower() == 'abstract'

abstract_heading = soup.find('h2', string=check_abstract)

# lambdaç®€åŒ–å½¢å¼ï¼š
lambda s: s and s.lower() == 'abstract'

# å‚æ•°ï¼šsï¼ˆæ ‡é¢˜æ–‡æœ¬ï¼‰
# é€»è¾‘ï¼š
# - sï¼šç¡®ä¿ä¸æ˜¯None
# - s.lower() == 'abstract'ï¼šå°å†™åç­‰äº'abstract'

# åŒ¹é…ç¤ºä¾‹ï¼š
"Abstract"   â†’ True
"ABSTRACT"   â†’ True
"abstract"   â†’ True
"Summary"    â†’ False

# 2. å°è¯•å¤šç§æ ‡é¢˜æ ‡ç­¾
for tag in ['h1', 'h3', 'strong', 'b']:
    abstract_heading = soup.find(tag, string=lambda s: s and s.lower() == 'abstract')
    if abstract_heading:
        break

# RFCæ–‡æ¡£å¯èƒ½ä½¿ç”¨ä¸åŒæ ‡ç­¾ä½œä¸ºæ ‡é¢˜ï¼š
<h2>Abstract</h2>  # æœ€å¸¸è§
<h1>Abstract</h1>  # æœ‰æ—¶ç”¨h1
<strong>Abstract</strong>  # æ—§ç‰ˆRFC
<b>Abstract</b>    # æ›´è€çš„ç‰ˆæœ¬

# 3. éå†å…„å¼Ÿå…ƒç´  â­â­â­â­â­
current = abstract_heading.next_sibling

while current:
    # å¤„ç†å½“å‰å…ƒç´ 
    current = current.next_sibling

# next_siblingè¯¦è§£ï¼š
# åŠŸèƒ½ï¼šè·å–ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 

# HTMLç»“æ„ï¼š
<h2>Abstract</h2>          â† abstract_heading
<p>This RFC presents...</p> â† next_sibling (1)
<p>The protocol...</p>      â† next_sibling (2)
<hr>                        â† next_sibling (3)ï¼Œåœæ­¢

# éå†ç¤ºä¾‹ï¼š
current = abstract_heading.next_sibling  # <p>This RFC...</p>

# ç¬¬1æ¬¡å¾ªç¯
current = current.next_sibling  # <p>The protocol...</p>

# ç¬¬2æ¬¡å¾ªç¯
current = current.next_sibling  # <hr>
if current.name == 'hr':
    break  # åœæ­¢

# 4. åœæ­¢æ¡ä»¶
if current.name == 'hr' or current.name in ['h1', 'h2', 'h3', 'h4']:
    break

# é‡åˆ°ä»¥ä¸‹æƒ…å†µåœæ­¢ï¼š
# - æ°´å¹³çº¿<hr>
# - æ–°çš„æ ‡é¢˜ï¼ˆh1, h2, h3, h4ï¼‰

# 5. æ”¶é›†æ®µè½
if current.name == 'p':
    text = current.get_text().strip()
    if text:
        paragraphs.append(text)

# current.nameè¯¦è§£ï¼š
# è·å–å…ƒç´ çš„æ ‡ç­¾å
<p> â†’ 'p'
<div> â†’ 'div'
æ–‡æœ¬èŠ‚ç‚¹ â†’ None

# 6. åå¤‡æ–¹æ¡ˆ
if not paragraphs:
    # å°è¯•æŸ¥æ‰¾åŒ…å«abstractåå†…å®¹çš„div
    abstract_section = soup.find('div', class_=lambda c: c and 'abstract' in c.lower())
    if abstract_section:
        for p in abstract_section.find_all('p'):
            text = p.get_text().strip()
            if text:
                paragraphs.append(text)

# 7. åˆå¹¶æ®µè½
if paragraphs:
    return ' '.join(paragraphs)
```

---

## ğŸ¯ æŠ€æœ¯æ€»ç»“

### æ ¸å¿ƒæŠ€æœ¯æ ˆ

| æŠ€æœ¯ | ç”¨é€” | å…³é”®ä»£ç  | é‡è¦ç¨‹åº¦ |
|------|------|---------|---------|
| **BeautifulSoup** | HTMLè§£æ | `soup.select_one()` | â­â­â­â­â­ |
| **CSSé€‰æ‹©å™¨** | å…ƒç´ å®šä½ | `'div.abstract'` | â­â­â­â­â­ |
| **æ­£åˆ™è¡¨è¾¾å¼** | æ–‡æœ¬æ¸…ç† | `re.sub()` | â­â­â­â­ |
| **JSONå¤„ç†** | ç»“æ„åŒ–æ•°æ® | `json.loads()` | â­â­â­â­ |
| **requests** | HTTPè¯·æ±‚ | `Session()` | â­â­â­â­ |
| **ç±»å‹æ³¨è§£** | ä»£ç è§„èŒƒ | `Optional[str]` | â­â­â­â­ |
| **å¼‚å¸¸å¤„ç†** | å®¹é”™æœºåˆ¶ | `try-except` | â­â­â­â­â­ |

### è®¾è®¡æ¨¡å¼

#### 1. ä¼˜å…ˆçº§æ¨¡å¼ï¼ˆPriority Patternï¼‰

```python
# æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ªæ–¹æ¡ˆ
selectors = [
    'meta[property="twitter:description"]',  # ä¼˜å…ˆçº§1
    'meta[name="description"]',              # ä¼˜å…ˆçº§2
    'div.abstract',                          # ä¼˜å…ˆçº§3
    # ...
]

for selector in selectors:
    result = try_extract(selector)
    if result:
        return result  # æ‰¾åˆ°å³è¿”å›
```

**ä¼˜åŠ¿**ï¼š
- âœ“ æé«˜æˆåŠŸç‡
- âœ“ ä¼˜å…ˆä½¿ç”¨æœ€å¯é çš„æ–¹æ¡ˆ
- âœ“ è‡ªåŠ¨é™çº§

#### 2. é˜²å¾¡å¼ç¼–ç¨‹ï¼ˆDefensive Programmingï¼‰

```python
# å¤šå±‚é˜²å¾¡
try:
    element = soup.select_one(selector)
    if not element:  # é˜²å¾¡1ï¼šæ£€æŸ¥None
        return None
    
    if element.name == 'meta':
        content = element.get('content', '')  # é˜²å¾¡2ï¼šé»˜è®¤å€¼
    else:
        for unwanted in element.find_all(['script']):
            unwanted.decompose()  # é˜²å¾¡3ï¼šæ¸…ç†
        text = element.get_text().strip()
    
    if text and len(text) > 50:  # é˜²å¾¡4ï¼šéªŒè¯
        return text
except Exception:  # é˜²å¾¡5ï¼šå¼‚å¸¸æ•è·
    return None
```

**ä¼˜åŠ¿**ï¼š
- âœ“ æ°¸ä¸å´©æºƒ
- âœ“ ä¼˜é›…é™çº§
- âœ“ æé«˜ç¨³å®šæ€§

#### 3. å·¥å‚æ¨¡å¼å˜ç§

```python
# æ ¹æ®æ¥æºé€‰æ‹©è§„åˆ™
def get_abstract(html: str, source: str) -> Optional[str]:
    rules = {
        'ieee': Rule.ieee_rule,
        'acm': Rule.acm_rule,
        'arxiv': Rule.arxiv_rule,
        # ...
    }
    
    rule_func = rules.get(source)
    if rule_func:
        return rule_func(html)
    return None
```

### æœ€ä½³å®è·µ

#### 1. CSSé€‰æ‹©å™¨ä¼˜å…ˆçº§ â­â­â­â­â­

```python
# æ¨èé¡ºåºï¼š
1. Metaæ ‡ç­¾ï¼ˆæœ€å¯é ï¼‰
   'meta[name="description"]'

2. IDé€‰æ‹©å™¨ï¼ˆå”¯ä¸€æ€§ï¼‰
   '#abstract'

3. ç±»é€‰æ‹©å™¨ï¼ˆå¸¸ç”¨ï¼‰
   'div.abstract'

4. å±æ€§é€‰æ‹©å™¨ï¼ˆçµæ´»ï¼‰
   'div[class*="abstract"]'

5. å±‚çº§é€‰æ‹©å™¨ï¼ˆç²¾ç¡®ï¼‰
   'div.content > p.abstract'
```

#### 2. æ–‡æœ¬æ¸…ç†æµç¨‹ â­â­â­â­â­

```python
# æ ‡å‡†æ¸…ç†æµç¨‹ï¼š
1. ç§»é™¤æ— ç”¨å…ƒç´ 
   for unwanted in element.find_all(['script', 'style', 'button']):
       unwanted.decompose()

2. æå–æ–‡æœ¬
   text = element.get_text()

3. å»é™¤ç©ºç™½
   text = text.strip()

4. è§„èŒƒåŒ–ç©ºç™½
   text = ' '.join(text.split())

5. ç§»é™¤å‰ç¼€
   text = re.sub(r'^abstract\s*:?\s*', '', text, flags=re.IGNORECASE)

6. éªŒè¯é•¿åº¦
   if len(text) > 50:
       return text
```

#### 3. å¼‚å¸¸å¤„ç†ç­–ç•¥ â­â­â­â­â­

```python
# Level 1ï¼šæ•´ä½“å¼‚å¸¸å¤„ç†
def ieee_rule(cls, html: str) -> Optional[str]:
    try:
        # æ‰€æœ‰é€»è¾‘
    except Exception as e:
        return None

# Level 2ï¼šå±€éƒ¨å¼‚å¸¸å¤„ç†
for script in script_tags:
    try:
        data = json.loads(script.string)
    except:
        continue  # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
```

### æ€§èƒ½ä¼˜åŒ–

#### 1. è§£æå™¨é€‰æ‹©

```python
# lxmlï¼ˆæ¨èï¼‰ï¼š
soup = BeautifulSoup(html, 'lxml')
# ä¼˜ç‚¹ï¼šé€Ÿåº¦å¿«ã€å®¹é”™å¥½
# ç¼ºç‚¹ï¼šéœ€è¦å®‰è£…

# html.parserï¼ˆå¤‡é€‰ï¼‰ï¼š
soup = BeautifulSoup(html, 'html.parser')
# ä¼˜ç‚¹ï¼šå†…ç½®ã€ç¨³å®š
# ç¼ºç‚¹ï¼šé€Ÿåº¦è¾ƒæ…¢

# html5libï¼ˆç‰¹æ®Šæƒ…å†µï¼‰ï¼š
soup = BeautifulSoup(html, 'html5lib')
# ä¼˜ç‚¹ï¼šæœ€ä¸¥æ ¼çš„HTML5è§£æ
# ç¼ºç‚¹ï¼šæœ€æ…¢
```

#### 2. é€‰æ‹©å™¨ä¼˜åŒ–

```python
# âœ“ é«˜æ•ˆï¼š
soup.select_one('#abstract')  # IDæœ€å¿«

# âœ“ ä¸€èˆ¬ï¼š
soup.select_one('div.abstract')  # ç±»å

# âœ— ä½æ•ˆï¼š
soup.select_one('div[class*="abstract"]')  # å±æ€§åŒ¹é…è¾ƒæ…¢
```

### æ‰©å±•æ–¹å‘

#### 1. æ·»åŠ æ–°å‡ºç‰ˆå•†

```python
@classmethod
def nature_rule(cls, html: str) -> Optional[str]:
    """Nature æ‘˜è¦è§£æè§„åˆ™"""
    try:
        soup = BeautifulSoup(html, 'lxml')
        selectors = [
            '#Abs1-content',
            'div[data-test="article-abstract"]',
            # ...
        ]
        # ä½¿ç”¨ç›¸åŒçš„å¤„ç†é€»è¾‘
    except:
        return None
```

#### 2. æ·»åŠ ç¼“å­˜æœºåˆ¶

```python
from functools import lru_cache

@classmethod
@lru_cache(maxsize=100)
def ieee_rule(cls, html: str) -> Optional[str]:
    # ç¼“å­˜ç»“æœï¼Œé¿å…é‡å¤è§£æ
    pass
```

#### 3. æ·»åŠ æ—¥å¿—è®°å½•

```python
import logging

@classmethod
def ieee_rule(cls, html: str) -> Optional[str]:
    try:
        soup = BeautifulSoup(html, 'lxml')
        logging.info(f"å¼€å§‹è§£æIEEEé¡µé¢ï¼Œé•¿åº¦: {len(html)}")
        # ...
    except Exception as e:
        logging.error(f"IEEEè§£æå¤±è´¥: {e}")
        return None
```

---

## ğŸ“ å­¦ä¹ ä»·å€¼æ€»ç»“

é€šè¿‡å­¦ä¹  `Rule.py`ï¼Œæ‚¨æŒæ¡äº†ï¼š

### Pythonæ ¸å¿ƒæŠ€èƒ½

| æŠ€èƒ½ | è¦†ç›–å†…å®¹ | é‡è¦ç¨‹åº¦ |
|------|----------|---------|
| **ç±»å‹æ³¨è§£** | Optionalã€Dictã€Listã€Union | â­â­â­â­â­ |
| **è£…é¥°å™¨** | @dataclassã€@staticmethodã€@classmethod | â­â­â­â­â­ |
| **å¼‚å¸¸å¤„ç†** | try-exceptã€å¤šå±‚é˜²å¾¡ | â­â­â­â­â­ |
| **æ­£åˆ™è¡¨è¾¾å¼** | re.subã€flags | â­â­â­â­ |
| **lambdaå‡½æ•°** | åŒ¿åå‡½æ•°ã€é«˜é˜¶å‡½æ•° | â­â­â­â­ |

### Webçˆ¬è™«æŠ€æœ¯

| æŠ€èƒ½ | è¦†ç›–å†…å®¹ | é‡è¦ç¨‹åº¦ |
|------|----------|---------|
| **HTMLè§£æ** | BeautifulSoupã€é€‰æ‹©å™¨ | â­â­â­â­â­ |
| **CSSé€‰æ‹©å™¨** | ç±»ã€IDã€å±æ€§ã€å±‚çº§ | â­â­â­â­â­ |
| **HTTPè¯·æ±‚** | requestsã€Sessionã€é‡è¯• | â­â­â­â­ |
| **æ–‡æœ¬å¤„ç†** | æ¸…ç†ã€è§„èŒƒåŒ–ã€éªŒè¯ | â­â­â­â­ |
| **JSONå¤„ç†** | JSON-LDã€ç»“æ„åŒ–æ•°æ® | â­â­â­â­ |

### è½¯ä»¶å·¥ç¨‹

| æŠ€èƒ½ | è¦†ç›–å†…å®¹ | é‡è¦ç¨‹åº¦ |
|------|----------|---------|
| **è®¾è®¡æ¨¡å¼** | ä¼˜å…ˆçº§ã€é˜²å¾¡å¼ç¼–ç¨‹ | â­â­â­â­â­ |
| **å®¹é”™å¤„ç†** | å¤šå±‚æ¬¡å¼‚å¸¸å¤„ç† | â­â­â­â­â­ |
| **ä»£ç å¤ç”¨** | å·¥å…·æ–¹æ³•ã€é€šç”¨é€»è¾‘ | â­â­â­â­ |
| **å¯æ‰©å±•æ€§** | æ˜“äºæ·»åŠ æ–°è§„åˆ™ | â­â­â­â­ |

---

**å®Œæ•´çš„HTMLè§£æè§„åˆ™ç³»ç»Ÿå®ç°ï¼** ğŸ“âœ¨

è¿™æ˜¯ä¸€ä¸ªç”Ÿäº§çº§çš„Webçˆ¬è™«è§£ææ¨¡å—ï¼Œå±•ç¤ºäº†å¦‚ä½•åº”å¯¹å¤æ‚å¤šå˜çš„ç½‘é¡µç»“æ„ï¼

**å»ºè®®å®è·µ**ï¼š
1. è®¿é—®IEEEã€ACMç­‰ç½‘ç«™ï¼ŒæŸ¥çœ‹HTMLç»“æ„
2. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ç»ƒä¹ CSSé€‰æ‹©å™¨
3. å°è¯•ä¸ºå…¶ä»–å­¦æœ¯ç½‘ç«™ç¼–å†™è§£æè§„åˆ™
4. å­¦ä¹ æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œæ–‡æœ¬æ¸…ç†

**ä»é›¶å¼€å§‹æˆä¸ºWebçˆ¬è™«ä¸“å®¶ï¼** ğŸš€ğŸ’

