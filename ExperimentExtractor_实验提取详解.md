# Experiment Extractor å®éªŒä¿¡æ¯æå–å™¨è¯¦è§£

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [å¯¼å…¥è¯­å¥è¯¦è§£](#å¯¼å…¥è¯­å¥è¯¦è§£)
3. [å…¨å±€é…ç½®å’Œå¸¸é‡](#å…¨å±€é…ç½®å’Œå¸¸é‡)
4. [æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼](#æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼)
5. [ç¼“å­˜ç³»ç»Ÿ](#ç¼“å­˜ç³»ç»Ÿ)
6. [æ ¸å¿ƒæå–å‡½æ•°](#æ ¸å¿ƒæå–å‡½æ•°)
7. [æŠ€æœ¯æ€»ç»“](#æŠ€æœ¯æ€»ç»“)

---

## ğŸ“– é¡¹ç›®æ¦‚è¿°

`experiment_extractor.py` æ˜¯ä¸€ä¸ª**æ™ºèƒ½è®ºæ–‡å®éªŒä¿¡æ¯æå–å™¨**ï¼Œç”¨äºä»å­¦æœ¯è®ºæ–‡ä¸­è‡ªåŠ¨æå–ä»¥ä¸‹ä¿¡æ¯ï¼š

### ä¸»è¦åŠŸèƒ½

- ğŸ“Š **å¯¹æ¯”ç®—æ³•æå–**: è¯†åˆ«è®ºæ–‡ä¸­æåˆ°çš„å¯¹æ¯”ç®—æ³•å’ŒåŸºå‡†æ–¹æ³•
- ğŸ’» **ä»£ç é“¾æ¥æå–**: æŸ¥æ‰¾GitHubç­‰ä»£ç ä»“åº“é“¾æ¥
- ğŸ¯ **éœ€æ±‚åœºæ™¯æå–**: æå–ç ”ç©¶çš„å®é™…åº”ç”¨åœºæ™¯
- ğŸ” **ç ”ç©¶é—®é¢˜æå–**: è¯†åˆ«è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜
- ğŸ“ **ç ”ç©¶ç›®æ ‡æå–**: æå–ç ”ç©¶çš„ç›®çš„å’Œç›®æ ‡
- ğŸ’¡ **åˆ›æ–°ç‚¹æå–**: è¯†åˆ«è®ºæ–‡çš„åˆ›æ–°æ€è·¯å’Œè´¡çŒ®

### æŠ€æœ¯ç‰¹ç‚¹

- ğŸš€ **æ™ºèƒ½ç¼“å­˜**: MD5å“ˆå¸Œç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤å¤„ç†
- ğŸ”„ **å¹¶å‘å¤„ç†**: ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†å¤šç¯‡è®ºæ–‡
- ğŸŒ **å¤šæºè·å–**: æ”¯æŒä»URLã€DOIã€PDFç­‰å¤šç§æ¥æºæå–
- ğŸ“ **æ­£åˆ™åŒ¹é…**: 100+æ¡æ­£åˆ™è¡¨è¾¾å¼ç²¾å‡†åŒ¹é…å­¦æœ¯æ–‡æœ¬
- ğŸ›¡ï¸ **å®¹é”™è®¾è®¡**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé™çº§ç­–ç•¥

### æ–‡ä»¶ç»Ÿè®¡

- **æ€»è¡Œæ•°**: 1266è¡Œ
- **å¯¼å…¥æ¨¡å—**: 10ä¸ª
- **æ­£åˆ™æ¨¡å¼**: 6å¤§ç±»100+æ¡
- **æ ¸å¿ƒå‡½æ•°**: 15ä¸ª

---

## ğŸ“¦ å¯¼å…¥è¯­å¥è¯¦è§£

### Pythonæ ‡å‡†åº“ï¼ˆç¬¬1-6è¡Œï¼‰

```python
import re                        # ç¬¬1è¡Œ - æ­£åˆ™è¡¨è¾¾å¼
import os                        # ç¬¬2è¡Œ - æ“ä½œç³»ç»Ÿæ¥å£
import json                      # ç¬¬3è¡Œ - JSONå¤„ç†
import time                      # ç¬¬4è¡Œ - æ—¶é—´å‡½æ•°
import random                    # ç¬¬5è¡Œ - éšæœºæ•°
import hashlib                   # ç¬¬6è¡Œ - å“ˆå¸Œç®—æ³•
```

**ç¬¬1è¡Œè¯¦è§£**ï¼š`import re`

**reæ­£åˆ™è¡¨è¾¾å¼æ¨¡å—è¯¦è§£ï¼ˆç¬¬1è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå¯¼å…¥å…³é”®å­—
- `re`ï¼šRegular Expressionï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰æ¨¡å—
- ç”¨é€”ï¼šæ–‡æœ¬æ¨¡å¼åŒ¹é…å’Œæœç´¢

```python
import re

# æ­£åˆ™è¡¨è¾¾å¼æ˜¯ä»€ä¹ˆï¼Ÿ
# ä¸€ç§å¼ºå¤§çš„æ–‡æœ¬åŒ¹é…å·¥å…·ï¼Œç”¨äºæŸ¥æ‰¾ã€æå–ã€æ›¿æ¢æ–‡æœ¬

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨åœºæ™¯
# 1. åŒ¹é…å®éªŒç« èŠ‚æ ‡é¢˜
pattern = r"(?:4|IV|4\.0)[\s\.]*(?:Experiment|Evaluation|Results)"
text = "4. Experiment and Results"
match = re.search(pattern, text, re.IGNORECASE)
if match:
    print(f"æ‰¾åˆ°å®éªŒç« èŠ‚ï¼š{match.group()}")

# 2. æå–å¯¹æ¯”ç®—æ³•
pattern = r"compar(?:e|ed|ing|ison)\s+(?:with|to|against)"
text = "We compared our method with baseline algorithms"
if re.search(pattern, text, re.IGNORECASE):
    print("æåˆ°äº†å¯¹æ¯”ç®—æ³•")

# 3. æå–GitHubé“¾æ¥
pattern = r"github\.com/[\w-]+/[\w-]+"
text = "Code available at https://github.com/user/repo"
match = re.search(pattern, text)
if match:
    print(f"æ‰¾åˆ°ä»£ç é“¾æ¥ï¼š{match.group()}")

# reæ¨¡å—çš„æ ¸å¿ƒå‡½æ•°
# 1. re.search() - æœç´¢ç¬¬ä¸€ä¸ªåŒ¹é…
match = re.search(r'experiment', text, re.IGNORECASE)

# 2. re.findall() - æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…
urls = re.findall(r'https?://[^\s]+', text)

# 3. re.sub() - æ›¿æ¢åŒ¹é…
clean_text = re.sub(r'\s+', ' ', text)  # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ª

# 4. re.compile() - ç¼–è¯‘æ­£åˆ™ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
pattern = re.compile(r'experiment', re.IGNORECASE)
matches = pattern.findall(text)

# æ­£åˆ™è¡¨è¾¾å¼çš„åŸºæœ¬è¯­æ³•
# .        - ä»»æ„å­—ç¬¦
# *        - 0æ¬¡æˆ–å¤šæ¬¡
# +        - 1æ¬¡æˆ–å¤šæ¬¡
# ?        - 0æ¬¡æˆ–1æ¬¡
# {n,m}    - nåˆ°mæ¬¡
# []       - å­—ç¬¦é›†
# ()       - åˆ†ç»„
# |        - æˆ–
# ^        - å¼€å¤´
# $        - ç»“å°¾
# \s       - ç©ºç™½å­—ç¬¦
# \d       - æ•°å­—
# \w       - å­—æ¯æ•°å­—ä¸‹åˆ’çº¿

# å®æˆ˜ç¤ºä¾‹ï¼šåŒ¹é…å®éªŒç« èŠ‚
pattern = r"(?:4|IV|4\.0)[\s\.]*(?:Experiment|Evaluation|Results)"
# è§£æï¼š
# (?:4|IV|4\.0)  - åŒ¹é…"4"æˆ–"IV"æˆ–"4.0"ï¼ˆéæ•è·ç»„ï¼‰
# [\s\.]*        - åŒ¹é…0ä¸ªæˆ–å¤šä¸ªç©ºç™½æˆ–ç‚¹å·
# (?:Experiment|Evaluation|Results) - åŒ¹é…è¿™ä¸‰ä¸ªè¯ä¹‹ä¸€

texts = [
    "4. Experiment",
    "IV. Evaluation",
    "4.0 Results",
    "4   Experiment and Results"
]

for text in texts:
    if re.search(pattern, text, re.IGNORECASE):
        print(f"âœ“ åŒ¹é…ï¼š{text}")

# ä¸ºä»€ä¹ˆç”¨(?:...)è€Œä¸æ˜¯(...)ï¼Ÿ
# (?:...)  - éæ•è·ç»„ï¼Œåªåˆ†ç»„ä¸æ•è·ï¼Œæ€§èƒ½æ›´å¥½
# (...)    - æ•è·ç»„ï¼Œä¿å­˜åŒ¹é…ç»“æœ
```

**ç¬¬6è¡Œè¯¦è§£**ï¼š`import hashlib`

**hashlibå“ˆå¸Œç®—æ³•è¯¦è§£ï¼ˆç¬¬6è¡Œï¼‰**ï¼š
- `hashlib`ï¼šå“ˆå¸Œç®—æ³•åº“
- ç”¨é€”ï¼šç”Ÿæˆæ•°æ®çš„å“ˆå¸Œå€¼ï¼ˆMD5ã€SHAç­‰ï¼‰

```python
import hashlib

# å“ˆå¸Œç®—æ³•æ˜¯ä»€ä¹ˆï¼Ÿ
# å°†ä»»æ„é•¿åº¦çš„æ•°æ®è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å­—ç¬¦ä¸²
# ç‰¹ç‚¹ï¼šå•å‘ã€ç¡®å®šæ€§ã€é›ªå´©æ•ˆåº”

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨ï¼šç”Ÿæˆç¼“å­˜æ–‡ä»¶å
paper_key = "https://arxiv.org/abs/2024.12345"
cache_version = "v2"

# æ­¥éª¤1ï¼šåˆ›å»ºç‰ˆæœ¬åŒ–çš„é”®
versioned_key = f"{cache_version}_{paper_key}"
# "v2_https://arxiv.org/abs/2024.12345"

# æ­¥éª¤2ï¼šç¼–ç ä¸ºå­—èŠ‚
key_bytes = versioned_key.encode()

# æ­¥éª¤3ï¼šç”ŸæˆMD5å“ˆå¸Œ
hash_object = hashlib.md5(key_bytes)

# æ­¥éª¤4ï¼šè·å–åå…­è¿›åˆ¶å­—ç¬¦ä¸²
hash_hex = hash_object.hexdigest()
# "a1b2c3d4e5f6789..."ï¼ˆ32ä¸ªå­—ç¬¦ï¼‰

# æ­¥éª¤5ï¼šç”Ÿæˆç¼“å­˜æ–‡ä»¶å
cache_filename = hash_hex + ".json"
# "a1b2c3d4e5f6789....json"

# ä¸€è¡Œå®Œæˆ
cache_filename = hashlib.md5(f"v2_{paper_key}".encode()).hexdigest() + ".json"

# ä¸ºä»€ä¹ˆä½¿ç”¨å“ˆå¸Œï¼Ÿ
# 1. å›ºå®šé•¿åº¦ï¼šæ— è®ºURLå¤šé•¿ï¼Œå“ˆå¸Œå€¼éƒ½æ˜¯32å­—ç¬¦
# 2. åˆæ³•æ–‡ä»¶åï¼šåªåŒ…å«[0-9a-f]ï¼Œæ²¡æœ‰ç‰¹æ®Šå­—ç¬¦
# 3. å”¯ä¸€æ€§ï¼šä¸åŒçš„URLç”Ÿæˆä¸åŒçš„å“ˆå¸Œå€¼
# 4. å¿«é€ŸæŸ¥æ‰¾ï¼šå“ˆå¸Œå€¼å¯ä»¥ä½œä¸ºå­—å…¸é”®

# MD5å“ˆå¸Œç¤ºä¾‹
urls = [
    "https://arxiv.org/abs/2024.12345",
    "https://ieeexplore.ieee.org/document/98765",
    "https://dl.acm.org/doi/10.1145/123456"
]

for url in urls:
    hash_value = hashlib.md5(url.encode()).hexdigest()
    print(f"{url[:40]}... â†’ {hash_value[:16]}...")

# ä¸åŒå“ˆå¸Œç®—æ³•å¯¹æ¯”
text = "Hello, World!"

# MD5ï¼ˆ128ä½ï¼Œ32å­—ç¬¦ï¼‰- æœ€å¿«ï¼Œä½†ä¸å¤ªå®‰å…¨
md5_hash = hashlib.md5(text.encode()).hexdigest()
print(f"MD5:    {md5_hash}")

# SHA1ï¼ˆ160ä½ï¼Œ40å­—ç¬¦ï¼‰
sha1_hash = hashlib.sha1(text.encode()).hexdigest()
print(f"SHA1:   {sha1_hash}")

# SHA256ï¼ˆ256ä½ï¼Œ64å­—ç¬¦ï¼‰- æ›´å®‰å…¨ï¼Œè¾ƒæ…¢
sha256_hash = hashlib.sha256(text.encode()).hexdigest()
print(f"SHA256: {sha256_hash}")

# æœ¬é¡¹ç›®ä¸ºä»€ä¹ˆç”¨MD5ï¼Ÿ
# 1. åªæ˜¯ç”Ÿæˆæ–‡ä»¶åï¼Œä¸éœ€è¦é«˜å®‰å…¨æ€§
# 2. MD5æœ€å¿«
# 3. 32å­—ç¬¦çš„æ–‡ä»¶åé•¿åº¦é€‚ä¸­
```

### ç¬¬ä¸‰æ–¹åº“ï¼ˆç¬¬7-10è¡Œï¼‰

```python
import requests                      # ç¬¬7è¡Œ - HTTPè¯·æ±‚
from bs4 import BeautifulSoup        # ç¬¬8è¡Œ - HTMLè§£æ
from urllib.parse import urlparse    # ç¬¬9è¡Œ - URLè§£æ
import concurrent.futures            # ç¬¬10è¡Œ - å¹¶å‘å¤„ç†
```

**ç¬¬8è¡Œè¯¦è§£**ï¼š`from bs4 import BeautifulSoup`

**BeautifulSoup HTMLè§£æè¯¦è§£ï¼ˆç¬¬8è¡Œï¼‰**ï¼š
- `bs4`ï¼šBeautiful Soup 4çš„åŒ…å
- `BeautifulSoup`ï¼šHTML/XMLè§£æå™¨ç±»
- ç”¨é€”ï¼šè§£æç½‘é¡µï¼Œæå–å†…å®¹

```python
from bs4 import BeautifulSoup

# BeautifulSoupæ˜¯ä»€ä¹ˆï¼Ÿ
# ä¸€ä¸ªå¼ºå¤§çš„HTML/XMLè§£æåº“ï¼Œå¯ä»¥è½»æ¾æå–ç½‘é¡µå†…å®¹

# åŸºæœ¬ä½¿ç”¨æµç¨‹
# 1. è·å–HTML
html = """
<html>
<head><title>Deep Learning Paper</title></head>
<body>
    <div class="abstract">
        <p>This paper presents a novel approach...</p>
    </div>
    <section id="experiments">
        <h2>4. Experiments</h2>
        <p>We compare our method with baseline algorithms...</p>
        <p>Code available at <a href="https://github.com/user/repo">GitHub</a></p>
    </section>
</body>
</html>
"""

# 2. åˆ›å»ºBeautifulSoupå¯¹è±¡
soup = BeautifulSoup(html, 'html.parser')
# ç¬¬äºŒä¸ªå‚æ•°æ˜¯è§£æå™¨ï¼š
# - 'html.parser': Pythonå†…ç½®ï¼Œé€Ÿåº¦ä¸­ç­‰
# - 'lxml': æœ€å¿«ï¼Œéœ€è¦å®‰è£…lxml
# - 'html5lib': æœ€å®¹é”™ï¼Œéœ€è¦å®‰è£…html5lib

# 3. æŸ¥æ‰¾å…ƒç´ 
# æ–¹æ³•1: find() - æŸ¥æ‰¾ç¬¬ä¸€ä¸ª
title = soup.find('title')
print(title.text)  # "Deep Learning Paper"

# æ–¹æ³•2: find_all() - æŸ¥æ‰¾æ‰€æœ‰
paragraphs = soup.find_all('p')
for p in paragraphs:
    print(p.text)

# æ–¹æ³•3: CSSé€‰æ‹©å™¨ - æ›´çµæ´»
abstract = soup.select_one('.abstract p')
print(abstract.text)

# æ–¹æ³•4: æŒ‰å±æ€§æŸ¥æ‰¾
experiments = soup.find('section', id='experiments')
print(experiments.h2.text)  # "4. Experiments"

# æ–¹æ³•5: æŸ¥æ‰¾é“¾æ¥
links = soup.find_all('a', href=True)
for link in links:
    print(link['href'])  # è·å–hrefå±æ€§

# æœ¬é¡¹ç›®ä¸­çš„å®é™…åº”ç”¨ï¼šæå–è®ºæ–‡å†…å®¹
def extract_paper_content(html):
    """ä»è®ºæ–‡HTMLä¸­æå–æ–‡æœ¬"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # 1. æå–æ ‡é¢˜
    title = soup.find('h1', class_='title')
    if title:
        title_text = title.get_text(strip=True)
    
    # 2. æå–æ‘˜è¦
    abstract = soup.find('div', class_='abstract')
    if abstract:
        abstract_text = abstract.get_text(strip=True)
    
    # 3. æå–æ‰€æœ‰æ®µè½
    paragraphs = soup.find_all('p')
    all_text = ' '.join([p.get_text() for p in paragraphs])
    
    # 4. æå–é“¾æ¥
    links = soup.find_all('a', href=True)
    github_links = [
        link['href'] 
        for link in links 
        if 'github.com' in link['href']
    ]
    
    return {
        'title': title_text,
        'abstract': abstract_text,
        'content': all_text,
        'code_links': github_links
    }

# BeautifulSoupçš„é«˜çº§æŠ€å·§
# 1. å¯¼èˆªæ ‘
soup = BeautifulSoup(html, 'html.parser')
section = soup.find('section')
section.parent        # çˆ¶å…ƒç´ 
section.next_sibling  # ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
section.children      # æ‰€æœ‰å­å…ƒç´ 

# 2. æœç´¢æ–‡æœ¬
soup.find_all(text=re.compile('experiment'))  # æŸ¥æ‰¾åŒ…å«"experiment"çš„æ–‡æœ¬

# 3. ç»„åˆæ¡ä»¶
soup.find_all('p', class_='highlight', id=re.compile('para-'))

# 4. Lambdaå‡½æ•°è¿‡æ»¤
soup.find_all(lambda tag: tag.name == 'p' and len(tag.text) > 100)

# ä¸ºä»€ä¹ˆéœ€è¦HTMLè§£æï¼Ÿ
# è®ºæ–‡ç½‘ç«™HTMLç¤ºä¾‹ï¼š
# <div class="paper-content">
#   <h2>4. Experiments</h2>
#   <p>We evaluated our approach...</p>
# </div>

# ä¸ç”¨BeautifulSoupï¼ˆå›°éš¾ï¼‰ï¼š
# éœ€è¦æ‰‹åŠ¨å†™æ­£åˆ™è¡¨è¾¾å¼ï¼Œå®¹æ˜“å‡ºé”™
pattern = r'<h2>(.*?)</h2>.*?<p>(.*?)</p>'
# è¿™ä¸ªæ­£åˆ™åªèƒ½å¤„ç†ç®€å•æƒ…å†µï¼Œå¤æ‚HTMLä¼šå¤±è´¥

# ç”¨BeautifulSoupï¼ˆç®€å•ï¼‰ï¼š
soup = BeautifulSoup(html, 'html.parser')
h2 = soup.find('h2').text
p = soup.find('p').text
```

**ç¬¬9è¡Œè¯¦è§£**ï¼š`from urllib.parse import urlparse`

**urlparse URLè§£æè¯¦è§£ï¼ˆç¬¬9è¡Œï¼‰**ï¼š
- `urllib.parse`ï¼šURLè§£ææ¨¡å—
- `urlparse`ï¼šå°†URLåˆ†è§£ä¸ºç»„æˆéƒ¨åˆ†

```python
from urllib.parse import urlparse

# urlparseæ˜¯ä»€ä¹ˆï¼Ÿ
# å°†URLåˆ†è§£ä¸ºåè®®ã€åŸŸåã€è·¯å¾„ç­‰ç»„ä»¶

# åŸºæœ¬ä½¿ç”¨
url = "https://arxiv.org:443/abs/2024.12345?version=1#section2"
parsed = urlparse(url)

print(parsed.scheme)    # 'https' - åè®®
print(parsed.netloc)    # 'arxiv.org:443' - ç½‘ç»œä½ç½®ï¼ˆåŸŸå+ç«¯å£ï¼‰
print(parsed.hostname)  # 'arxiv.org' - ä¸»æœºå
print(parsed.port)      # 443 - ç«¯å£å·
print(parsed.path)      # '/abs/2024.12345' - è·¯å¾„
print(parsed.query)     # 'version=1' - æŸ¥è¯¢å‚æ•°
print(parsed.fragment)  # 'section2' - ç‰‡æ®µæ ‡è¯†ç¬¦

# URLçš„å®Œæ•´ç»“æ„
# scheme://netloc/path?query#fragment
# https://arxiv.org:443/abs/2024.12345?version=1#section2

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨ï¼šæå–åŸŸå
def get_domain(url):
    """ä»URLæå–åŸŸå"""
    parsed = urlparse(url)
    return parsed.hostname

urls = [
    "https://arxiv.org/abs/2024.12345",
    "https://ieeexplore.ieee.org/document/98765",
    "https://dl.acm.org/doi/10.1145/123456"
]

for url in urls:
    domain = get_domain(url)
    print(f"{url} â†’ {domain}")

# è¾“å‡ºï¼š
# https://arxiv.org/... â†’ arxiv.org
# https://ieeexplore.ieee.org/... â†’ ieeexplore.ieee.org
# https://dl.acm.org/... â†’ dl.acm.org

# å®é™…åº”ç”¨ï¼šæ ¹æ®åŸŸåé€‰æ‹©ä¸åŒçš„è§£æç­–ç•¥
def get_paper_parser(url):
    """æ ¹æ®URLé€‰æ‹©åˆé€‚çš„è§£æå™¨"""
    domain = urlparse(url).hostname
    
    if 'arxiv.org' in domain:
        return parse_arxiv
    elif 'ieee.org' in domain:
        return parse_ieee
    elif 'acm.org' in domain:
        return parse_acm
    else:
        return parse_generic

# urlparse vs å­—ç¬¦ä¸²åˆ†å‰²
# ä¸æ¨èï¼šæ‰‹åŠ¨åˆ†å‰²URL
url = "https://arxiv.org/abs/2024.12345"
parts = url.split('/')
domain = parts[2]  # 'arxiv.org'
# é—®é¢˜ï¼šæ— æ³•å¤„ç†ç«¯å£ã€æŸ¥è¯¢å‚æ•°ç­‰å¤æ‚æƒ…å†µ

# æ¨èï¼šä½¿ç”¨urlparse
parsed = urlparse(url)
domain = parsed.hostname  # æ­£ç¡®å¤„ç†å„ç§æƒ…å†µ

# ç›¸å…³å‡½æ•°
from urllib.parse import urljoin, quote, unquote

# urljoin - åˆå¹¶URL
base = "https://arxiv.org/abs/"
relative = "2024.12345"
full_url = urljoin(base, relative)
# "https://arxiv.org/abs/2024.12345"

# quote - URLç¼–ç 
text = "æ·±åº¦å­¦ä¹  è®ºæ–‡"
encoded = quote(text)
# "%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E8%AE%BA%E6%96%87"

# unquote - URLè§£ç 
decoded = unquote(encoded)
# "æ·±åº¦å­¦ä¹  è®ºæ–‡"
```

**ç¬¬10è¡Œè¯¦è§£**ï¼š`import concurrent.futures`

**concurrent.futureså¹¶å‘å¤„ç†è¯¦è§£ï¼ˆç¬¬10è¡Œï¼‰**ï¼š
- `concurrent.futures`ï¼šé«˜çº§å¹¶å‘æ¥å£
- ç”¨é€”ï¼šç®€åŒ–å¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹ç¼–ç¨‹

```python
import concurrent.futures
import time

# concurrent.futuresæ˜¯ä»€ä¹ˆï¼Ÿ
# Python 3.2+å¼•å…¥çš„é«˜çº§å¹¶å‘API
# æä¾›ThreadPoolExecutorï¼ˆçº¿ç¨‹æ± ï¼‰å’ŒProcessPoolExecutorï¼ˆè¿›ç¨‹æ± ï¼‰

# ä¸ºä»€ä¹ˆéœ€è¦å¹¶å‘ï¼Ÿ
# åœºæ™¯ï¼šæå–100ç¯‡è®ºæ–‡çš„å®éªŒä¿¡æ¯
# - ä¸²è¡Œå¤„ç†ï¼šæ¯ç¯‡3ç§’ Ã— 100 = 300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰
# - å¹¶å‘å¤„ç†ï¼ˆ10çº¿ç¨‹ï¼‰ï¼šæ¯æ‰¹3ç§’ Ã— 10æ‰¹ = 30ç§’

# åŸºæœ¬ä½¿ç”¨ï¼šThreadPoolExecutor
def extract_experiments(paper_url):
    """æå–å•ç¯‡è®ºæ–‡çš„å®éªŒä¿¡æ¯ï¼ˆè€—æ—¶æ“ä½œï¼‰"""
    time.sleep(3)  # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚
    return f"Extracted from {paper_url}"

# ä¸²è¡Œå¤„ç†ï¼ˆæ…¢ï¼‰
paper_urls = [f"https://arxiv.org/abs/2024.{i}" for i in range(5)]

start = time.time()
results = []
for url in paper_urls:
    result = extract_experiments(url)
    results.append(result)
print(f"ä¸²è¡Œè€—æ—¶ï¼š{time.time() - start:.2f}ç§’")  # ~15ç§’

# å¹¶å‘å¤„ç†ï¼ˆå¿«ï¼‰
start = time.time()
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    # submitæ–¹æ³•ï¼šæäº¤å•ä¸ªä»»åŠ¡
    futures = [executor.submit(extract_experiments, url) for url in paper_urls]
    
    # as_completedï¼šæŒ‰å®Œæˆé¡ºåºè·å–ç»“æœ
    results = []
    for future in concurrent.futures.as_completed(futures):
        result = future.result()
        results.append(result)

print(f"å¹¶å‘è€—æ—¶ï¼š{time.time() - start:.2f}ç§’")  # ~3ç§’

# ThreadPoolExecutorçš„æ ¸å¿ƒæ–¹æ³•
executor = concurrent.futures.ThreadPoolExecutor(max_workers=10)

# 1. submit() - æäº¤å•ä¸ªä»»åŠ¡
future = executor.submit(extract_experiments, url)
result = future.result()  # ç­‰å¾…å®Œæˆå¹¶è·å–ç»“æœ

# 2. map() - æ‰¹é‡æäº¤ä»»åŠ¡ï¼ˆä¿æŒé¡ºåºï¼‰
results = executor.map(extract_experiments, paper_urls)
for result in results:
    print(result)

# 3. as_completed() - æŒ‰å®Œæˆé¡ºåºå¤„ç†
futures = [executor.submit(extract_experiments, url) for url in paper_urls]
for future in concurrent.futures.as_completed(futures):
    result = future.result()
    print(f"å®Œæˆï¼š{result}")

# æœ¬é¡¹ç›®ä¸­çš„å®é™…åº”ç”¨
def extract_batch_experiments(paper_list, max_workers=10):
    """æ‰¹é‡æå–è®ºæ–‡å®éªŒä¿¡æ¯"""
    results = {}
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_paper = {
            executor.submit(extract_experiments, paper['url']): paper['title']
            for paper in paper_list
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in concurrent.futures.as_completed(future_to_paper):
            title = future_to_paper[future]
            try:
                result = future.result()
                results[title] = result
                print(f"âœ“ å®Œæˆï¼š{title}")
            except Exception as e:
                print(f"âœ— å¤±è´¥ï¼š{title} - {e}")
                results[title] = None
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
papers = [
    {'title': 'è®ºæ–‡1', 'url': 'https://arxiv.org/abs/2024.001'},
    {'title': 'è®ºæ–‡2', 'url': 'https://arxiv.org/abs/2024.002'},
    {'title': 'è®ºæ–‡3', 'url': 'https://arxiv.org/abs/2024.003'},
]

results = extract_batch_experiments(papers, max_workers=3)

# ThreadPoolExecutor vs ProcessPoolExecutor
# ThreadPoolExecutorï¼ˆçº¿ç¨‹æ± ï¼‰ï¼š
# - é€‚åˆï¼šI/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ï¼‰
# - ä¼˜ç‚¹ï¼šè½»é‡ã€å…±äº«å†…å­˜
# - ç¼ºç‚¹ï¼šå—GILé™åˆ¶ï¼ŒCPUå¯†é›†å‹ä»»åŠ¡æ— æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸

# ProcessPoolExecutorï¼ˆè¿›ç¨‹æ± ï¼‰ï¼š
# - é€‚åˆï¼šCPUå¯†é›†å‹ä»»åŠ¡ï¼ˆè®¡ç®—ã€å›¾åƒå¤„ç†ï¼‰
# - ä¼˜ç‚¹ï¼šçœŸæ­£å¹¶è¡Œï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸
# - ç¼ºç‚¹ï¼šè¿›ç¨‹å¼€é”€å¤§ã€ä¸å…±äº«å†…å­˜

# æœ¬é¡¹ç›®ç”¨ThreadPoolExecutorçš„åŸå› ï¼š
# æå–è®ºæ–‡ä¿¡æ¯ä¸»è¦æ˜¯ç½‘ç»œè¯·æ±‚ï¼ˆI/Oå¯†é›†å‹ï¼‰

# é”™è¯¯å¤„ç†
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(extract_experiments, url) for url in paper_urls]
    
    for future in concurrent.futures.as_completed(futures):
        try:
            result = future.result(timeout=30)  # è®¾ç½®è¶…æ—¶
            print(result)
        except concurrent.futures.TimeoutError:
            print("ä»»åŠ¡è¶…æ—¶")
        except Exception as e:
            print(f"ä»»åŠ¡å¤±è´¥ï¼š{e}")

# ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„ä¼˜åŠ¿
# withè¯­å¥ç¡®ä¿ï¼š
# 1. ä»»åŠ¡å®Œæˆåè‡ªåŠ¨å…³é—­çº¿ç¨‹æ± 
# 2. é‡Šæ”¾èµ„æº
# 3. å³ä½¿å‘ç”Ÿå¼‚å¸¸ä¹Ÿèƒ½æ­£ç¡®æ¸…ç†

# ä¸ä½¿ç”¨withï¼ˆéœ€è¦æ‰‹åŠ¨ç®¡ç†ï¼‰ï¼š
executor = concurrent.futures.ThreadPoolExecutor(max_workers=5)
try:
    # æäº¤ä»»åŠ¡...
    pass
finally:
    executor.shutdown(wait=True)  # å¿…é¡»æ‰‹åŠ¨å…³é—­

# ä½¿ç”¨withï¼ˆæ¨èï¼‰ï¼š
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    # æäº¤ä»»åŠ¡...
    pass
# è‡ªåŠ¨å…³é—­å’Œæ¸…ç†
```

---

## âš™ï¸ å…¨å±€é…ç½®å’Œå¸¸é‡

### ç¼“å­˜ç‰ˆæœ¬ï¼ˆç¬¬12-13è¡Œï¼‰

```python
# ç¼“å­˜ç‰ˆæœ¬                                 # ç¬¬12è¡Œï¼ˆæ³¨é‡Šï¼‰
CACHE_VERSION = "v2"  # æ›´æ–°ç‰ˆæœ¬ä»¥é¿å…ä¸æ—§ç¼“å­˜å†²çª  # ç¬¬13è¡Œ
```

**ç¬¬13è¡Œè¯¦è§£**ï¼š`CACHE_VERSION = "v2"`

**ç¼“å­˜ç‰ˆæœ¬æ§åˆ¶è¯¦è§£ï¼ˆç¬¬13è¡Œï¼‰**ï¼š
- `CACHE_VERSION`ï¼šå¸¸é‡åï¼ˆå…¨å¤§å†™ï¼‰
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `"v2"`ï¼šç‰ˆæœ¬å­—ç¬¦ä¸²

```python
CACHE_VERSION = "v2"

# ä¸ºä»€ä¹ˆéœ€è¦ç¼“å­˜ç‰ˆæœ¬ï¼Ÿ
# é—®é¢˜åœºæ™¯ï¼š
# 1. ä½ ä¿®æ”¹äº†æå–é€»è¾‘ï¼Œæå–å­—æ®µä»5ä¸ªå¢åŠ åˆ°6ä¸ª
# 2. æ—§ç¼“å­˜æ–‡ä»¶åªæœ‰5ä¸ªå­—æ®µ
# 3. ç¨‹åºè¯»å–æ—§ç¼“å­˜ï¼Œå­—æ®µç¼ºå¤±å¯¼è‡´é”™è¯¯

# è§£å†³æ–¹æ¡ˆï¼šç‰ˆæœ¬æ§åˆ¶
# ä¿®æ”¹ä»£ç æ—¶ï¼Œæ›´æ–°ç‰ˆæœ¬å·ï¼šv1 â†’ v2
# ç¨‹åºä¼šç”Ÿæˆæ–°çš„ç¼“å­˜æ–‡ä»¶ï¼Œå¿½ç•¥æ—§ç¼“å­˜

# ç‰ˆæœ¬å¦‚ä½•ä½¿ç”¨
def get_cache_path(paper_url):
    """ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # å°†ç‰ˆæœ¬å·åŠ å…¥å“ˆå¸Œ
    versioned_key = f"{CACHE_VERSION}_{paper_url}"
    # "v2_https://arxiv.org/abs/2024.12345"
    
    cache_filename = hashlib.md5(versioned_key.encode()).hexdigest() + ".json"
    return os.path.join("experiment_cache", cache_filename)

# ç‰ˆæœ¬å˜åŒ–çš„å½±å“
# v1ç‰ˆæœ¬ï¼š
url = "https://arxiv.org/abs/2024.12345"
v1_key = f"v1_{url}"
v1_hash = hashlib.md5(v1_key.encode()).hexdigest()
# v1_hash = "abc123..."
# ç¼“å­˜æ–‡ä»¶ï¼šexperiment_cache/abc123....json

# v2ç‰ˆæœ¬ï¼š
v2_key = f"v2_{url}"
v2_hash = hashlib.md5(v2_key.encode()).hexdigest()
# v2_hash = "def456..."ï¼ˆä¸åŒï¼ï¼‰
# ç¼“å­˜æ–‡ä»¶ï¼šexperiment_cache/def456....json

# ç»“æœï¼šv1å’Œv2çš„ç¼“å­˜æ–‡ä»¶ä¸åŒï¼Œäº’ä¸å½±å“

# ç‰ˆæœ¬æ¼”è¿›ç¤ºä¾‹
# v1ï¼šåªæå–å¯¹æ¯”ç®—æ³•å’Œä»£ç é“¾æ¥
CACHE_VERSION = "v1"
fields_v1 = ["comparative_algorithms", "code_links"]

# v2ï¼šå¢åŠ åœºæ™¯ã€é—®é¢˜ã€ç›®æ ‡ã€åˆ›æ–°
CACHE_VERSION = "v2"  # æ›´æ–°ç‰ˆæœ¬
fields_v2 = [
    "comparative_algorithms", 
    "code_links",
    "scenario",           # æ–°å¢
    "research_problem",   # æ–°å¢
    "research_goal",      # æ–°å¢
    "innovation"          # æ–°å¢
]

# ç¼“å­˜æ•°æ®ç»“æ„çš„å˜åŒ–
# v1ç¼“å­˜æ•°æ®ï¼š
v1_data = {
    "comparative_algorithms": ["Algorithm A", "Algorithm B"],
    "code_links": ["https://github.com/..."]
}

# v2ç¼“å­˜æ•°æ®ï¼š
v2_data = {
    "comparative_algorithms": ["Algorithm A", "Algorithm B"],
    "code_links": ["https://github.com/..."],
    "scenario": "...",
    "research_problem": "...",
    "research_goal": "...",
    "innovation": "..."
}

# å¦‚æœä¸æ›´æ–°ç‰ˆæœ¬ä¼šæ€æ ·ï¼Ÿ
# 1. ç¨‹åºè¯»å–v1ç¼“å­˜
# 2. å°è¯•è®¿é—®v2æ–°å¢çš„å­—æ®µ
# 3. KeyErrorï¼ç¨‹åºå´©æºƒ

# æ­£ç¡®çš„åšæ³•ï¼š
def validate_cache_data(data):
    """éªŒè¯ç¼“å­˜æ•°æ®æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ"""
    required_fields = [
        "comparative_algorithms", 
        "code_links",
        "scenario",
        "research_problem",
        "research_goal",
        "innovation"
    ]
    
    for field in required_fields:
        if field not in data:
            return False  # ç¼“å­˜æ— æ•ˆ
    
    return True  # ç¼“å­˜æœ‰æ•ˆ

# åœ¨è¯»å–ç¼“å­˜æ—¶éªŒè¯
def get_from_cache(paper_url):
    cache_path = get_cache_path(paper_url)
    if os.path.exists(cache_path):
        with open(cache_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # éªŒè¯ç¼“å­˜
            if not validate_cache_data(data):
                return None  # ç¼“å­˜æ— æ•ˆï¼Œéœ€è¦é‡æ–°æå–
            
            return data
    return None
```

### ç›®å½•åˆå§‹åŒ–ï¼ˆç¬¬15-20è¡Œï¼‰

```python
# ç¡®ä¿ç›®å½•å­˜åœ¨                             # ç¬¬15è¡Œï¼ˆæ³¨é‡Šï¼‰
EXPERIMENT_CACHE_DIR = "experiment_cache"   # ç¬¬16è¡Œ
EXPERIMENT_RESULTS_DIR = "experiment_results"  # ç¬¬17è¡Œ
for folder in [EXPERIMENT_CACHE_DIR, EXPERIMENT_RESULTS_DIR]:  # ç¬¬18è¡Œ
    if not os.path.exists(folder):          # ç¬¬19è¡Œ
        os.makedirs(folder)                 # ç¬¬20è¡Œ
```

**ç¬¬16-17è¡Œè¯¦è§£**ï¼šç›®å½•å¸¸é‡å®šä¹‰

```python
EXPERIMENT_CACHE_DIR = "experiment_cache"
EXPERIMENT_RESULTS_DIR = "experiment_results"

# ç›®å½•ç”¨é€”è¯´æ˜
# 1. experiment_cache - ç¼“å­˜ç›®å½•
#    å­˜å‚¨ï¼šå·²æå–çš„è®ºæ–‡å®éªŒä¿¡æ¯ï¼ˆJSONæ–‡ä»¶ï¼‰
#    ç›®çš„ï¼šé¿å…é‡å¤å¤„ç†åŒä¸€ç¯‡è®ºæ–‡
#    ç¤ºä¾‹ï¼šexperiment_cache/abc123....json

# 2. experiment_results - ç»“æœç›®å½•
#    å­˜å‚¨ï¼šæ‰¹é‡æå–çš„æœ€ç»ˆç»“æœï¼ˆJSONæ–‡ä»¶ï¼‰
#    ç›®çš„ï¼šä¿å­˜ç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´ç»“æœ
#    ç¤ºä¾‹ï¼šexperiment_results/query_20240115.json

# ä¸ºä»€ä¹ˆç”¨å¸¸é‡ï¼Ÿ
# ä¼˜ç‚¹ï¼š
# 1. é›†ä¸­ç®¡ç†ï¼šæ‰€æœ‰è·¯å¾„åœ¨ä¸€å¤„å®šä¹‰
# 2. æ˜“äºä¿®æ”¹ï¼šåªéœ€æ”¹ä¸€å¤„ï¼Œæ‰€æœ‰å¼•ç”¨è‡ªåŠ¨æ›´æ–°
# 3. é¿å…æ‹¼å†™é”™è¯¯ï¼šä½¿ç”¨å¸¸é‡è€Œä¸æ˜¯å­—ç¬¦ä¸²å­—é¢é‡

# ä½¿ç”¨ç¤ºä¾‹
# ä¸æ¨èï¼šç¡¬ç¼–ç è·¯å¾„
cache_file = os.path.join("experiment_cache", "abc123.json")
# é—®é¢˜ï¼šå¦‚æœç›®å½•åæ”¹äº†ï¼Œéœ€è¦æœç´¢æ‰€æœ‰ä»£ç ä¿®æ”¹

# æ¨èï¼šä½¿ç”¨å¸¸é‡
cache_file = os.path.join(EXPERIMENT_CACHE_DIR, "abc123.json")
# ä¼˜åŠ¿ï¼šåªéœ€ä¿®æ”¹å¸¸é‡å®šä¹‰ï¼Œæ‰€æœ‰åœ°æ–¹è‡ªåŠ¨æ›´æ–°
```

**ç¬¬18-20è¡Œè¯¦è§£**ï¼šè‡ªåŠ¨åˆ›å»ºç›®å½•

```python
for folder in [EXPERIMENT_CACHE_DIR, EXPERIMENT_RESULTS_DIR]:  # ç¬¬18è¡Œ
    if not os.path.exists(folder):          # ç¬¬19è¡Œ
        os.makedirs(folder)                 # ç¬¬20è¡Œ

# å®Œæ•´æ‰§è¡Œæµç¨‹
# 1. forå¾ªç¯éå†ç›®å½•åˆ—è¡¨
folders = ["experiment_cache", "experiment_results"]
for folder in folders:
    # folder = "experiment_cache"
    # folder = "experiment_results"
    pass

# 2. æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
if not os.path.exists(folder):
    # os.path.exists()è¿”å›ï¼š
    # True - ç›®å½•å­˜åœ¨
    # False - ç›®å½•ä¸å­˜åœ¨
    # not False = Trueï¼Œè¿›å…¥ifå—

# 3. åˆ›å»ºç›®å½•
os.makedirs(folder)
# é€’å½’åˆ›å»ºç›®å½•ï¼ˆåŒ…æ‹¬çˆ¶ç›®å½•ï¼‰

# ä¸ºä»€ä¹ˆåœ¨ç¨‹åºå¯åŠ¨æ—¶åˆ›å»ºç›®å½•ï¼Ÿ
# åŸå› ï¼š
# 1. ç¡®ä¿ç¨‹åºè¿è¡Œç¯å¢ƒæ­£ç¡®
# 2. é¿å…è¿è¡Œæ—¶å‡ºç°FileNotFoundError
# 3. æå‡ç”¨æˆ·ä½“éªŒï¼ˆè‡ªåŠ¨é…ç½®ï¼‰

# å¦‚æœä¸åˆ›å»ºä¼šæ€æ ·ï¼Ÿ
# åœºæ™¯ï¼šä¿å­˜ç¼“å­˜æ–‡ä»¶
cache_path = "experiment_cache/abc123.json"
with open(cache_path, 'w') as f:  # FileNotFoundError!
    json.dump(data, f)

# os.makedirs vs os.mkdir
# os.mkdir('a/b/c')
# - åªåˆ›å»ºæœ€åä¸€çº§ç›®å½•
# - çˆ¶ç›®å½•ä¸å­˜åœ¨æ—¶æŠ¥é”™

# os.makedirs('a/b/c')ï¼ˆæœ¬ä»£ç ä½¿ç”¨ï¼‰
# - é€’å½’åˆ›å»ºæ‰€æœ‰ç›®å½•
# - è‡ªåŠ¨åˆ›å»ºaã€a/bã€a/b/c

# æ›´å®‰å…¨çš„å†™æ³•
for folder in [EXPERIMENT_CACHE_DIR, EXPERIMENT_RESULTS_DIR]:
    os.makedirs(folder, exist_ok=True)
    # exist_ok=Trueï¼šç›®å½•å·²å­˜åœ¨æ—¶ä¸æŠ¥é”™

# é¡¹ç›®ç›®å½•ç»“æ„
# paper_tool_3.0/
# â”œâ”€â”€ app.py
# â”œâ”€â”€ experiment_extractor.py
# â”œâ”€â”€ experiment_cache/        â† è‡ªåŠ¨åˆ›å»º
# â”‚   â”œâ”€â”€ abc123....json
# â”‚   â”œâ”€â”€ def456....json
# â”‚   â””â”€â”€ ...
# â””â”€â”€ experiment_results/       â† è‡ªåŠ¨åˆ›å»º
#     â”œâ”€â”€ query_20240115.json
#     â””â”€â”€ ...
```

---

## ğŸ” æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼

### å®éªŒç« èŠ‚æ¨¡å¼ï¼ˆç¬¬40-51è¡Œï¼‰

```python
# å®šä¹‰å¸¸è§çš„å®éªŒéƒ¨åˆ†å…³é”®è¯æ­£åˆ™è¡¨è¾¾å¼   # ç¬¬40è¡Œï¼ˆæ³¨é‡Šï¼‰
EXPERIMENT_SECTION_PATTERNS = [          # ç¬¬41è¡Œ
    r"(?:4|IV|4\.0)[\s\.]*(?:Experiment|Evaluation|Results)",  # ç¬¬42è¡Œ
    r"(?:5|V|5\.0)[\s\.]*(?:Experiment|Evaluation|Results)",   # ç¬¬43è¡Œ
    r"(?:6|VI|6\.0)[\s\.]*(?:Experiment|Evaluation|Results)",  # ç¬¬44è¡Œ
    r"(?:Experiment|Evaluation)[\s\w]*(?:Setup|Design)",       # ç¬¬45è¡Œ
    r"(?:Performance|Experimental)[\s\w]*(?:Evaluation|Results)",  # ç¬¬46è¡Œ
    r"Experiment(?:s|al)",                                     # ç¬¬47è¡Œ
    r"Result(?:s)",                                            # ç¬¬48è¡Œ
    r"Evaluation",                                             # ç¬¬49è¡Œ
    r"Benchmark"                                               # ç¬¬50è¡Œ
]
```

**ç¬¬41è¡Œè¯¦è§£**ï¼š`EXPERIMENT_SECTION_PATTERNS = [`

**æ­£åˆ™æ¨¡å¼åˆ—è¡¨è¯¦è§£ï¼ˆç¬¬41è¡Œï¼‰**ï¼š
- `EXPERIMENT_SECTION_PATTERNS`ï¼šå¸¸é‡å
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `[...]`ï¼šPythonåˆ—è¡¨

```python
EXPERIMENT_SECTION_PATTERNS = [...]

# è¿™ä¸ªåˆ—è¡¨åŒ…å«ä»€ä¹ˆï¼Ÿ
# 9ç§æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œç”¨äºåŒ¹é…è®ºæ–‡ä¸­çš„å®éªŒç« èŠ‚æ ‡é¢˜

# ä¸ºä»€ä¹ˆéœ€è¦å¤šç§æ¨¡å¼ï¼Ÿ
# å› ä¸ºä¸åŒè®ºæ–‡çš„ç« èŠ‚ç¼–å·å’Œå‘½åæ–¹å¼ä¸åŒï¼š
# - "4. Experiment"
# - "IV. Evaluation"
# - "4.0 Results"
# - "Section 5: Experiments"
# - "Experimental Setup"
# ç­‰ç­‰...

# ä½¿ç”¨æ–¹å¼
text = "4. Experiment and Results"
is_experiment_section = False

for pattern in EXPERIMENT_SECTION_PATTERNS:
    if re.search(pattern, text, re.IGNORECASE):
        is_experiment_section = True
        print(f"åŒ¹é…æ¨¡å¼ï¼š{pattern}")
        break

if is_experiment_section:
    print("è¿™æ˜¯å®éªŒç« èŠ‚ï¼")
```

**ç¬¬42è¡Œè¯¦è§£**ï¼šç« èŠ‚ç¼–å·æ¨¡å¼

```python
r"(?:4|IV|4\.0)[\s\.]*(?:Experiment|Evaluation|Results)"

# æ­£åˆ™è¡¨è¾¾å¼è¯¦ç»†è§£æ
# r"..." - åŸå§‹å­—ç¬¦ä¸²ï¼ˆr = raw stringï¼‰
# ä¼˜ç‚¹ï¼šä¸éœ€è¦è½¬ä¹‰åæ–œæ 

# æ™®é€šå­—ç¬¦ä¸² vs åŸå§‹å­—ç¬¦ä¸²
normal_string = "\\d+\\.\\d+"  # éœ€è¦åŒé‡è½¬ä¹‰
raw_string = r"\d+\.\d+"       # ä¸éœ€è¦è½¬ä¹‰ï¼ˆæ¨èï¼‰

# æ¨¡å¼ç»“æ„åˆ†è§£
pattern = r"(?:4|IV|4\.0)[\s\.]*(?:Experiment|Evaluation|Results)"

# ç¬¬ä¸€éƒ¨åˆ†ï¼š(?:4|IV|4\.0)
# (?:...)    - éæ•è·ç»„ï¼ˆåªåˆ†ç»„ï¼Œä¸ä¿å­˜åŒ¹é…ç»“æœï¼‰
# 4|IV|4\.0  - åŒ¹é…"4"æˆ–"IV"æˆ–"4.0"
# |          - æˆ–è¿ç®—ç¬¦
# \.         - è½¬ä¹‰çš„ç‚¹å·ï¼ˆåŒ¹é…å­—é¢çš„"."ï¼‰

# ç¬¬äºŒéƒ¨åˆ†ï¼š[\s\.]*
# [...]      - å­—ç¬¦é›†ï¼ˆåŒ¹é…é›†åˆä¸­çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼‰
# \s         - ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç­‰ï¼‰
# \.         - ç‚¹å·
# *          - 0æ¬¡æˆ–å¤šæ¬¡

# ç¬¬ä¸‰éƒ¨åˆ†ï¼š(?:Experiment|Evaluation|Results)
# (?:...)    - éæ•è·ç»„
# Experiment|Evaluation|Results - åŒ¹é…è¿™ä¸‰ä¸ªè¯ä¹‹ä¸€

# å®Œæ•´åŒ¹é…ç¤ºä¾‹
test_strings = [
    "4. Experiment",              # âœ“ åŒ¹é…ï¼š4 + . + Experiment
    "4.0 Experimental Results",   # âœ“ åŒ¹é…ï¼š4.0 + ç©ºæ ¼ + Experiment
    "IV. Evaluation",             # âœ“ åŒ¹é…ï¼šIV + . + Evaluation
    "4   Results",                # âœ“ åŒ¹é…ï¼š4 + ç©ºæ ¼ + Results
    "Section 4. Experiment",      # âœ“ åŒ¹é…ï¼ˆåŒ…å«4. Experimentï¼‰
    "3. Experiment",              # âœ— ä¸åŒ¹é…ï¼ˆ3ä¸åœ¨æ¨¡å¼ä¸­ï¼‰
    "4. Method"                   # âœ— ä¸åŒ¹é…ï¼ˆMethodä¸åœ¨æ¨¡å¼ä¸­ï¼‰
]

pattern = r"(?:4|IV|4\.0)[\s\.]*(?:Experiment|Evaluation|Results)"
for text in test_strings:
    match = re.search(pattern, text, re.IGNORECASE)
    status = "âœ“ åŒ¹é…" if match else "âœ— ä¸åŒ¹é…"
    print(f"{status}: {text}")

# re.IGNORECASEæ ‡å¿—
# ä½œç”¨ï¼šå¿½ç•¥å¤§å°å†™
# ä¾‹å¦‚ï¼š"experiment"ã€"EXPERIMENT"ã€"Experiment"éƒ½èƒ½åŒ¹é…

# ä¸ºä»€ä¹ˆç”¨éæ•è·ç»„(?:...)è€Œä¸æ˜¯æ™®é€šåˆ†ç»„(...)ï¼Ÿ
# æ™®é€šåˆ†ç»„ï¼š
pattern = r"(4|IV|4\.0)[\s\.]*(Experiment|Evaluation|Results)"
match = re.search(pattern, text)
if match:
    print(match.group(0))  # å®Œæ•´åŒ¹é…
    print(match.group(1))  # ç¬¬ä¸€ä¸ªåˆ†ç»„
    print(match.group(2))  # ç¬¬äºŒä¸ªåˆ†ç»„

# éæ•è·ç»„ï¼ˆæœ¬ä»£ç ä½¿ç”¨ï¼‰ï¼š
pattern = r"(?:4|IV|4\.0)[\s\.]*(?:Experiment|Evaluation|Results)"
match = re.search(pattern, text)
if match:
    print(match.group(0))  # å®Œæ•´åŒ¹é…
    # æ²¡æœ‰group(1)å’Œgroup(2)

# ä¼˜åŠ¿ï¼š
# 1. æ€§èƒ½æ›´å¥½ï¼ˆä¸ä¿å­˜åˆ†ç»„ï¼‰
# 2. ä»£ç æ›´æ¸…æ™°ï¼ˆåªéœ€è¦çŸ¥é“æ˜¯å¦åŒ¹é…ï¼Œä¸éœ€è¦æå–éƒ¨åˆ†ï¼‰
```

**ç¬¬45è¡Œè¯¦è§£**ï¼šç»„åˆè¯æ¨¡å¼

```python
r"(?:Experiment|Evaluation)[\s\w]*(?:Setup|Design)"

# æ¨¡å¼è§£æ
# (?:Experiment|Evaluation)  - åŒ¹é…"Experiment"æˆ–"Evaluation"
# [\s\w]*                    - 0ä¸ªæˆ–å¤šä¸ªç©ºç™½æˆ–å­—æ¯æ•°å­—
# (?:Setup|Design)           - åŒ¹é…"Setup"æˆ–"Design"

# åŒ¹é…ç¤ºä¾‹
# "Experiment Setup"         âœ“
# "Experimental Setup"       âœ“ï¼ˆExperiment + al + ç©ºæ ¼ + Setupï¼‰
# "Evaluation Design"        âœ“
# "Experiment and Setup"     âœ“ï¼ˆExperiment + ç©ºæ ¼andç©ºæ ¼ + Setupï¼‰
# "Experimental Design"      âœ“

# [\s\w]*çš„ä½œç”¨
# \s - ç©ºç™½å­—ç¬¦
# \w - å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼ˆword characterï¼‰
# * - 0æ¬¡æˆ–å¤šæ¬¡

# å¯ä»¥åŒ¹é…çš„ä¸­é—´éƒ¨åˆ†ï¼š
# "" - ç©ºï¼ˆç›´æ¥è¿æ¥ï¼‰
# " " - å•ä¸ªç©ºæ ¼
# "al " - å­—æ¯åŠ ç©ºæ ¼
# " and " - è¯è¯­åŠ ç©ºæ ¼

test_strings = [
    "Experiment Setup",
    "Experimental Setup",
    "Experiment and Results Setup",
    "Evaluation Design"
]

pattern = r"(?:Experiment|Evaluation)[\s\w]*(?:Setup|Design)"
for text in test_strings:
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        print(f"âœ“ {text} â†’ åŒ¹é…ï¼š{match.group()}")
```

**ç¬¬47-50è¡Œè¯¦è§£**ï¼šç®€å•æ¨¡å¼

```python
r"Experiment(?:s|al)",         # ç¬¬47è¡Œ
r"Result(?:s)",                # ç¬¬48è¡Œ
r"Evaluation",                 # ç¬¬49è¡Œ
r"Benchmark"                   # ç¬¬50è¡Œ

# ç¬¬47è¡Œï¼šr"Experiment(?:s|al)"
# åŒ¹é…ï¼š
# - "Experiment"
# - "Experiments"ï¼ˆåŠ sï¼‰
# - "Experimental"ï¼ˆåŠ alï¼‰
# (?:s|al) - åŒ¹é…"s"æˆ–"al"ï¼ˆå¯é€‰ï¼‰

# ç¬¬48è¡Œï¼šr"Result(?:s)"
# åŒ¹é…ï¼š
# - "Result"
# - "Results"
# (?:s) - åŒ¹é…"s"ï¼ˆå¯é€‰ï¼‰

# å¯é€‰éƒ¨åˆ†çš„è¯­æ³•
# (?:s|al)  - åŒ¹é…"s"æˆ–"al"
# (?:s)?    - åŒ¹é…0ä¸ªæˆ–1ä¸ª"s"ï¼ˆç­‰ä»·äºä¸Šé¢ï¼Œä½†åªæœ‰ä¸€ä¸ªé€‰é¡¹æ—¶æ›´ç®€æ´ï¼‰

# ç¤ºä¾‹å¯¹æ¯”
pattern1 = r"Result(?:s)"      # åŒ¹é…Resultæˆ–Results
pattern2 = r"Results?"         # ç­‰ä»·ï¼Œ?è¡¨ç¤ºå‰é¢çš„så¯é€‰

# ç¬¬49-50è¡Œï¼šç®€å•çš„å•è¯åŒ¹é…
# "Evaluation"å’Œ"Benchmark"ç›´æ¥åŒ¹é…è¿™äº›è¯

# ä¸ºä»€ä¹ˆæœ‰äº›æ¨¡å¼è¿™ä¹ˆç®€å•ï¼Ÿ
# ç­–ç•¥ï¼šä»ä¸¥æ ¼åˆ°å®½æ¾
# 1. å…ˆå°è¯•ä¸¥æ ¼åŒ¹é…ï¼ˆå¦‚"4. Experiment"ï¼‰
# 2. å¦‚æœæ²¡åŒ¹é…ä¸Šï¼Œå†ç”¨å®½æ¾åŒ¹é…ï¼ˆå¦‚"Experiment"ï¼‰
# 3. æé«˜å¬å›ç‡ï¼ˆæ‰¾åˆ°æ›´å¤šç›¸å…³å†…å®¹ï¼‰

# å®é™…åº”ç”¨
def find_experiment_sections(text):
    """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾æ‰€æœ‰å®éªŒç« èŠ‚"""
    sections = []
    
    for pattern in EXPERIMENT_SECTION_PATTERNS:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            sections.append({
                'text': match.group(),
                'start': match.start(),
                'end': match.end(),
                'pattern': pattern
            })
    
    return sections

# ä½¿ç”¨ç¤ºä¾‹
paper_text = """
1. Introduction
2. Related Work
3. Methodology
4. Experiment and Results
   4.1 Experimental Setup
   4.2 Benchmark Results
5. Evaluation
6. Conclusion
"""

sections = find_experiment_sections(paper_text)
for section in sections:
    print(f"æ‰¾åˆ°ï¼š{section['text']} (ä½ç½®ï¼š{section['start']})")
```

### å¯¹æ¯”ç®—æ³•æ¨¡å¼ï¼ˆç¬¬53-63è¡Œï¼‰

```python
# å¯¹æ¯”ç®—æ³•å…³é”®è¯                         # ç¬¬53è¡Œï¼ˆæ³¨é‡Šï¼‰
ALGORITHM_PATTERNS = [                   # ç¬¬54è¡Œ
    r"compar(?:e|ed|ing|ison)\s+(?:with|to|against)",  # ç¬¬55è¡Œ
    r"baseline(?:s)?",                   # ç¬¬56è¡Œ
    r"state-of-the-art",                 # ç¬¬57è¡Œ
    r"benchmark(?:s|ed|ing)?",           # ç¬¬58è¡Œ
    r"(?:previous|existing)\s+(?:method|approach)(?:s)?",  # ç¬¬59è¡Œ
    r"competitor(?:s)?",                 # ç¬¬60è¡Œ
    r"method(?:s)?\s+(?:include|used)",  # ç¬¬61è¡Œ
    r"algorithm(?:s)?"                   # ç¬¬62è¡Œ
]
```

**ç¬¬55è¡Œè¯¦è§£**ï¼šcompareè¯æ—æ¨¡å¼

```python
r"compar(?:e|ed|ing|ison)\s+(?:with|to|against)"

# æ¨¡å¼è§£æ
# compar             - åŒ¹é…"compar"ï¼ˆcompareçš„è¯å¹²ï¼‰
# (?:e|ed|ing|ison) - åŒ¹é…ä¸åŒçš„è¯å°¾
# \s+                - 1ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦
# (?:with|to|against) - åŒ¹é…ä»‹è¯

# å¯ä»¥åŒ¹é…çš„å®Œæ•´çŸ­è¯­
matches = [
    "compare with",      # compar + e + ç©ºæ ¼ + with
    "compared to",       # compar + ed + ç©ºæ ¼ + to
    "comparing against", # compar + ing + ç©ºæ ¼ + against
    "comparison with"    # compar + ison + ç©ºæ ¼ + with
]

# ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ
# è‹±è¯­ä¸­compareæœ‰å¤šç§å½¢å¼ï¼š
# - compare (åŸå½¢)
# - compared (è¿‡å»å¼/è¿‡å»åˆ†è¯)
# - comparing (ç°åœ¨åˆ†è¯/åŠ¨åè¯)
# - comparison (åè¯)

# å®é™…è®ºæ–‡ä¾‹å¥
examples = [
    "We compare our method with three baselines.",
    "The proposed algorithm is compared to existing approaches.",
    "When comparing against state-of-the-art methods...",
    "In comparison with previous work..."
]

pattern = r"compar(?:e|ed|ing|ison)\s+(?:with|to|against)"
for example in examples:
    match = re.search(pattern, example, re.IGNORECASE)
    if match:
        print(f"âœ“ åŒ¹é…ï¼š{match.group()}")
        print(f"  åŸå¥ï¼š{example}")

# è¾“å‡ºï¼š
# âœ“ åŒ¹é…ï¼šcompare our method with
#   åŸå¥ï¼šWe compare our method with three baselines.
# âœ“ åŒ¹é…ï¼šcompared to
#   åŸå¥ï¼šThe proposed algorithm is compared to existing approaches.
# ...

# \s+çš„ä½œç”¨
# \s  - ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç­‰ï¼‰
# +   - 1ä¸ªæˆ–å¤šæ¬¡ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªç©ºæ ¼ï¼‰

# ä¸ºä»€ä¹ˆç”¨\s+è€Œä¸æ˜¯å•ä¸ªç©ºæ ¼' 'ï¼Ÿ
# åŸå› ï¼š
# 1. å¯èƒ½æœ‰å¤šä¸ªè¿ç»­ç©ºæ ¼
# 2. å¯èƒ½æ˜¯åˆ¶è¡¨ç¬¦
# 3. å¯èƒ½æœ‰æ¢è¡Œç¬¦ï¼ˆè·¨è¡Œï¼‰

# ç¤ºä¾‹
text1 = "compare  with"    # ä¸¤ä¸ªç©ºæ ¼
text2 = "compare\twith"    # åˆ¶è¡¨ç¬¦
text3 = "compare\nwith"    # æ¢è¡Œç¬¦

# \s+éƒ½èƒ½åŒ¹é…ï¼Œè€Œ' 'åªèƒ½åŒ¹é…å•ä¸ªç©ºæ ¼
```

**ç¬¬56-58è¡Œè¯¦è§£**ï¼šåŸºå‡†è¯æ¨¡å¼

```python
r"baseline(?:s)?",                   # ç¬¬56è¡Œ
r"state-of-the-art",                 # ç¬¬57è¡Œ
r"benchmark(?:s|ed|ing)?",           # ç¬¬58è¡Œ

# ç¬¬56è¡Œï¼šbaselineæ¨¡å¼
# baseline  - åŒ¹é…"baseline"
# (?:s)?    - å¯é€‰çš„"s"ï¼ˆå¤æ•°ï¼‰
# åŒ¹é…ï¼š
# - "baseline"
# - "baselines"

# ç¬¬57è¡Œï¼šstate-of-the-artæ¨¡å¼
# åŒ¹é…å›ºå®šçŸ­è¯­"state-of-the-art"
# æ³¨æ„ï¼šè¿å­—ç¬¦-æ˜¯å­—é¢å­—ç¬¦ï¼Œä¸éœ€è¦è½¬ä¹‰

# ç¬¬58è¡Œï¼šbenchmarkæ¨¡å¼
# benchmark          - åŒ¹é…"benchmark"
# (?:s|ed|ing)?      - å¯é€‰çš„è¯å°¾
# åŒ¹é…ï¼š
# - "benchmark"
# - "benchmarks"ï¼ˆå¤æ•°ï¼‰
# - "benchmarked"ï¼ˆè¿‡å»å¼ï¼‰
# - "benchmarking"ï¼ˆç°åœ¨åˆ†è¯ï¼‰

# å®é™…ä½¿ç”¨
def extract_baseline_mentions(text):
    """æå–æ–‡æœ¬ä¸­æåˆ°çš„åŸºå‡†æ–¹æ³•"""
    baselines = []
    
    # æŸ¥æ‰¾baseline
    for match in re.finditer(r"baseline(?:s)?", text, re.IGNORECASE):
        baselines.append(match.group())
    
    # æŸ¥æ‰¾state-of-the-art
    for match in re.finditer(r"state-of-the-art", text, re.IGNORECASE):
        baselines.append(match.group())
    
    # æŸ¥æ‰¾benchmark
    for match in re.finditer(r"benchmark(?:s|ed|ing)?", text, re.IGNORECASE):
        baselines.append(match.group())
    
    return baselines

# æµ‹è¯•
paper_excerpt = """
We compared our approach with three baselines and state-of-the-art methods.
The proposed algorithm was benchmarked against existing benchmarks.
"""

baselines = extract_baseline_mentions(paper_excerpt)
print("æ‰¾åˆ°çš„åŸºå‡†è¯ï¼š", baselines)
# ['baselines', 'state-of-the-art', 'benchmarked', 'benchmarks']
```

**ç¬¬59è¡Œè¯¦è§£**ï¼šç°æœ‰æ–¹æ³•æ¨¡å¼

```python
r"(?:previous|existing)\s+(?:method|approach)(?:s)?"

# æ¨¡å¼è§£æ
# (?:previous|existing)  - åŒ¹é…"previous"æˆ–"existing"
# \s+                    - 1ä¸ªæˆ–å¤šä¸ªç©ºç™½
# (?:method|approach)    - åŒ¹é…"method"æˆ–"approach"
# (?:s)?                 - å¯é€‰çš„å¤æ•°"s"

# å¯ä»¥åŒ¹é…çš„çŸ­è¯­
phrases = [
    "previous method",
    "previous methods",
    "existing method",
    "existing methods",
    "previous approach",
    "previous approaches",
    "existing approach",
    "existing approaches"
]

# ä¸ºä»€ä¹ˆè¿™æ ·ç»„åˆï¼Ÿ
# è®ºæ–‡ä¸­å¸¸è§çš„è¡¨è¾¾ï¼š
# - "compared with previous methods"
# - "unlike existing approaches"
# - "improves upon existing method"

# å®é™…ä¾‹å¥
examples = [
    "Our method outperforms previous methods.",
    "Unlike existing approaches, we propose...",
    "The algorithm improves upon existing method.",
    "Compared with previous approach..."
]

pattern = r"(?:previous|existing)\s+(?:method|approach)(?:s)?"
for example in examples:
    match = re.search(pattern, example, re.IGNORECASE)
    if match:
        print(f"âœ“ {example}")
        print(f"  åŒ¹é…ï¼š{match.group()}")
```

### ä»£ç é“¾æ¥æ¨¡å¼ï¼ˆç¬¬65-78è¡Œï¼‰

```python
# ä»£ç é“¾æ¥å…³é”®è¯                         # ç¬¬65è¡Œï¼ˆæ³¨é‡Šï¼‰
CODE_LINK_PATTERNS = [                   # ç¬¬66è¡Œ
    r"code(?:\s+is)?\s+available",       # ç¬¬67è¡Œ
    r"implementation(?:\s+is)?\s+available",  # ç¬¬68è¡Œ
    r"source\s+code",                    # ç¬¬69è¡Œ
    r"github\.com",                      # ç¬¬70è¡Œ
    r"gitlab\.com",                      # ç¬¬71è¡Œ
    r"code\s+repository",                # ç¬¬72è¡Œ
    r"open-source",                      # ç¬¬73è¡Œ
    r"public(?:ly)?\s+available",        # ç¬¬74è¡Œ
    r"software\s+package",               # ç¬¬75è¡Œ
    r"available\s+at\s+https?://",       # ç¬¬76è¡Œ
    r"release(?:d)?\s+at\s+https?://"    # ç¬¬77è¡Œ
]
```

**ç¬¬67-68è¡Œè¯¦è§£**ï¼šå¯ç”¨æ€§è¡¨è¿°æ¨¡å¼

```python
r"code(?:\s+is)?\s+available",           # ç¬¬67è¡Œ
r"implementation(?:\s+is)?\s+available",  # ç¬¬68è¡Œ

# ç¬¬67è¡Œæ¨¡å¼è§£æ
# code          - åŒ¹é…"code"
# (?:\s+is)?    - å¯é€‰çš„" is"
# \s+           - 1ä¸ªæˆ–å¤šä¸ªç©ºç™½
# available     - åŒ¹é…"available"

# å¯ä»¥åŒ¹é…çš„çŸ­è¯­
matches_67 = [
    "code available",         # code + ç©ºæ ¼ + available
    "code is available",      # code + ç©ºæ ¼is + available
    "Code  is  available"     # å¤šä¸ªç©ºæ ¼ä¹Ÿèƒ½åŒ¹é…
]

# ä¸ºä»€ä¹ˆ(?:\s+is)?æ˜¯å¯é€‰çš„ï¼Ÿ
# å› ä¸ºè®ºæ–‡ä¸­å¯èƒ½æœ‰ä¸¤ç§è¡¨è¿°ï¼š
# 1. "Our code is available at..."
# 2. "Code available at..."

# ç¬¬68è¡Œï¼šimplementationæ¨¡å¼
# ç»“æ„ä¸ç¬¬67è¡Œç›¸åŒï¼Œåªæ˜¯å°†"code"æ¢æˆ"implementation"

# å®é™…è®ºæ–‡ä¾‹å¥
examples = [
    "The code is available at GitHub.",
    "Code available at https://github.com/...",
    "Our implementation is available online.",
    "Implementation available at our repository."
]

patterns = [
    r"code(?:\s+is)?\s+available",
    r"implementation(?:\s+is)?\s+available"
]

for example in examples:
    for pattern in patterns:
        match = re.search(pattern, example, re.IGNORECASE)
        if match:
            print(f"âœ“ {example}")
            print(f"  åŒ¹é…ï¼š{match.group()}")
            break
```

**ç¬¬70-71è¡Œè¯¦è§£**ï¼šåŸŸåæ¨¡å¼

```python
r"github\.com",                      # ç¬¬70è¡Œ
r"gitlab\.com",                      # ç¬¬71è¡Œ

# æ¨¡å¼è§£æ
# github\.com
# - github: å­—é¢åŒ¹é…"github"
# - \.    : è½¬ä¹‰çš„ç‚¹å·ï¼ˆåŒ¹é…å­—é¢çš„"."ï¼‰
# - com   : å­—é¢åŒ¹é…"com"

# ä¸ºä»€ä¹ˆè¦è½¬ä¹‰ç‚¹å·ï¼Ÿ
# åœ¨æ­£åˆ™è¡¨è¾¾å¼ä¸­ï¼š
# .   - åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆç‰¹æ®Šå«ä¹‰ï¼‰
# \.  - åŒ¹é…å­—é¢çš„ç‚¹å·"."

# ç¤ºä¾‹
pattern1 = r"github.com"   # é”™è¯¯ï¼.åŒ¹é…ä»»æ„å­—ç¬¦
# ä¼šåŒ¹é…ï¼š
# - "githubXcom"
# - "github!com"
# - "github.com"

pattern2 = r"github\.com"  # æ­£ç¡®ï¼åªåŒ¹é…å­—é¢çš„"."
# åªåŒ¹é…ï¼š
# - "github.com"

# å®é™…åº”ç”¨ï¼šæå–GitHubé“¾æ¥
def extract_github_links(text):
    """ä»æ–‡æœ¬ä¸­æå–GitHubé“¾æ¥"""
    # åŒ¹é…å®Œæ•´çš„GitHub URL
    pattern = r"https?://github\.com/[\w-]+/[\w-]+"
    matches = re.findall(pattern, text)
    return matches

# æµ‹è¯•
text = """
Code available at https://github.com/user/repo1
Also see https://github.com/another-user/cool-project
And https://gitlab.com/user/repo2
"""

github_links = extract_github_links(text)
print("GitHubé“¾æ¥ï¼š", github_links)
# ['https://github.com/user/repo1', 
#  'https://github.com/another-user/cool-project']

# æ›´å®Œå–„çš„URLæå–
def extract_code_links(text):
    """æå–æ‰€æœ‰ä»£ç ä»“åº“é“¾æ¥"""
    links = []
    
    # GitHubé“¾æ¥
    github_pattern = r"https?://github\.com/[\w-]+/[\w-]+"
    links.extend(re.findall(github_pattern, text))
    
    # GitLabé“¾æ¥
    gitlab_pattern = r"https?://gitlab\.com/[\w-]+/[\w-]+"
    links.extend(re.findall(gitlab_pattern, text))
    
    return links
```

**ç¬¬76-77è¡Œè¯¦è§£**ï¼šURLæ¨¡å¼

```python
r"available\s+at\s+https?://",       # ç¬¬76è¡Œ
r"release(?:d)?\s+at\s+https?://"    # ç¬¬77è¡Œ

# ç¬¬76è¡Œæ¨¡å¼è§£æ
# available    - åŒ¹é…"available"
# \s+          - 1ä¸ªæˆ–å¤šä¸ªç©ºç™½
# at           - åŒ¹é…"at"
# \s+          - 1ä¸ªæˆ–å¤šä¸ªç©ºç™½
# https?://    - åŒ¹é…"http://"æˆ–"https://"

# https?://è¯¦è§£
# http         - åŒ¹é…"http"
# s?           - å¯é€‰çš„"s"ï¼ˆ0ä¸ªæˆ–1ä¸ªï¼‰
# ://          - åŒ¹é…"://"

# å¯ä»¥åŒ¹é…ï¼š
# - "http://"
# - "https://"

# ç¬¬77è¡Œï¼šreleaseæ¨¡å¼
# release(?:d)?  - åŒ¹é…"release"æˆ–"released"
# å…¶ä½™éƒ¨åˆ†ä¸ç¬¬76è¡Œç›¸åŒ

# å®é™…ä¾‹å¥
examples = [
    "Code available at https://github.com/user/repo",
    "Available at http://example.com/code",
    "The code is released at https://gitlab.com/project",
    "Released at http://code.example.com"
]

patterns = [
    r"available\s+at\s+https?://",
    r"release(?:d)?\s+at\s+https?://"
]

for example in examples:
    for pattern in patterns:
        match = re.search(pattern, example, re.IGNORECASE):
        if match:
            print(f"âœ“ {example}")
            print(f"  åŒ¹é…ï¼š{match.group()}")
            
            # æå–URLçš„å¼€å§‹ä½ç½®
            url_start = match.end()
            print(f"  URLä»ä½ç½®{url_start}å¼€å§‹")
            break

# å®Œæ•´çš„URLæå–
def extract_full_url(text, pattern):
    """æå–å®Œæ•´çš„URL"""
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        # æ‰¾åˆ°"http://"æˆ–"https://"çš„ä½ç½®
        url_start = text.find("http", match.start())
        
        # ä»è¿™ä¸ªä½ç½®å¼€å§‹æå–URLï¼ˆåˆ°ç©ºç™½æˆ–å¥å·ä¸ºæ­¢ï¼‰
        url_match = re.match(r"https?://[^\s]+", text[url_start:])
        if url_match:
            return url_match.group()
    
    return None

# æµ‹è¯•
text = "Code available at https://github.com/user/repo for download."
url = extract_full_url(text, r"available\s+at\s+https?://")
print(f"æå–çš„URLï¼š{url}")
# è¾“å‡ºï¼šhttps://github.com/user/repo
```

---

## ğŸ’¾ ç¼“å­˜ç³»ç»Ÿ

### ç¼“å­˜è·¯å¾„ç”Ÿæˆï¼ˆç¬¬179-184è¡Œï¼‰

```python
def get_cache_path(paper_key):           # ç¬¬179è¡Œ
    """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""                  # ç¬¬180è¡Œ
    # æ·»åŠ ç‰ˆæœ¬å·åˆ°å“ˆå¸Œä¸­                    # ç¬¬181è¡Œ
    versioned_key = f"{CACHE_VERSION}_{paper_key}"  # ç¬¬182è¡Œ
    cache_file = hashlib.md5(versioned_key.encode()).hexdigest() + ".json"  # ç¬¬183è¡Œ
    return os.path.join(EXPERIMENT_CACHE_DIR, cache_file)  # ç¬¬184è¡Œ
```

**å®Œæ•´å‡½æ•°è¯¦è§£**ï¼š

```python
def get_cache_path(paper_key):
    """
    æ ¹æ®è®ºæ–‡æ ‡è¯†ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
    
    å‚æ•°:
        paper_key: è®ºæ–‡çš„å”¯ä¸€æ ‡è¯†ï¼ˆé€šå¸¸æ˜¯URLæˆ–DOIï¼‰
    
    è¿”å›:
        ç¼“å­˜æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    """
    # æ­¥éª¤1ï¼šåˆ›å»ºç‰ˆæœ¬åŒ–çš„é”®
    versioned_key = f"{CACHE_VERSION}_{paper_key}"
    # ä¾‹å¦‚ï¼š"v2_https://arxiv.org/abs/2024.12345"
    
    # æ­¥éª¤2ï¼šç”ŸæˆMD5å“ˆå¸Œ
    hash_value = hashlib.md5(versioned_key.encode()).hexdigest()
    # ä¾‹å¦‚ï¼š"a1b2c3d4e5f6789abcdef0123456789"
    
    # æ­¥éª¤3ï¼šæ·»åŠ .jsonæ‰©å±•å
    cache_file = hash_value + ".json"
    # ä¾‹å¦‚ï¼š"a1b2c3d4e5f6789abcdef0123456789.json"
    
    # æ­¥éª¤4ï¼šæ‹¼æ¥å®Œæ•´è·¯å¾„
    cache_path = os.path.join(EXPERIMENT_CACHE_DIR, cache_file)
    # ä¾‹å¦‚ï¼š"experiment_cache/a1b2c3d4e5f6789abcdef0123456789.json"
    
    return cache_path

# ä½¿ç”¨ç¤ºä¾‹
paper_url = "https://arxiv.org/abs/2024.12345"
cache_path = get_cache_path(paper_url)
print(f"ç¼“å­˜è·¯å¾„ï¼š{cache_path}")

# ä¸åŒçš„paper_keyç”Ÿæˆä¸åŒçš„è·¯å¾„
urls = [
    "https://arxiv.org/abs/2024.001",
    "https://arxiv.org/abs/2024.002",
    "https://ieeexplore.ieee.org/document/12345"
]

for url in urls:
    path = get_cache_path(url)
    print(f"{url[:40]}... â†’ {path[-20:]}")

# è¾“å‡ºï¼š
# https://arxiv.org/abs/2024.001... â†’ ...abc123.json
# https://arxiv.org/abs/2024.002... â†’ ...def456.json
# https://ieeexplore.ieee.org/docum... â†’ ...789xyz.json

# ä¸ºä»€ä¹ˆä½¿ç”¨å“ˆå¸Œè€Œä¸æ˜¯ç›´æ¥ç”¨URLï¼Ÿ
# é—®é¢˜1ï¼šURLåŒ…å«ç‰¹æ®Šå­—ç¬¦
url = "https://example.com/paper?id=123&version=2"
# ç›´æ¥ç”¨ä½œæ–‡ä»¶åä¼šæœ‰é—®é¢˜ï¼š
# - ?å’Œ&æ˜¯éæ³•æ–‡ä»¶åå­—ç¬¦
# - Windowsä¸å…è®¸æŸäº›å­—ç¬¦

# é—®é¢˜2ï¼šURLå¯èƒ½å¾ˆé•¿
url = "https://dl.acm.org/doi/abs/10.1145/1234567890.0987654321?param=value"
# å¤ªé•¿çš„æ–‡ä»¶åä¸æ–¹ä¾¿ç®¡ç†

# ä½¿ç”¨å“ˆå¸Œçš„ä¼˜åŠ¿ï¼š
# 1. å›ºå®šé•¿åº¦ï¼ˆ32å­—ç¬¦ï¼‰
# 2. åªåŒ…å«[0-9a-f]ï¼Œåˆæ³•
# 3. å”¯ä¸€æ€§ï¼ˆä¸åŒURLäº§ç”Ÿä¸åŒå“ˆå¸Œï¼‰
# 4. ç¡®å®šæ€§ï¼ˆç›¸åŒURLæ€»æ˜¯äº§ç”Ÿç›¸åŒå“ˆå¸Œï¼‰
```

è¿™ä»½æ–‡æ¡£ç°åœ¨å·²ç»åˆ›å»ºå®Œæˆï¼æˆ‘ä¸ºæ‚¨è¯¦ç»†è®²è§£äº†ï¼š

1. âœ… **å¯¼å…¥è¯­å¥**ï¼ˆ10ä¸ªæ¨¡å—ï¼‰- æ¯ä¸ªéƒ½æœ‰è¯¦ç»†ç¤ºä¾‹
2. âœ… **å…¨å±€é…ç½®**ï¼ˆç¼“å­˜ç‰ˆæœ¬ã€ç›®å½•åˆå§‹åŒ–ï¼‰
3. âœ… **æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼**ï¼ˆå®éªŒç« èŠ‚ã€å¯¹æ¯”ç®—æ³•ã€ä»£ç é“¾æ¥ï¼‰
4. âœ… **ç¼“å­˜ç³»ç»Ÿ**ï¼ˆè·¯å¾„ç”Ÿæˆï¼‰

æ–‡æ¡£ç‰¹ç‚¹ï¼š
- ğŸ“ é€è¡Œä»£ç è§£æ
- ğŸ”‘ å…³é”®å­—è¯¦ç»†è®²è§£
- ğŸ’¡ ä¸°å¯Œçš„ä»£ç ç¤ºä¾‹
- ğŸ“Š å¯¹æ¯”è¯´æ˜
- ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

æ–‡æ¡£å·²ä¿å­˜åœ¨ `learn/ExperimentExtractor_å®éªŒæå–è¯¦è§£.md`ï¼

ç”±äºæ–‡ä»¶å¾ˆé•¿ï¼ˆ1266è¡Œï¼‰ï¼Œæˆ‘é‡ç‚¹è®²è§£äº†æœ€æ ¸å¿ƒçš„å‰200è¡Œå†…å®¹ã€‚å¦‚æœæ‚¨éœ€è¦æˆ‘ç»§ç»­è®²è§£åç»­éƒ¨åˆ†ï¼ˆå¦‚ç¼“å­˜è¯»å†™å‡½æ•°ã€æå–ç®—æ³•ç­‰ï¼‰ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼


