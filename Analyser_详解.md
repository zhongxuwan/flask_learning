# Analyser.py è®ºæ–‡æ‘˜è¦åˆ†æå™¨è¯¦è§£

## ğŸ“‹ ç›®å½•

1. [æ¨¡å—æ¦‚è¿°](#æ¨¡å—æ¦‚è¿°)
2. [å¯¼å…¥è¯­å¥è¯¦è§£](#å¯¼å…¥è¯­å¥è¯¦è§£)
3. [å…¨å±€å¸¸é‡é…ç½®](#å…¨å±€å¸¸é‡é…ç½®)
4. [Cacheç¼“å­˜ç±»](#cacheç¼“å­˜ç±»)
5. [AdvancedSessionå¢å¼ºä¼šè¯ç±»](#advancedsessionå¢å¼ºä¼šè¯ç±»)
6. [Analyseræ ¸å¿ƒåˆ†æç±»](#analyseræ ¸å¿ƒåˆ†æç±»)
7. [æ‘˜è¦è·å–ç­–ç•¥](#æ‘˜è¦è·å–ç­–ç•¥)
8. [ä»£ç†ç®¡ç†æœºåˆ¶](#ä»£ç†ç®¡ç†æœºåˆ¶)
9. [é”™è¯¯å¤„ç†ä¸é‡è¯•](#é”™è¯¯å¤„ç†ä¸é‡è¯•)
10. [æŠ€æœ¯æ€»ç»“](#æŠ€æœ¯æ€»ç»“)

---

## ğŸ“– æ¨¡å—æ¦‚è¿°

`Analyser.py` æ˜¯è®ºæ–‡æ‘˜è¦æ™ºèƒ½æå–å™¨ï¼Œè´Ÿè´£ä»å„ç§å­¦æœ¯ç½‘ç«™è‡ªåŠ¨è·å–è®ºæ–‡æ‘˜è¦ä¿¡æ¯ã€‚å®ƒé‡‡ç”¨äº†å¤šé‡ç­–ç•¥ã€æ™ºèƒ½ç¼“å­˜ã€ä»£ç†ç®¡ç†ç­‰æŠ€æœ¯ï¼Œç¡®ä¿é«˜æˆåŠŸç‡å’Œç¨³å®šæ€§ã€‚

### æ ¸å¿ƒåŠŸèƒ½

- **æ™ºèƒ½æ‘˜è¦æå–**: ä»å¤šä¸ªå­¦æœ¯å‡ºç‰ˆå•†ç½‘ç«™æå–è®ºæ–‡æ‘˜è¦
- **å¤šæºå¤‡é€‰æœºåˆ¶**: æ”¯æŒ10+ä¸ªå­¦æœ¯APIæ•°æ®æº
- **æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ**: æœ¬åœ°ç¼“å­˜é¿å…é‡å¤è¯·æ±‚
- **ä»£ç†ç®¡ç†**: è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ä»£ç†ï¼Œåº”å¯¹åçˆ¬è™«æœºåˆ¶
- **å®¹é”™é‡è¯•**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œè‡ªåŠ¨é‡è¯•æœºåˆ¶

---

## ğŸ“¦ å¯¼å…¥è¯­å¥è¯¦è§£

### æ ‡å‡†åº“å¯¼å…¥ï¼ˆç¬¬1-2è¡Œï¼‰

```python
import random  # ç¬¬1è¡Œ
import time    # ç¬¬2è¡Œ
```

**é€è¡Œä»£ç è¯¦è§£**ï¼š

**ç¬¬1è¡Œ**ï¼š`import random`

**randomæ¨¡å—è¯¦è§£ï¼ˆç¬¬1è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå…³é”®å­—ï¼Œç”¨äºå¯¼å…¥æ¨¡å—
- `random`ï¼šPythonæ ‡å‡†åº“æ¨¡å—ï¼Œæä¾›éšæœºæ•°ç”ŸæˆåŠŸèƒ½
- æœ¬æ¨¡å—ç”¨é€”ï¼š
  1. éšæœºé€‰æ‹©HTTPè¯·æ±‚å¤´ï¼ˆé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«ï¼‰
  2. ç”Ÿæˆéšæœºå»¶è¿Ÿæ—¶é—´ï¼ˆæ¨¡æ‹Ÿäººç±»è®¿é—®è¡Œä¸ºï¼‰
  3. éšæœºé€‰æ‹©ä»£ç†æœåŠ¡å™¨

**randomæ¨¡å—å¸¸ç”¨æ–¹æ³•**ï¼š
```python
random.choice([1, 2, 3])      # ä»åˆ—è¡¨ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå…ƒç´ 
random.uniform(2.0, 5.0)      # ç”Ÿæˆ2.0åˆ°5.0ä¹‹é—´çš„éšæœºæµ®ç‚¹æ•°
random.randint(1, 10)         # ç”Ÿæˆ1åˆ°10ä¹‹é—´çš„éšæœºæ•´æ•°
random.shuffle([1, 2, 3])     # éšæœºæ‰“ä¹±åˆ—è¡¨é¡ºåº
```

**ç¬¬2è¡Œ**ï¼š`import time`

**timeæ¨¡å—è¯¦è§£ï¼ˆç¬¬2è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå…³é”®å­—ï¼Œå¯¼å…¥æ•´ä¸ªæ¨¡å—
- `time`ï¼šPythonæ ‡å‡†åº“æ¨¡å—ï¼Œæä¾›æ—¶é—´ç›¸å…³åŠŸèƒ½
- æœ¬æ¨¡å—ç”¨é€”ï¼š
  1. `time.sleep()`ï¼šæš‚åœç¨‹åºæ‰§è¡Œï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
  2. `time.time()`ï¼šè·å–å½“å‰æ—¶é—´æˆ³ï¼ˆç”¨äºè®¿é—®é—´éš”æ§åˆ¶ï¼‰
  3. è®¡ç®—è®¿é—®é¢‘ç‡ï¼Œé˜²æ­¢è¢«å°ç¦

**timeæ¨¡å—å¸¸ç”¨æ–¹æ³•**ï¼š
```python
time.time()          # è¿”å›å½“å‰æ—¶é—´æˆ³ï¼ˆç§’ï¼‰ï¼š1699999999.123456
time.sleep(2)        # æš‚åœç¨‹åº2ç§’
current = time.time()
# ä½¿ç”¨ç¤ºä¾‹ï¼šæ§åˆ¶è®¿é—®é—´éš”
if current - last_access < 5:
    time.sleep(5 - (current - last_access))
```

### ç±»å‹æç¤ºå¯¼å…¥ï¼ˆç¬¬3è¡Œï¼‰

```python
from typing import Optional, Dict, Callable, Tuple, Any, List  # ç¬¬3è¡Œ
```

**ç¬¬3è¡Œè¯¦è§£**ï¼š`from typing import Optional, Dict, Callable, Tuple, Any, List`

**typingæ¨¡å—è¯¦è§£ï¼ˆç¬¬3è¡Œï¼‰**ï¼š
- `from typing import`ï¼šä»typingæ¨¡å—å¯¼å…¥ç±»å‹æç¤ºå·¥å…·
- `typing`ï¼šPythonæ ‡å‡†åº“ï¼Œæä¾›ç±»å‹æ³¨è§£æ”¯æŒï¼ˆPython 3.5+ï¼‰
- ç±»å‹æç¤ºä½œç”¨ï¼š
  1. **ä»£ç å¯è¯»æ€§**ï¼šè®©å…¶ä»–å¼€å‘è€…ä¸€çœ¼çœ‹æ‡‚å‚æ•°å’Œè¿”å›å€¼ç±»å‹
  2. **IDEæ”¯æŒ**ï¼šä»£ç è¡¥å…¨å’Œç±»å‹æ£€æŸ¥
  3. **æ–‡æ¡£åŒ–**ï¼šè‡ªåŠ¨ç”Ÿæˆæ›´æ¸…æ™°çš„APIæ–‡æ¡£
  4. **é¿å…é”™è¯¯**ï¼šåœ¨å¼€å‘é˜¶æ®µå‘ç°ç±»å‹é”™è¯¯

**å¯¼å…¥çš„ç±»å‹è¯´æ˜**ï¼š

| ç±»å‹ | å«ä¹‰ | ä½¿ç”¨ç¤ºä¾‹ |
|------|------|---------|
| `Optional[str]` | å¯é€‰çš„å­—ç¬¦ä¸²ç±»å‹ï¼Œå¯ä»¥æ˜¯stræˆ–None | `def get_name() -> Optional[str]:` |
| `Dict[str, str]` | å­—å…¸ç±»å‹ï¼Œé”®å’Œå€¼éƒ½æ˜¯å­—ç¬¦ä¸² | `headers: Dict[str, str] = {"User-Agent": "..."}` |
| `Callable[[str], str]` | å¯è°ƒç”¨å¯¹è±¡ï¼ˆå‡½æ•°ï¼‰ï¼Œæ¥å—strå‚æ•°è¿”å›str | `func: Callable[[str], str]` |
| `Tuple[bool, int, int]` | å…ƒç»„ç±»å‹ï¼ŒåŒ…å«å¸ƒå°”å€¼å’Œä¸¤ä¸ªæ•´æ•° | `def check() -> Tuple[bool, int, int]:` |
| `Any` | ä»»æ„ç±»å‹ | `data: Any = get_data()` |
| `List[Dict]` | åˆ—è¡¨ç±»å‹ï¼ŒåŒ…å«å­—å…¸å…ƒç´  | `sources: List[Dict[str, Any]]` |

**ç±»å‹æç¤ºç¤ºä¾‹**ï¼š
```python
# æ²¡æœ‰ç±»å‹æç¤º
def process_user(name, age):
    return f"{name} is {age} years old"

# æœ‰ç±»å‹æç¤ºï¼ˆæ¨èï¼‰
def process_user(name: str, age: int) -> str:
    return f"{name} is {age} years old"

# Optionalç±»å‹ç¤ºä¾‹
def find_abstract(url: str) -> Optional[str]:
    """å¯èƒ½æ‰¾åˆ°æ‘˜è¦ï¼ˆè¿”å›strï¼‰ï¼Œä¹Ÿå¯èƒ½æ‰¾ä¸åˆ°ï¼ˆè¿”å›Noneï¼‰"""
    if url:
        return "Abstract text"
    return None

# Dictç±»å‹ç¤ºä¾‹
headers: Dict[str, str] = {
    "User-Agent": "Mozilla/5.0",
    "Accept": "text/html"
}

# Callableç±»å‹ç¤ºä¾‹
def apply_rule(rule: Callable[[str], Optional[str]], html: str) -> Optional[str]:
    """ruleæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å—strå‚æ•°ï¼Œè¿”å›Optional[str]"""
    return rule(html)
```

### ç½‘ç»œè¯·æ±‚ç›¸å…³å¯¼å…¥ï¼ˆç¬¬4-7è¡Œï¼‰

```python
from requests.adapters import HTTPAdapter  # ç¬¬4è¡Œ
from urllib3.util.retry import Retry       # ç¬¬5è¡Œ
import requests                            # ç¬¬6è¡Œ
from urllib.parse import urlparse          # ç¬¬7è¡Œ
```

**ç¬¬4è¡Œè¯¦è§£**ï¼š`from requests.adapters import HTTPAdapter`

**HTTPAdapterè¯¦è§£ï¼ˆç¬¬4è¡Œï¼‰**ï¼š
- `from requests.adapters import`ï¼šä»requestsåº“çš„adaptersæ¨¡å—å¯¼å…¥
- `HTTPAdapter`ï¼šHTTPé€‚é…å™¨ç±»ï¼Œç”¨äºé…ç½®requestsä¼šè¯çš„é«˜çº§åŠŸèƒ½
- ä¸»è¦ä½œç”¨ï¼š
  1. é…ç½®è¿æ¥æ± å¤§å°
  2. è®¾ç½®é‡è¯•ç­–ç•¥
  3. ç®¡ç†è¿æ¥å¤ç”¨
  4. ä¼˜åŒ–ç½‘ç»œæ€§èƒ½

**HTTPAdapterä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from requests.adapters import HTTPAdapter
from requests import Session

# åˆ›å»ºä¼šè¯
session = Session()

# åˆ›å»ºé€‚é…å™¨ï¼ˆé…ç½®é‡è¯•ï¼‰
adapter = HTTPAdapter(max_retries=3)

# å°†é€‚é…å™¨æŒ‚è½½åˆ°ä¼šè¯
session.mount('http://', adapter)   # HTTPè¯·æ±‚ä½¿ç”¨æ­¤é€‚é…å™¨
session.mount('https://', adapter)  # HTTPSè¯·æ±‚ä½¿ç”¨æ­¤é€‚é…å™¨

# ç°åœ¨ä½¿ç”¨sessionå‘é€è¯·æ±‚ï¼Œä¼šè‡ªåŠ¨é‡è¯•
response = session.get('https://example.com')
```

**ç¬¬5è¡Œè¯¦è§£**ï¼š`from urllib3.util.retry import Retry`

**Retryé‡è¯•ç­–ç•¥è¯¦è§£ï¼ˆç¬¬5è¡Œï¼‰**ï¼š
- `from urllib3.util.retry import`ï¼šä»urllib3åº“å¯¼å…¥é‡è¯•å·¥å…·
- `Retry`ï¼šé‡è¯•ç­–ç•¥é…ç½®ç±»
- é…ç½®é‡è¯•è¡Œä¸ºï¼š
  1. é‡è¯•æ¬¡æ•°é™åˆ¶
  2. å“ªäº›HTTPçŠ¶æ€ç éœ€è¦é‡è¯•
  3. é‡è¯•é—´éš”æ—¶é—´ï¼ˆé€€é¿ç­–ç•¥ï¼‰
  4. å…è®¸é‡è¯•çš„HTTPæ–¹æ³•

**Retryé…ç½®ç¤ºä¾‹**ï¼š
```python
from urllib3.util.retry import Retry

# åˆ›å»ºé‡è¯•ç­–ç•¥
retry_strategy = Retry(
    total=5,                        # æ€»é‡è¯•æ¬¡æ•°
    backoff_factor=1.0,             # é€€é¿å› å­ï¼ˆé‡è¯•é—´éš”å€æ•°ï¼‰
    status_forcelist=[429, 500, 502, 503, 504],  # éœ€è¦é‡è¯•çš„çŠ¶æ€ç 
    allowed_methods=["GET", "POST"] # å…è®¸é‡è¯•çš„HTTPæ–¹æ³•
)

# é€€é¿å› å­å·¥ä½œåŸç†ï¼š
# ç¬¬1æ¬¡é‡è¯•ï¼šç­‰å¾… backoff_factor * 2^0 = 1ç§’
# ç¬¬2æ¬¡é‡è¯•ï¼šç­‰å¾… backoff_factor * 2^1 = 2ç§’
# ç¬¬3æ¬¡é‡è¯•ï¼šç­‰å¾… backoff_factor * 2^2 = 4ç§’
# ç¬¬4æ¬¡é‡è¯•ï¼šç­‰å¾… backoff_factor * 2^3 = 8ç§’
```

**ç¬¬6è¡Œè¯¦è§£**ï¼š`import requests`

**requestsåº“è¯¦è§£ï¼ˆç¬¬6è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå…³é”®å­—ï¼Œå¯¼å…¥æ•´ä¸ªæ¨¡å—
- `requests`ï¼šç¬¬ä¸‰æ–¹HTTPåº“ï¼Œæä¾›ç®€æ´çš„HTTPè¯·æ±‚æ¥å£
- æ˜¯Pythonæœ€æµè¡Œçš„HTTPåº“ï¼Œæ¯”å†…ç½®çš„urllibæ›´æ˜“ç”¨
- ä¸»è¦åŠŸèƒ½ï¼š
  1. å‘é€HTTPè¯·æ±‚ï¼ˆGETã€POSTç­‰ï¼‰
  2. å¤„ç†å“åº”æ•°æ®
  3. ç®¡ç†ä¼šè¯å’ŒCookie
  4. å¤„ç†è®¤è¯å’Œä»£ç†

**requestsåŸºæœ¬ç”¨æ³•**ï¼š
```python
import requests

# GETè¯·æ±‚
response = requests.get('https://api.github.com')
print(response.status_code)  # 200
print(response.json())       # è‡ªåŠ¨è§£æJSON

# POSTè¯·æ±‚
data = {'key': 'value'}
response = requests.post('https://api.example.com', json=data)

# è®¾ç½®è¯·æ±‚å¤´
headers = {'User-Agent': 'My App'}
response = requests.get('https://example.com', headers=headers)

# ä½¿ç”¨ä»£ç†
proxies = {'http': 'http://127.0.0.1:7890'}
response = requests.get('https://example.com', proxies=proxies)

# è®¾ç½®è¶…æ—¶
response = requests.get('https://example.com', timeout=10)
```

**ç¬¬7è¡Œè¯¦è§£**ï¼š`from urllib.parse import urlparse`

**urlparse URLè§£æè¯¦è§£ï¼ˆç¬¬7è¡Œï¼‰**ï¼š
- `from urllib.parse import`ï¼šä»urllibæ ‡å‡†åº“çš„parseæ¨¡å—å¯¼å…¥
- `urlparse`ï¼šURLè§£æå‡½æ•°ï¼Œå°†URLå­—ç¬¦ä¸²åˆ†è§£ä¸ºå„ä¸ªç»„æˆéƒ¨åˆ†
- è¿”å›ParseResultå¯¹è±¡ï¼ŒåŒ…å«schemeã€netlocã€pathç­‰å±æ€§

**urlparseä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from urllib.parse import urlparse

url = "https://www.example.com:8080/path/to/page?query=123#section"
parsed = urlparse(url)

print(parsed.scheme)    # 'https' - åè®®
print(parsed.netloc)    # 'www.example.com:8080' - ç½‘ç»œä½ç½®ï¼ˆåŸŸå+ç«¯å£ï¼‰
print(parsed.hostname)  # 'www.example.com' - ä¸»æœºå
print(parsed.port)      # 8080 - ç«¯å£å·
print(parsed.path)      # '/path/to/page' - è·¯å¾„
print(parsed.query)     # 'query=123' - æŸ¥è¯¢å­—ç¬¦ä¸²
print(parsed.fragment)  # 'section' - é”šç‚¹

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨ï¼šæå–åŸŸå
def get_domain(url: str) -> str:
    parsed = urlparse(url)
    # å»é™¤wwwå‰ç¼€ï¼Œç»Ÿä¸€åŸŸåæ ¼å¼
    return parsed.netloc.lower().lstrip("www.")

# ç¤ºä¾‹
get_domain("https://www.ieee.org/articles/123")  # "ieee.org"
get_domain("http://dl.acm.org/paper/456")        # "dl.acm.org"
```

### é¡¹ç›®å†…éƒ¨æ¨¡å—å¯¼å…¥ï¼ˆç¬¬8-9è¡Œï¼‰

```python
from Rule import Rule                      # ç¬¬8è¡Œ
from datetime import datetime, timedelta   # ç¬¬9è¡Œ
```

**ç¬¬8è¡Œè¯¦è§£**ï¼š`from Rule import Rule`

**Ruleæ¨¡å—å¯¼å…¥è¯¦è§£ï¼ˆç¬¬8è¡Œï¼‰**ï¼š
- `from Rule import`ï¼šä»é¡¹ç›®ä¸­çš„Rule.pyæ–‡ä»¶å¯¼å…¥
- `Rule`ï¼šç½‘ç«™è§£æè§„åˆ™ç±»ï¼Œå®šä¹‰äº†å„ä¸ªå­¦æœ¯ç½‘ç«™çš„æ‘˜è¦æå–è§„åˆ™
- Ruleç±»åŒ…å«çš„é™æ€æ–¹æ³•ï¼š
  - `Rule.ieee_rule()` - IEEEç½‘ç«™è§£æè§„åˆ™
  - `Rule.acm_rule()` - ACMç½‘ç«™è§£æè§„åˆ™
  - `Rule.springer_rule()` - Springerç½‘ç«™è§£æè§„åˆ™
  - ç­‰ç­‰...

**Ruleç±»ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from Rule import Rule

# IEEEç½‘ç«™çš„HTMLå†…å®¹
ieee_html = """<div class="abstract-text">This paper presents...</div>"""

# ä½¿ç”¨IEEEè§„åˆ™æå–æ‘˜è¦
abstract = Rule.ieee_rule(ieee_html)
print(abstract)  # "This paper presents..."

# ACMç½‘ç«™çš„HTMLå†…å®¹
acm_html = """<div class="abstractSection">We propose...</div>"""
abstract = Rule.acm_rule(acm_html)
print(abstract)  # "We propose..."
```

**ç¬¬9è¡Œè¯¦è§£**ï¼š`from datetime import datetime, timedelta`

**datetimeæ¨¡å—è¯¦è§£ï¼ˆç¬¬9è¡Œï¼‰**ï¼š
- `from datetime import`ï¼šä»datetimeæ ‡å‡†åº“æ¨¡å—å¯¼å…¥
- `datetime`ï¼šæ—¥æœŸæ—¶é—´ç±»ï¼Œè¡¨ç¤ºå…·ä½“çš„æ—¥æœŸå’Œæ—¶é—´
- `timedelta`ï¼šæ—¶é—´é—´éš”ç±»ï¼Œè¡¨ç¤ºä¸¤ä¸ªæ—¶é—´ç‚¹ä¹‹é—´çš„å·®å€¼

**datetimeå’Œtimedeltaä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from datetime import datetime, timedelta

# datetimeç±» - è¡¨ç¤ºå…·ä½“æ—¶é—´
now = datetime.now()                    # å½“å‰æ—¶é—´ï¼š2024-01-15 14:30:25
print(now.year)                         # 2024
print(now.month)                        # 1
print(now.day)                          # 15
print(now.hour)                         # 14

# æ ¼å¼åŒ–è¾“å‡º
print(now.strftime("%Y-%m-%d %H:%M:%S"))  # "2024-01-15 14:30:25"

# timedeltaç±» - è¡¨ç¤ºæ—¶é—´é—´éš”
one_week = timedelta(days=7)            # 7å¤©çš„æ—¶é—´é—´éš”
one_hour = timedelta(hours=1)           # 1å°æ—¶çš„æ—¶é—´é—´éš”
five_mins = timedelta(minutes=5)        # 5åˆ†é’Ÿçš„æ—¶é—´é—´éš”

# æ—¶é—´è®¡ç®—
last_week = now - one_week              # ä¸€å‘¨å‰çš„æ—¶é—´
tomorrow = now + timedelta(days=1)      # æ˜å¤©çš„æ—¶é—´
deadline = now + timedelta(hours=24)    # 24å°æ—¶å

# æ—¶é—´æ¯”è¾ƒ
cached_time = datetime.now() - timedelta(days=3)
cache_duration = timedelta(days=7)

if datetime.now() - cached_time <= cache_duration:
    print("ç¼“å­˜ä»ç„¶æœ‰æ•ˆ")  # 3å¤© <= 7å¤©ï¼Œæ‰€ä»¥æœ‰æ•ˆ
else:
    print("ç¼“å­˜å·²è¿‡æœŸ")

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨
class Cache:
    def __init__(self):
        self.cache_duration = timedelta(days=7)  # ç¼“å­˜7å¤©æœ‰æ•ˆ
    
    def is_valid(self, timestamp):
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦åœ¨æœ‰æ•ˆæœŸå†…
        return datetime.now() - timestamp <= self.cache_duration
```

### æ•°æ®å¤„ç†ç›¸å…³å¯¼å…¥ï¼ˆç¬¬10-17è¡Œï¼‰

```python
from bs4 import BeautifulSoup          # ç¬¬10è¡Œ
from collections import defaultdict    # ç¬¬11è¡Œ
import json                            # ç¬¬12è¡Œ
import os                              # ç¬¬13è¡Œ
from pathlib import Path               # ç¬¬14è¡Œ
import hashlib                         # ç¬¬15è¡Œ
import pickle                          # ç¬¬16è¡Œ
import re                              # ç¬¬17è¡Œ
```

**ç¬¬10è¡Œè¯¦è§£**ï¼š`from bs4 import BeautifulSoup`

**BeautifulSoup HTMLè§£æè¯¦è§£ï¼ˆç¬¬10è¡Œï¼‰**ï¼š
- `from bs4 import`ï¼šä»BeautifulSoup4åº“å¯¼å…¥ï¼ˆbs4æ˜¯åŒ…åï¼‰
- `BeautifulSoup`ï¼šHTML/XMLè§£æç±»ï¼Œç”¨äºè§£æå’Œæå–ç½‘é¡µå†…å®¹
- ä¸»è¦åŠŸèƒ½ï¼š
  1. è§£æHTMLæ–‡æ¡£
  2. ä½¿ç”¨CSSé€‰æ‹©å™¨æŸ¥æ‰¾å…ƒç´ 
  3. æå–æ ‡ç­¾å†…å®¹å’Œå±æ€§
  4. éå†å’Œä¿®æ”¹DOMæ ‘

**BeautifulSoupä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from bs4 import BeautifulSoup

html = """
<html>
    <head><title>è®ºæ–‡æ ‡é¢˜</title></head>
    <body>
        <div class="abstract">
            <p>è¿™æ˜¯æ‘˜è¦å†…å®¹...</p>
        </div>
        <meta name="description" content="è®ºæ–‡æè¿°">
    </body>
</html>
"""

# åˆ›å»ºBeautifulSoupå¯¹è±¡
soup = BeautifulSoup(html, 'lxml')  # 'lxml'æ˜¯è§£æå™¨

# æ–¹æ³•1: ä½¿ç”¨CSSé€‰æ‹©å™¨
abstract_div = soup.select_one('div.abstract')  # é€‰æ‹©class="abstract"çš„div
print(abstract_div.text)  # "è¿™æ˜¯æ‘˜è¦å†…å®¹..."

# æ–¹æ³•2: ä½¿ç”¨findæ–¹æ³•
meta_tag = soup.find('meta', attrs={'name': 'description'})
print(meta_tag.get('content'))  # "è®ºæ–‡æè¿°"

# æ–¹æ³•3: å¤šä¸ªé€‰æ‹©å™¨
elements = soup.select('p, div')  # é€‰æ‹©æ‰€æœ‰på’Œdivæ ‡ç­¾

# æ–¹æ³•4: å±æ€§åŒ…å«åŒ¹é…
abstract_elements = soup.select('[class*="abstract"]')  # classåŒ…å«"abstract"

# ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
for script in soup.find_all('script'):
    script.decompose()  # åˆ é™¤scriptæ ‡ç­¾

# æœ¬é¡¹ç›®ä¸­çš„å®é™…åº”ç”¨
def extract_ieee_abstract(html):
    soup = BeautifulSoup(html, 'lxml')
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        'meta[property="twitter:description"]',
        'div.abstract-text',
        'div.abstract-container'
    ]
    for selector in selectors:
        element = soup.select_one(selector)
        if element:
            if element.name == 'meta':
                return element.get('content')
            else:
                return element.get_text().strip()
    return None
```

**ç¬¬11è¡Œè¯¦è§£**ï¼š`from collections import defaultdict`

**defaultdicté»˜è®¤å­—å…¸è¯¦è§£ï¼ˆç¬¬11è¡Œï¼‰**ï¼š
- `from collections import`ï¼šä»collectionsæ ‡å‡†åº“æ¨¡å—å¯¼å…¥
- `defaultdict`ï¼šé»˜è®¤å­—å…¸ç±»ï¼Œæä¾›é»˜è®¤å€¼çš„å­—å…¸
- ç‰¹ç‚¹ï¼šè®¿é—®ä¸å­˜åœ¨çš„é”®æ—¶ï¼Œè‡ªåŠ¨åˆ›å»ºå¹¶è¿”å›é»˜è®¤å€¼
- é¿å…äº†KeyErrorå¼‚å¸¸

**defaultdictä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from collections import defaultdict

# æ™®é€šå­—å…¸çš„é—®é¢˜
normal_dict = {}
# print(normal_dict['key'])  # KeyError: 'key'

# éœ€è¦å…ˆæ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
if 'key' not in normal_dict:
    normal_dict['key'] = 0
normal_dict['key'] += 1

# defaultdictçš„ä¼˜åŠ¿
count_dict = defaultdict(int)  # é»˜è®¤å€¼ä¸º0
count_dict['key'] += 1  # è‡ªåŠ¨åˆå§‹åŒ–ä¸º0ï¼Œç„¶å+1
print(count_dict['key'])  # 1

# ä¸åŒçš„é»˜è®¤å€¼ç±»å‹
int_dict = defaultdict(int)        # é»˜è®¤å€¼: 0
list_dict = defaultdict(list)      # é»˜è®¤å€¼: []
set_dict = defaultdict(set)        # é»˜è®¤å€¼: set()
float_dict = defaultdict(float)    # é»˜è®¤å€¼: 0.0

# ä½¿ç”¨ç¤ºä¾‹ï¼šç»Ÿè®¡è®¿é—®æ¬¡æ•°
access_counts = defaultdict(int)
access_counts['Aminer'] += 1  # ç¬¬ä¸€æ¬¡è®¿é—®ï¼Œè‡ªåŠ¨åˆå§‹åŒ–ä¸º0å†+1
access_counts['Aminer'] += 1  # ç¬¬äºŒæ¬¡è®¿é—®
print(access_counts['Aminer'])  # 2

# ä½¿ç”¨ç¤ºä¾‹ï¼šåˆ†ç»„
students_by_class = defaultdict(list)
students_by_class['Class A'].append('Alice')
students_by_class['Class A'].append('Bob')
students_by_class['Class B'].append('Charlie')
print(students_by_class)
# {'Class A': ['Alice', 'Bob'], 'Class B': ['Charlie']}

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨
_access_counts = defaultdict(int)      # è®°å½•æ¯ä¸ªæ•°æ®æºçš„è®¿é—®æ¬¡æ•°
_last_access_time = defaultdict(float) # è®°å½•æ¯ä¸ªæ•°æ®æºçš„æœ€åè®¿é—®æ—¶é—´
```

**ç¬¬12è¡Œè¯¦è§£**ï¼š`import json`

**JSONæ¨¡å—è¯¦è§£ï¼ˆç¬¬12è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå…³é”®å­—ï¼Œå¯¼å…¥æ•´ä¸ªæ¨¡å—
- `json`ï¼šPythonæ ‡å‡†åº“ï¼Œç”¨äºå¤„ç†JSONæ ¼å¼æ•°æ®
- ä¸»è¦åŠŸèƒ½ï¼š
  1. å°†Pythonå¯¹è±¡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼ˆåºåˆ—åŒ–ï¼‰
  2. å°†JSONå­—ç¬¦ä¸²è§£æä¸ºPythonå¯¹è±¡ï¼ˆååºåˆ—åŒ–ï¼‰
  3. è¯»å†™JSONæ–‡ä»¶

**jsonæ¨¡å—ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
import json

# 1. å°†Pythonå¯¹è±¡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
data = {
    "name": "è®ºæ–‡æ ‡é¢˜",
    "authors": ["å¼ ä¸‰", "æå››"],
    "year": 2024,
    "cited": 10
}
json_str = json.dumps(data, ensure_ascii=False, indent=2)
print(json_str)
# {
#   "name": "è®ºæ–‡æ ‡é¢˜",
#   "authors": ["å¼ ä¸‰", "æå››"],
#   "year": 2024,
#   "cited": 10
# }

# 2. å°†JSONå­—ç¬¦ä¸²è§£æä¸ºPythonå¯¹è±¡
json_text = '{"title": "AI Research", "year": 2024}'
parsed_data = json.loads(json_text)
print(parsed_data['title'])  # "AI Research"

# 3. å†™å…¥JSONæ–‡ä»¶
with open('data.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

# 4. è¯»å–JSONæ–‡ä»¶
with open('data.json', 'r', encoding='utf-8') as f:
    loaded_data = json.load(f)

# 5. å¤„ç†APIå“åº”ï¼ˆæœ¬é¡¹ç›®ä¸­çš„åº”ç”¨ï¼‰
response = requests.get('https://api.example.com/papers')
if response.status_code == 200:
    data = response.json()  # è‡ªåŠ¨è§£æJSONå“åº”
    abstract = data.get('abstract', '')

# 6. å¤„ç†JSONDecodeErrorå¼‚å¸¸
try:
    data = json.loads(invalid_json_string)
except json.JSONDecodeError as e:
    print(f"JSONè§£æå¤±è´¥: {e}")
```

**ç¬¬13è¡Œè¯¦è§£**ï¼š`import os`

**osæ¨¡å—è¯¦è§£ï¼ˆç¬¬13è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå…³é”®å­—ï¼Œå¯¼å…¥æ•´ä¸ªæ¨¡å—
- `os`ï¼šPythonæ ‡å‡†åº“ï¼Œæä¾›æ“ä½œç³»ç»Ÿç›¸å…³åŠŸèƒ½
- ä¸»è¦åŠŸèƒ½ï¼š
  1. æ–‡ä»¶å’Œç›®å½•æ“ä½œ
  2. è·¯å¾„å¤„ç†
  3. ç¯å¢ƒå˜é‡è®¿é—®
  4. è¿›ç¨‹ç®¡ç†

**osæ¨¡å—ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
import os

# 1. ç›®å½•æ“ä½œ
os.makedirs('paper_cache', exist_ok=True)  # åˆ›å»ºç›®å½•ï¼Œå·²å­˜åœ¨ä¸æŠ¥é”™
os.listdir('.')                            # åˆ—å‡ºå½“å‰ç›®å½•å†…å®¹
os.getcwd()                                # è·å–å½“å‰å·¥ä½œç›®å½•
os.chdir('/path/to/dir')                   # åˆ‡æ¢å·¥ä½œç›®å½•

# 2. æ–‡ä»¶æ“ä½œ
os.path.exists('file.txt')                 # æ£€æŸ¥æ–‡ä»¶/ç›®å½•æ˜¯å¦å­˜åœ¨
os.path.isfile('file.txt')                 # æ˜¯å¦æ˜¯æ–‡ä»¶
os.path.isdir('folder')                    # æ˜¯å¦æ˜¯ç›®å½•
os.path.getsize('file.txt')                # è·å–æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰

# 3. è·¯å¾„æ“ä½œ
os.path.join('folder', 'file.txt')         # æ‹¼æ¥è·¯å¾„ï¼ˆè·¨å¹³å°ï¼‰
os.path.dirname('/path/to/file.txt')       # è·å–ç›®å½•éƒ¨åˆ†ï¼š'/path/to'
os.path.basename('/path/to/file.txt')      # è·å–æ–‡ä»¶åéƒ¨åˆ†ï¼š'file.txt'
os.path.splitext('file.txt')               # åˆ†ç¦»æ‰©å±•åï¼š('file', '.txt')

# 4. ç¯å¢ƒå˜é‡
os.environ.get('HOME')                     # è·å–ç¯å¢ƒå˜é‡
os.environ['MY_VAR'] = 'value'             # è®¾ç½®ç¯å¢ƒå˜é‡

# 5. æ–‡ä»¶åˆ é™¤å’Œé‡å‘½å
os.remove('file.txt')                      # åˆ é™¤æ–‡ä»¶
os.rename('old.txt', 'new.txt')            # é‡å‘½åæ–‡ä»¶

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨
cache_dir = 'paper_cache'
if not os.path.exists(cache_dir):
    os.makedirs(cache_dir)  # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
```

**ç¬¬14è¡Œè¯¦è§£**ï¼š`from pathlib import Path`

**Pathè·¯å¾„å¯¹è±¡è¯¦è§£ï¼ˆç¬¬14è¡Œï¼‰**ï¼š
- `from pathlib import`ï¼šä»pathlibæ ‡å‡†åº“æ¨¡å—å¯¼å…¥ï¼ˆPython 3.4+ï¼‰
- `Path`ï¼šé¢å‘å¯¹è±¡çš„è·¯å¾„æ“ä½œç±»ï¼Œæ¯”os.pathæ›´ç°ä»£ã€æ›´æ˜“ç”¨
- ä¼˜åŠ¿ï¼š
  1. é¢å‘å¯¹è±¡çš„APIï¼Œæ›´ç›´è§‚
  2. æ”¯æŒè·¯å¾„è¿ç®—ç¬¦ï¼ˆ/ï¼‰
  3. æ–¹æ³•é“¾å¼è°ƒç”¨
  4. è·¨å¹³å°å…¼å®¹

**Pathä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from pathlib import Path

# 1. åˆ›å»ºPathå¯¹è±¡
cache_dir = Path('paper_cache')        # ç›¸å¯¹è·¯å¾„
abs_path = Path('/home/user/data')     # ç»å¯¹è·¯å¾„
home = Path.home()                     # ç”¨æˆ·ä¸»ç›®å½•
cwd = Path.cwd()                       # å½“å‰å·¥ä½œç›®å½•

# 2. è·¯å¾„æ‹¼æ¥ï¼ˆä½¿ç”¨/è¿ç®—ç¬¦ï¼‰
file_path = cache_dir / 'paper1.pkl'   # paper_cache/paper1.pkl
subdir = cache_dir / 'subfolder' / 'file.txt'

# ç­‰ä»·çš„os.pathå†™æ³•ï¼ˆå¯¹æ¯”ï¼‰
import os
file_path_os = os.path.join('paper_cache', 'paper1.pkl')

# 3. ç›®å½•æ“ä½œ
cache_dir.mkdir(exist_ok=True)         # åˆ›å»ºç›®å½•
cache_dir.mkdir(parents=True)          # é€’å½’åˆ›å»ºçˆ¶ç›®å½•

# 4. æ–‡ä»¶æ“ä½œ
file_path.exists()                     # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
file_path.is_file()                    # æ˜¯å¦æ˜¯æ–‡ä»¶
file_path.is_dir()                     # æ˜¯å¦æ˜¯ç›®å½•
file_path.stat().st_size               # æ–‡ä»¶å¤§å°

# 5. è¯»å†™æ–‡ä»¶ï¼ˆç®€æ´æ–¹å¼ï¼‰
file_path.write_text('Hello World')    # å†™å…¥æ–‡æœ¬
content = file_path.read_text()        # è¯»å–æ–‡æœ¬
file_path.write_bytes(b'\x00\x01')     # å†™å…¥å­—èŠ‚
data = file_path.read_bytes()          # è¯»å–å­—èŠ‚

# 6. è·¯å¾„å±æ€§
file_path = Path('/home/user/paper.pdf')
file_path.name         # 'paper.pdf' - æ–‡ä»¶å
file_path.stem         # 'paper' - æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰
file_path.suffix       # '.pdf' - æ‰©å±•å
file_path.parent       # Path('/home/user') - çˆ¶ç›®å½•
file_path.parts        # ('/', 'home', 'user', 'paper.pdf') - è·¯å¾„ç»„æˆéƒ¨åˆ†

# 7. éå†ç›®å½•
for file in cache_dir.glob('*.pkl'):   # æŸ¥æ‰¾æ‰€æœ‰.pklæ–‡ä»¶
    print(file)

for file in cache_dir.rglob('*.txt'):  # é€’å½’æŸ¥æ‰¾æ‰€æœ‰.txtæ–‡ä»¶
    print(file)

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨
class Cache:
    def __init__(self, cache_dir="paper_cache"):
        self.cache_dir = Path(cache_dir)           # åˆ›å»ºPathå¯¹è±¡
        self.cache_dir.mkdir(exist_ok=True)        # ç¡®ä¿ç›®å½•å­˜åœ¨
    
    def get_cache_file(self, cache_key):
        return self.cache_dir / f"{cache_key}.pkl"  # è·¯å¾„æ‹¼æ¥
```

**ç¬¬15è¡Œè¯¦è§£**ï¼š`import hashlib`

**hashlibå“ˆå¸Œåº“è¯¦è§£ï¼ˆç¬¬15è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå…³é”®å­—ï¼Œå¯¼å…¥æ•´ä¸ªæ¨¡å—
- `hashlib`ï¼šPythonæ ‡å‡†åº“ï¼Œæä¾›å„ç§å“ˆå¸Œç®—æ³•
- ä¸»è¦åŠŸèƒ½ï¼š
  1. ç”Ÿæˆæ•°æ®çš„å“ˆå¸Œå€¼ï¼ˆMD5ã€SHA1ã€SHA256ç­‰ï¼‰
  2. ç”¨äºæ•°æ®å®Œæ•´æ€§éªŒè¯
  3. ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
  4. å¯†ç å­¦åº”ç”¨

**hashlibä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
import hashlib

# 1. MD5å“ˆå¸Œï¼ˆæœ€å¸¸ç”¨ï¼Œé€Ÿåº¦å¿«ï¼Œ128ä½ï¼‰
text = "è®ºæ–‡æ ‡é¢˜_https://example.com"
md5_hash = hashlib.md5(text.encode()).hexdigest()
print(md5_hash)  # 'a1b2c3d4e5f6...' (32ä¸ªåå…­è¿›åˆ¶å­—ç¬¦)

# 2. SHA256å“ˆå¸Œï¼ˆæ›´å®‰å…¨ï¼Œ256ä½ï¼‰
sha256_hash = hashlib.sha256(text.encode()).hexdigest()
print(sha256_hash)  # 64ä¸ªåå…­è¿›åˆ¶å­—ç¬¦

# 3. å“ˆå¸Œæ–‡ä»¶å†…å®¹
def hash_file(filepath):
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b''):
            hasher.update(chunk)
    return hasher.hexdigest()

# 4. æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨ï¼šç”Ÿæˆç¼“å­˜é”®
def get_cache_key(url: str, title: str) -> str:
    """ä½¿ç”¨URLå’Œæ ‡é¢˜ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®"""
    key = f"{url}_{title}"
    # MD5å“ˆå¸Œç¡®ä¿ï¼š
    # 1. å›ºå®šé•¿åº¦ï¼ˆ32å­—ç¬¦ï¼‰
    # 2. åˆæ³•çš„æ–‡ä»¶åï¼ˆä¸å«ç‰¹æ®Šå­—ç¬¦ï¼‰
    # 3. ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒå“ˆå¸Œ
    return hashlib.md5(key.encode()).hexdigest()

# ç¤ºä¾‹
url1 = "https://ieeexplore.ieee.org/document/12345"
title1 = "Deep Learning for Computer Vision"
cache_key1 = get_cache_key(url1, title1)
print(cache_key1)  # '7d8f9a2b3c4d5e6f...'

# ç›¸åŒçš„URLå’Œæ ‡é¢˜ï¼Œæ€»æ˜¯äº§ç”Ÿç›¸åŒçš„å“ˆå¸Œ
cache_key2 = get_cache_key(url1, title1)
print(cache_key1 == cache_key2)  # True

# ä¸åŒçš„è¾“å…¥äº§ç”Ÿä¸åŒçš„å“ˆå¸Œ
cache_key3 = get_cache_key(url1, "Different Title")
print(cache_key1 == cache_key3)  # False

# å“ˆå¸Œçš„ç‰¹æ€§ï¼š
# 1. ç¡®å®šæ€§ï¼šç›¸åŒè¾“å…¥æ€»æ˜¯äº§ç”Ÿç›¸åŒè¾“å‡º
# 2. é›ªå´©æ•ˆåº”ï¼šè¾“å…¥å¾®å°å˜åŒ–å¯¼è‡´è¾“å‡ºå®Œå…¨ä¸åŒ
# 3. å•å‘æ€§ï¼šæ— æ³•ä»å“ˆå¸Œå€¼åæ¨åŸå§‹æ•°æ®
# 4. ç¢°æ’éš¾åº¦ï¼šä¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒå“ˆå¸Œçš„æ¦‚ç‡æä½
```

**ç¬¬16è¡Œè¯¦è§£**ï¼š`import pickle`

**pickleåºåˆ—åŒ–åº“è¯¦è§£ï¼ˆç¬¬16è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå…³é”®å­—ï¼Œå¯¼å…¥æ•´ä¸ªæ¨¡å—
- `pickle`ï¼šPythonæ ‡å‡†åº“ï¼Œç”¨äºå¯¹è±¡åºåˆ—åŒ–å’Œååºåˆ—åŒ–
- ä¸»è¦åŠŸèƒ½ï¼š
  1. å°†Pythonå¯¹è±¡ä¿å­˜åˆ°æ–‡ä»¶
  2. ä»æ–‡ä»¶è¯»å–Pythonå¯¹è±¡
  3. æ”¯æŒå‡ ä¹æ‰€æœ‰Pythonæ•°æ®ç±»å‹
  4. ä¿æŒå¯¹è±¡çš„åŸå§‹ç»“æ„

**pickleä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
import pickle
from datetime import datetime

# 1. åºåˆ—åŒ–ï¼ˆä¿å­˜å¯¹è±¡ï¼‰
data = {
    'abstract': 'è¿™æ˜¯è®ºæ–‡æ‘˜è¦å†…å®¹...',
    'timestamp': datetime.now(),
    'metadata': {
        'title': 'è®ºæ–‡æ ‡é¢˜',
        'year': 2024
    }
}

# ä¿å­˜åˆ°æ–‡ä»¶
with open('cache.pkl', 'wb') as f:  # 'wb' = write binaryï¼ˆäºŒè¿›åˆ¶å†™å…¥ï¼‰
    pickle.dump(data, f)

# 2. ååºåˆ—åŒ–ï¼ˆè¯»å–å¯¹è±¡ï¼‰
with open('cache.pkl', 'rb') as f:  # 'rb' = read binaryï¼ˆäºŒè¿›åˆ¶è¯»å–ï¼‰
    loaded_data = pickle.load(f)

print(loaded_data['abstract'])      # 'è¿™æ˜¯è®ºæ–‡æ‘˜è¦å†…å®¹...'
print(loaded_data['timestamp'])     # datetimeå¯¹è±¡ï¼Œä¿æŒåŸæ ·

# 3. åºåˆ—åŒ–ä¸ºå­—èŠ‚ä¸²ï¼ˆä¸ä¿å­˜åˆ°æ–‡ä»¶ï¼‰
byte_string = pickle.dumps(data)    # è¿”å›byteså¯¹è±¡
restored_data = pickle.loads(byte_string)

# 4. æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨ï¼šç¼“å­˜ç³»ç»Ÿ
class Cache:
    def set(self, url: str, title: str, abstract: str):
        """ä¿å­˜æ‘˜è¦åˆ°ç¼“å­˜"""
        cache_key = self._get_cache_key(url, title)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        # ä¿å­˜æ‘˜è¦å’Œæ—¶é—´æˆ³
        data = {
            'abstract': abstract,
            'timestamp': datetime.now()
        }
        
        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)
    
    def get(self, url: str, title: str) -> Optional[str]:
        """ä»ç¼“å­˜è¯»å–æ‘˜è¦"""
        cache_key = self._get_cache_key(url, title)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                data = pickle.load(f)  # è¯»å–å­—å…¸å¯¹è±¡
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if datetime.now() - data['timestamp'] <= timedelta(days=7):
                    return data['abstract']
        return None

# pickle vs JSON å¯¹æ¯”ï¼š
# JSON:
#   - ä¼˜ç‚¹ï¼šè·¨è¯­è¨€ã€å¯è¯»æ€§å¥½ã€å®‰å…¨
#   - ç¼ºç‚¹ï¼šåªæ”¯æŒåŸºæœ¬æ•°æ®ç±»å‹ï¼Œä¸æ”¯æŒdatetimeç­‰å¤æ‚å¯¹è±¡
# Pickle:
#   - ä¼˜ç‚¹ï¼šæ”¯æŒæ‰€æœ‰Pythonå¯¹è±¡ã€ä¿æŒå®Œæ•´ç»“æ„
#   - ç¼ºç‚¹ï¼šåªèƒ½åœ¨Pythonä¸­ä½¿ç”¨ã€ä¸å¯è¯»ã€æœ‰å®‰å…¨é£é™©ï¼ˆä¸è¦åŠ è½½ä¸ä¿¡ä»»çš„pickleæ–‡ä»¶ï¼‰
```

**ç¬¬17è¡Œè¯¦è§£**ï¼š`import re`

**reæ­£åˆ™è¡¨è¾¾å¼æ¨¡å—è¯¦è§£ï¼ˆç¬¬17è¡Œï¼‰**ï¼š
- `import`ï¼šPythonå…³é”®å­—ï¼Œå¯¼å…¥æ•´ä¸ªæ¨¡å—
- `re`ï¼šPythonæ ‡å‡†åº“ï¼Œæä¾›æ­£åˆ™è¡¨è¾¾å¼æ”¯æŒ
- ä¸»è¦åŠŸèƒ½ï¼š
  1. æ¨¡å¼åŒ¹é…å’Œæœç´¢
  2. æ–‡æœ¬æ›¿æ¢
  3. å­—ç¬¦ä¸²åˆ†å‰²
  4. æ•°æ®æå–

**reæ¨¡å—ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
import re

# 1. åŸºæœ¬åŒ¹é…
text = "è®ºæ–‡å‘è¡¨äº2024å¹´"
match = re.search(r'\d{4}', text)  # æŸ¥æ‰¾4ä½æ•°å­—
if match:
    print(match.group())  # '2024'

# 2. æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…
text = "ä½œè€…ï¼šå¼ ä¸‰ï¼Œæå››ï¼Œç‹äº”"
authors = re.findall(r'[\u4e00-\u9fa5]{2,3}', text)  # æŸ¥æ‰¾2-3ä¸ªæ±‰å­—
print(authors)  # ['å¼ ä¸‰', 'æå››', 'ç‹äº”']

# 3. æ›¿æ¢
text = "Abstract: This paper..."
cleaned = re.sub(r'^abstract\s*[:\-\.]\s*', '', text, flags=re.IGNORECASE)
print(cleaned)  # "This paper..."

# 4. åˆ†å‰²
text = "word1, word2; word3"
words = re.split(r'[,;]\s*', text)  # æŒ‰é€—å·æˆ–åˆ†å·åˆ†å‰²
print(words)  # ['word1', 'word2', 'word3']

# 5. ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæé«˜æ•ˆç‡ï¼‰
pattern = re.compile(r'\d+')
numbers = pattern.findall("æœ‰123ä¸ªè‹¹æœå’Œ456ä¸ªæ©™å­")
print(numbers)  # ['123', '456']

# 6. æ­£åˆ™è¡¨è¾¾å¼ç¬¦å·è¯´æ˜
# \d  - æ•°å­— [0-9]
# \w  - å­—æ¯æ•°å­—ä¸‹åˆ’çº¿ [a-zA-Z0-9_]
# \s  - ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç­‰ï¼‰
# .   - ä»»æ„å­—ç¬¦ï¼ˆæ¢è¡Œç¬¦é™¤å¤–ï¼‰
# *   - 0æ¬¡æˆ–å¤šæ¬¡
# +   - 1æ¬¡æˆ–å¤šæ¬¡
# ?   - 0æ¬¡æˆ–1æ¬¡
# {n} - æ°å¥½næ¬¡
# {n,m} - nåˆ°mæ¬¡
# []  - å­—ç¬¦é›†åˆ
# ()  - åˆ†ç»„
# |   - æˆ–
# ^   - å¼€å¤´
# $   - ç»“å°¾

# 7. æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨ç¤ºä¾‹

# åº”ç”¨1: æå–æ ‡é¢˜å…³é”®è¯
title = "Deep Learning for Computer Vision: A Survey"
keywords = re.findall(r'\b\w{4,}\b', title.lower())
# æ‰¾å‡ºè‡³å°‘4ä¸ªå­—ç¬¦çš„å•è¯
print(keywords)  # ['deep', 'learning', 'computer', 'vision', 'survey']

# åº”ç”¨2: æ¸…ç†æ‘˜è¦æ–‡æœ¬
abstract = "   Abstract: This paper presents...   "
cleaned = re.sub(r'^abstract\s*[:\-\.]\s*', '', abstract.strip(), flags=re.IGNORECASE)
# ç§»é™¤å¼€å¤´çš„"Abstract:"ç­‰å‰ç¼€

# åº”ç”¨3: éªŒè¯DOIæ ¼å¼
doi = "10.1145/3517077"
is_valid = bool(re.match(r'^10\.\d{4,}/[\w\.\-]+$', doi))
print(is_valid)  # True

# åº”ç”¨4: æå–å…ƒæ•°æ®
html = '<meta name="citation_year" content="2024">'
year = re.search(r'content="(\d{4})"', html)
if year:
    print(year.group(1))  # '2024'

# åº”ç”¨5: æ™ºèƒ½æ–‡æœ¬æå–
def extract_keywords(text):
    """ä»æ–‡æœ¬ä¸­æå–4ä¸ªå­—ç¬¦ä»¥ä¸Šçš„å•è¯"""
    return set(re.findall(r'\b\w{4,}\b', text.lower()))

title = "Efficient Deep Learning"
content = "This paper proposes an efficient deep learning method..."
title_words = extract_keywords(title)
content_words = extract_keywords(content)
common_words = title_words.intersection(content_words)
print(common_words)  # {'deep', 'learning', 'efficient'}

# æ­£åˆ™è¡¨è¾¾å¼æ ‡å¿—ï¼ˆflagsï¼‰
re.IGNORECASE  # å¿½ç•¥å¤§å°å†™
re.MULTILINE   # å¤šè¡Œæ¨¡å¼
re.DOTALL      # .åŒ¹é…åŒ…æ‹¬æ¢è¡Œç¬¦åœ¨å†…çš„æ‰€æœ‰å­—ç¬¦
re.VERBOSE     # è¯¦ç»†æ¨¡å¼ï¼Œå¯ä»¥æ·»åŠ æ³¨é‡Š
```

---

## ğŸ”§ å…¨å±€å¸¸é‡é…ç½®

### HTTPè¯·æ±‚å¤´åˆ—è¡¨ï¼ˆç¬¬19-104è¡Œï¼‰

```python
# æ›´å®Œæ•´çš„è¯·æ±‚å¤´åˆ—è¡¨
HEADERS_LIST: List[Dict[str, str]] = [  # ç¬¬20è¡Œ
    {  # ç¬¬21è¡Œ - Chromeæµè§ˆå™¨è¯·æ±‚å¤´
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...",  # ç¬¬22è¡Œ
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9...",  # ç¬¬23è¡Œ
        "Accept-Language": "en-US,en;q=0.5",  # ç¬¬24è¡Œ
        "Accept-Encoding": "gzip, deflate, br",  # ç¬¬25è¡Œ
        "Connection": "keep-alive",  # ç¬¬26è¡Œ
        # ... æ›´å¤šè¯·æ±‚å¤´å­—æ®µ
    },
    # ... å…¶ä»–æµè§ˆå™¨çš„è¯·æ±‚å¤´é…ç½®
]
```

**é€è¡Œä»£ç è¯¦è§£**ï¼š

**ç¬¬20è¡Œ**ï¼š`HEADERS_LIST: List[Dict[str, str]] = [`

**ç±»å‹æ³¨è§£çš„è¯·æ±‚å¤´åˆ—è¡¨å®šä¹‰ï¼ˆç¬¬20è¡Œï¼‰**ï¼š
- `HEADERS_LIST`ï¼šå…¨å±€å¸¸é‡å˜é‡åï¼Œå…¨å¤§å†™è¡¨ç¤ºè¿™æ˜¯å¸¸é‡
- `:`ï¼šç±»å‹æ³¨è§£ç¬¦å·ï¼Œå£°æ˜å˜é‡ç±»å‹
- `List[Dict[str, str]]`ï¼šç±»å‹æ³¨è§£ï¼Œè¡¨ç¤º"å­—å…¸åˆ—è¡¨"
  - `List`ï¼šåˆ—è¡¨ç±»å‹
  - `Dict[str, str]`ï¼šå­—å…¸ç±»å‹ï¼Œé”®æ˜¯å­—ç¬¦ä¸²ï¼Œå€¼ä¹Ÿæ˜¯å­—ç¬¦ä¸²
  - å®Œæ•´å«ä¹‰ï¼šè¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå­—å…¸
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `[`ï¼šåˆ—è¡¨å¼€å§‹ç¬¦å·

**ç±»å‹æ³¨è§£è¯¦è§£**ï¼š
```python
# ç±»å‹æ³¨è§£çš„ç»“æ„åˆ†æ
List[Dict[str, str]]
â”‚    â”‚    â”‚    â””â”€ å€¼çš„ç±»å‹æ˜¯å­—ç¬¦ä¸²
â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€ é”®çš„ç±»å‹æ˜¯å­—ç¬¦ä¸²  
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å­—å…¸ç±»å‹
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆ—è¡¨ç±»å‹

# å®é™…æ•°æ®ç»“æ„ç¤ºä¾‹
HEADERS_LIST = [
    {"User-Agent": "Chrome...", "Accept": "text/html"},  # Dict[str, str]
    {"User-Agent": "Firefox...", "Accept": "text/html"}, # Dict[str, str]
    {"User-Agent": "Safari...", "Accept": "text/html"}   # Dict[str, str]
]  # List[...]

# ä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªè¯·æ±‚å¤´ï¼Ÿ
# 1. æ¨¡æ‹Ÿä¸åŒæµè§ˆå™¨ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
# 2. è½®æ¢ä½¿ç”¨ï¼Œé™ä½è¢«å°ç¦çš„é£é™©
# 3. æé«˜è¯·æ±‚æˆåŠŸç‡
```

**ç¬¬22è¡Œ**ï¼š`"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",`

**User-Agentè¯·æ±‚å¤´è¯¦è§£ï¼ˆç¬¬22è¡Œï¼‰**ï¼š
- `"User-Agent"`ï¼šHTTPè¯·æ±‚å¤´å­—æ®µåï¼Œæ ‡è¯†å®¢æˆ·ç«¯ï¼ˆæµè§ˆå™¨ï¼‰ä¿¡æ¯
- `:`ï¼šå­—å…¸é”®å€¼å¯¹åˆ†éš”ç¬¦
- `"Mozilla/5.0 ..."`ï¼šUser-Agentçš„å€¼ï¼Œæµè§ˆå™¨æ ‡è¯†å­—ç¬¦ä¸²
- `,`ï¼šå­—å…¸ä¸­ä¸‹ä¸€ä¸ªé”®å€¼å¯¹çš„åˆ†éš”ç¬¦

**User-Agentå­—ç¬¦ä¸²è§£æ**ï¼š
```python
# User-Agentå­—ç¬¦ä¸²çš„ç»„æˆéƒ¨åˆ†
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# æ‹†è§£åˆ†æï¼š
# Mozilla/5.0          - å†å²å…¼å®¹æ€§æ ‡è¯†ï¼ˆæ‰€æœ‰ç°ä»£æµè§ˆå™¨éƒ½åŒ…å«ï¼‰
# Windows NT 10.0      - æ“ä½œç³»ç»Ÿï¼šWindows 10
# Win64; x64           - 64ä½ç³»ç»Ÿæ¶æ„
# AppleWebKit/537.36   - æµè§ˆå™¨å¼•æ“ç‰ˆæœ¬
# KHTML, like Gecko    - å¼•æ“å…¼å®¹æ€§å£°æ˜
# Chrome/91.0.4472.124 - Chromeæµè§ˆå™¨ç‰ˆæœ¬
# Safari/537.36        - Safariå…¼å®¹æ€§ï¼ˆChromeåŸºäºWebKitï¼‰

# ä¸ºä»€ä¹ˆUser-Agentå¾ˆé‡è¦ï¼Ÿ
# 1. æœåŠ¡å™¨æ ¹æ®User-Agentåˆ¤æ–­å®¢æˆ·ç«¯ç±»å‹
# 2. è¿”å›é€‚åˆçš„å†…å®¹ç‰ˆæœ¬ï¼ˆç§»åŠ¨ç‰ˆ/æ¡Œé¢ç‰ˆï¼‰
# 3. ç»Ÿè®¡ç”¨æˆ·æµè§ˆå™¨ä½¿ç”¨æƒ…å†µ
# 4. åçˆ¬è™«ï¼šæ£€æµ‹å¼‚å¸¸çš„User-Agent

# çˆ¬è™«å¸¸è§é—®é¢˜ï¼š
# é”™è¯¯çš„User-Agentï¼š
requests.get(url)  # é»˜è®¤User-Agentæ˜¯"python-requests/2.x"ï¼Œå®¹æ˜“è¢«è¯†åˆ«

# æ­£ç¡®çš„åšæ³•ï¼š
headers = {"User-Agent": "Mozilla/5.0 ..."}
requests.get(url, headers=headers)  # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
```

**ç¬¬23è¡Œ**ï¼š`"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",`

**Acceptè¯·æ±‚å¤´è¯¦è§£ï¼ˆç¬¬23è¡Œï¼‰**ï¼š
- `"Accept"`ï¼šHTTPè¯·æ±‚å¤´å­—æ®µåï¼Œå‘Šè¯‰æœåŠ¡å™¨å®¢æˆ·ç«¯æ¥å—çš„å†…å®¹ç±»å‹
- `"text/html,application/xhtml+xml,application/xml;q=0.9..."`ï¼šæ¥å—çš„MIMEç±»å‹åˆ—è¡¨
- `q=0.9`ï¼šè´¨é‡å› å­ï¼ˆæƒé‡ï¼‰ï¼ŒèŒƒå›´0-1ï¼Œè¡¨ç¤ºä¼˜å…ˆçº§

**Acceptå­—æ®µè§£æ**ï¼š
```python
# Acceptå­—æ®µçš„æ ¼å¼
"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"

# æ‹†è§£åˆ†æï¼ˆæŒ‰é€—å·åˆ†å‰²ï¼‰ï¼š
# 1. text/html                    - HTMLæ–‡æ¡£ï¼ˆé»˜è®¤q=1.0ï¼Œæœ€é«˜ä¼˜å…ˆçº§ï¼‰
# 2. application/xhtml+xml        - XHTMLæ–‡æ¡£ï¼ˆq=1.0ï¼‰
# 3. application/xml;q=0.9        - XMLæ–‡æ¡£ï¼ˆæƒé‡0.9ï¼‰
# 4. image/webp                   - WebPå›¾ç‰‡æ ¼å¼ï¼ˆq=1.0ï¼‰
# 5. */*;q=0.8                    - ä»»ä½•ç±»å‹ï¼ˆæƒé‡0.8ï¼Œæœ€ä½ä¼˜å…ˆçº§ï¼‰

# æœåŠ¡å™¨å¦‚ä½•ä½¿ç”¨Acceptï¼Ÿ
# 1. æŸ¥çœ‹å®¢æˆ·ç«¯æ¥å—çš„ç±»å‹åˆ—è¡¨
# 2. æŒ‰æƒé‡(qå€¼)æ’åº
# 3. è¿”å›å®¢æˆ·ç«¯æœ€åå¥½çš„å¯ç”¨æ ¼å¼

# å¸¸è§çš„MIMEç±»å‹ï¼š
MIME_TYPES = {
    'text/html': 'HTMLæ–‡æ¡£',
    'text/plain': 'çº¯æ–‡æœ¬',
    'application/json': 'JSONæ•°æ®',
    'application/xml': 'XMLæ•°æ®',
    'image/jpeg': 'JPEGå›¾ç‰‡',
    'image/png': 'PNGå›¾ç‰‡',
    'application/pdf': 'PDFæ–‡æ¡£'
}
```

**ç¬¬24è¡Œ**ï¼š`"Accept-Language": "en-US,en;q=0.5",`

**Accept-Languageè¯­è¨€åå¥½è¯¦è§£ï¼ˆç¬¬24è¡Œï¼‰**ï¼š
- `"Accept-Language"`ï¼šHTTPè¯·æ±‚å¤´å­—æ®µåï¼ŒæŒ‡å®šå®¢æˆ·ç«¯åå¥½çš„è¯­è¨€
- `"en-US,en;q=0.5"`ï¼šè¯­è¨€åˆ—è¡¨åŠæƒé‡
  - `en-US`ï¼šç¾å¼è‹±è¯­ï¼ˆq=1.0ï¼Œæœ€é«˜ä¼˜å…ˆçº§ï¼‰
  - `en;q=0.5`ï¼šè‹±è¯­ï¼ˆæƒé‡0.5ï¼‰

**è¯­è¨€åå¥½ç¤ºä¾‹**ï¼š
```python
# ä¸åŒçš„è¯­è¨€é…ç½®
"en-US,en;q=0.5"           # åå¥½ç¾å¼è‹±è¯­
"zh-CN,zh;q=0.9,en;q=0.5"  # åå¥½ç®€ä½“ä¸­æ–‡ï¼Œå…¶æ¬¡ä¸­æ–‡ï¼Œæœ€åè‹±æ–‡
"ja-JP,ja;q=0.9"           # åå¥½æ—¥è¯­

# æœåŠ¡å™¨æ ¹æ®Accept-Languageè¿”å›å¯¹åº”è¯­è¨€ç‰ˆæœ¬
# è®¿é—®Googleé¦–é¡µï¼š
# Accept-Language: zh-CN  â†’ æ˜¾ç¤ºä¸­æ–‡ç•Œé¢
# Accept-Language: en-US  â†’ æ˜¾ç¤ºè‹±æ–‡ç•Œé¢
```

**ç¬¬25è¡Œ**ï¼š`"Accept-Encoding": "gzip, deflate, br",`

**Accept-Encodingå‹ç¼©æ ¼å¼è¯¦è§£ï¼ˆç¬¬25è¡Œï¼‰**ï¼š
- `"Accept-Encoding"`ï¼šHTTPè¯·æ±‚å¤´å­—æ®µåï¼ŒæŒ‡å®šå®¢æˆ·ç«¯æ”¯æŒçš„å‹ç¼©æ ¼å¼
- `"gzip, deflate, br"`ï¼šæ”¯æŒçš„å‹ç¼©ç®—æ³•åˆ—è¡¨
  - `gzip`ï¼šGZipå‹ç¼©ï¼ˆæœ€å¸¸ç”¨ï¼‰
  - `deflate`ï¼šDeflateå‹ç¼©
  - `br`ï¼šBrotliå‹ç¼©ï¼ˆæ›´é«˜å‹ç¼©ç‡ï¼‰

**å‹ç¼©çš„å¥½å¤„**ï¼š
```python
# ä¸ºä»€ä¹ˆè¦å‹ç¼©ï¼Ÿ
# åŸå§‹HTMLï¼š100KB
# GZipå‹ç¼©åï¼š20KBï¼ˆå‹ç¼©ç‡80%ï¼‰
# èŠ‚çœå¸¦å®½ï¼š80KB
# åŠ å¿«åŠ è½½ï¼šå¿«5å€

# requestsåº“è‡ªåŠ¨å¤„ç†å‹ç¼©
response = requests.get(url, headers=headers)
# requestsè‡ªåŠ¨è§£å‹ç¼©å“åº”å†…å®¹
# ä½ å¾—åˆ°çš„response.textå·²ç»æ˜¯è§£å‹åçš„å†…å®¹
```

**ç¬¬26è¡Œ**ï¼š`"Connection": "keep-alive",`

**Connectionè¿æ¥ä¿æŒè¯¦è§£ï¼ˆç¬¬26è¡Œï¼‰**ï¼š
- `"Connection"`ï¼šHTTPè¯·æ±‚å¤´å­—æ®µåï¼Œæ§åˆ¶ç½‘ç»œè¿æ¥è¡Œä¸º
- `"keep-alive"`ï¼šä¿æŒè¿æ¥ä¸å…³é—­ï¼Œç”¨äºåç»­è¯·æ±‚å¤ç”¨

**keep-aliveçš„ä¼˜åŠ¿**ï¼š
```python
# Connection: keep-alive çš„ä½œç”¨

# ä¸ä½¿ç”¨keep-aliveï¼ˆHTTP/1.0é»˜è®¤ï¼‰ï¼š
# è¯·æ±‚1: å»ºç«‹è¿æ¥ â†’ å‘é€è¯·æ±‚ â†’ æ¥æ”¶å“åº” â†’ å…³é—­è¿æ¥
# è¯·æ±‚2: å»ºç«‹è¿æ¥ â†’ å‘é€è¯·æ±‚ â†’ æ¥æ”¶å“åº” â†’ å…³é—­è¿æ¥
# è¯·æ±‚3: å»ºç«‹è¿æ¥ â†’ å‘é€è¯·æ±‚ â†’ æ¥æ”¶å“åº” â†’ å…³é—­è¿æ¥
# æ¯æ¬¡éƒ½è¦é‡æ–°å»ºç«‹TCPè¿æ¥ï¼ˆä¸‰æ¬¡æ¡æ‰‹ï¼‰ï¼Œæµªè´¹æ—¶é—´

# ä½¿ç”¨keep-aliveï¼š
# å»ºç«‹è¿æ¥ â†’ è¯·æ±‚1 â†’ å“åº”1 â†’ è¯·æ±‚2 â†’ å“åº”2 â†’ è¯·æ±‚3 â†’ å“åº”3 â†’ å…³é—­è¿æ¥
# å¤ç”¨åŒä¸€ä¸ªTCPè¿æ¥ï¼ŒèŠ‚çœå»ºç«‹è¿æ¥çš„æ—¶é—´

# æ€§èƒ½å¯¹æ¯”ï¼š
# å»ºç«‹TCPè¿æ¥ï¼š~100ms
# 10ä¸ªè¯·æ±‚ä¸ç”¨keep-aliveï¼š1000ms (10 * 100ms)
# 10ä¸ªè¯·æ±‚ç”¨keep-aliveï¼š100ms (åªå»ºç«‹ä¸€æ¬¡è¿æ¥)
# é€Ÿåº¦æå‡ï¼š10å€ï¼

# requests.Session è‡ªåŠ¨å¯ç”¨keep-alive
session = requests.Session()
session.get('https://example.com/page1')  # å»ºç«‹è¿æ¥
session.get('https://example.com/page2')  # å¤ç”¨è¿æ¥
session.get('https://example.com/page3')  # å¤ç”¨è¿æ¥
```

### å…¶ä»–é‡è¦è¯·æ±‚å¤´å­—æ®µ

**ç¬¬27-36è¡Œ**ï¼šå®‰å…¨å’Œç¼“å­˜ç›¸å…³è¯·æ±‚å¤´

```python
"Upgrade-Insecure-Requests": "1",       # ç¬¬27è¡Œ
"Cache-Control": "max-age=0",           # ç¬¬28è¡Œ
"Referer": "https://www.google.com/",   # ç¬¬29è¡Œ
"sec-ch-ua": "\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"91\"...",  # ç¬¬30è¡Œ
"sec-ch-ua-mobile": "?0",               # ç¬¬31è¡Œ
"sec-ch-ua-platform": "\"Windows\"",    # ç¬¬32è¡Œ
"Sec-Fetch-Dest": "document",           # ç¬¬33è¡Œ
"Sec-Fetch-Mode": "navigate",           # ç¬¬34è¡Œ
"Sec-Fetch-Site": "cross-site",         # ç¬¬35è¡Œ
"Sec-Fetch-User": "?1"                  # ç¬¬36è¡Œ
```

**å„å­—æ®µè¯¦è§£**ï¼š

```python
# 1. Upgrade-Insecure-Requests: "1"
# å«ä¹‰ï¼šå‘Šè¯‰æœåŠ¡å™¨ï¼Œå®¢æˆ·ç«¯æ”¯æŒHTTPå‡çº§åˆ°HTTPS
# ä½œç”¨ï¼šæœåŠ¡å™¨å¯ä»¥è‡ªåŠ¨å°†HTTPé“¾æ¥å‡çº§ä¸ºHTTPS
# å®‰å…¨æ€§æå‡

# 2. Cache-Control: "max-age=0"
# å«ä¹‰ï¼šä¸ä½¿ç”¨ç¼“å­˜ï¼Œæ€»æ˜¯è¯·æ±‚æœ€æ–°å†…å®¹
# max-age=0ï¼šç¼“å­˜æœ‰æ•ˆæœŸä¸º0ç§’
# æ›¿ä»£å€¼ç¤ºä¾‹ï¼š
#   "max-age=3600" - ç¼“å­˜1å°æ—¶
#   "no-cache" - æ¯æ¬¡éƒ½éªŒè¯
#   "no-store" - å®Œå…¨ä¸ç¼“å­˜

# 3. Referer: "https://www.google.com/"
# å«ä¹‰ï¼šå‘Šè¯‰æœåŠ¡å™¨ï¼Œæ˜¯ä»å“ªä¸ªé¡µé¢è·³è½¬è¿‡æ¥çš„
# ä½œç”¨ï¼š
#   - æœåŠ¡å™¨å¯ä»¥ç»Ÿè®¡æµé‡æ¥æº
#   - åçˆ¬è™«ï¼šæ£€æŸ¥Refereræ˜¯å¦åˆæ³•
#   - é˜²ç›—é“¾ï¼šå›¾ç‰‡ç½‘ç«™æ£€æŸ¥Referer
# ç¤ºä¾‹ï¼š
#   ç”¨æˆ·åœ¨Googleæœç´¢ â†’ ç‚¹å‡»é“¾æ¥ â†’ è¯·æ±‚å¤´ä¸­Refererä¸ºGoogleç½‘å€

# 4. sec-ch-ua: å®¢æˆ·ç«¯æç¤º-ç”¨æˆ·ä»£ç†
# å«ä¹‰ï¼šæµè§ˆå™¨å“ç‰Œå’Œç‰ˆæœ¬ä¿¡æ¯ï¼ˆChrome 91ï¼‰
# æ–°çš„æ ‡å‡†ï¼Œç”¨äºæ›¿ä»£éƒ¨åˆ†User-AgentåŠŸèƒ½

# 5. sec-ch-ua-mobile: "?0"
# å«ä¹‰ï¼šæ˜¯å¦æ˜¯ç§»åŠ¨è®¾å¤‡
# ?0 = å¦ï¼ˆæ¡Œé¢è®¾å¤‡ï¼‰
# ?1 = æ˜¯ï¼ˆç§»åŠ¨è®¾å¤‡ï¼‰

# 6. sec-ch-ua-platform: "Windows"
# å«ä¹‰ï¼šæ“ä½œç³»ç»Ÿå¹³å°
# å¯èƒ½çš„å€¼ï¼š"Windows", "macOS", "Linux", "Android", "iOS"

# 7. Sec-Fetch-Dest: "document"
# å«ä¹‰ï¼šè¯·æ±‚çš„èµ„æºç±»å‹
# document - HTMLæ–‡æ¡£
# image - å›¾ç‰‡
# script - JavaScript
# style - CSS

# 8. Sec-Fetch-Mode: "navigate"
# å«ä¹‰ï¼šè¯·æ±‚æ¨¡å¼
# navigate - å¯¼èˆªï¼ˆç”¨æˆ·ç‚¹å‡»é“¾æ¥ï¼‰
# cors - è·¨åŸŸè¯·æ±‚
# no-cors - éè·¨åŸŸè¯·æ±‚

# 9. Sec-Fetch-Site: "cross-site"
# å«ä¹‰ï¼šè¯·æ±‚æ¥æº
# same-origin - åŒæºï¼ˆç›¸åŒåŸŸåï¼‰
# same-site - åŒç«™ç‚¹
# cross-site - è·¨ç«™ç‚¹
# none - ç›´æ¥è®¿é—®ï¼ˆåœ°å€æ è¾“å…¥ï¼‰

# 10. Sec-Fetch-User: "?1"
# å«ä¹‰ï¼šæ˜¯å¦æ˜¯ç”¨æˆ·ä¸»åŠ¨è§¦å‘çš„è¯·æ±‚
# ?1 = æ˜¯ï¼ˆç”¨æˆ·ç‚¹å‡»ã€è¾“å…¥åœ°å€ç­‰ï¼‰
# ?0 = å¦ï¼ˆè„šæœ¬è‡ªåŠ¨å‘èµ·ï¼‰

# åçˆ¬è™«æ£€æµ‹ç¤ºä¾‹ï¼š
def check_request_authenticity(headers):
    """æ£€æŸ¥è¯·æ±‚æ˜¯å¦æ¥è‡ªçœŸå®æµè§ˆå™¨"""
    # æ£€æŸ¥1ï¼šæ˜¯å¦æœ‰User-Agent
    if 'User-Agent' not in headers:
        return False
    
    # æ£€æŸ¥2ï¼šUser-Agentæ˜¯å¦åƒçˆ¬è™«
    if 'python' in headers['User-Agent'].lower():
        return False
    
    # æ£€æŸ¥3ï¼šæ˜¯å¦æœ‰Sec-Fetch-*å­—æ®µï¼ˆçœŸå®æµè§ˆå™¨æ‰æœ‰ï¼‰
    if 'Sec-Fetch-Dest' not in headers:
        return False
    
    # æ£€æŸ¥4ï¼šRefereræ˜¯å¦åˆç†
    if headers.get('Referer', '').startswith('http://localhost'):
        return False  # å¯ç–‘çš„æœ¬åœ°æ¥æº
    
    return True

# ä¸ºä»€ä¹ˆæœ¬é¡¹ç›®é…ç½®äº†è¿™ä¹ˆå¤šè¯·æ±‚å¤´ï¼Ÿ
# 1. æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º
# 2. ç»•è¿‡åçˆ¬è™«æ£€æµ‹
# 3. æé«˜è¯·æ±‚æˆåŠŸç‡
# 4. è·å–å®Œæ•´çš„é¡µé¢å†…å®¹
```

### ä»£ç†æœåŠ¡å™¨åˆ—è¡¨ï¼ˆç¬¬106-151è¡Œï¼‰

```python
# ä»£ç†æœåŠ¡å™¨åˆ—è¡¨
PROXY_LIST: List[Optional[Dict[str, str]]] = [  # ç¬¬107è¡Œ
    None,  # ç¬¬108è¡Œ - ç›´æ¥è¿æ¥ï¼Œä¸ä½¿ç”¨ä»£ç†
    # æ³¨é‡Šæ‰çš„ä»£ç†é…ç½®ï¼ˆç¬¬109-150è¡Œï¼‰
]
```

**ç¬¬107è¡Œè¯¦è§£**ï¼š`PROXY_LIST: List[Optional[Dict[str, str]]] = [`

**ä»£ç†åˆ—è¡¨ç±»å‹æ³¨è§£è¯¦è§£ï¼ˆç¬¬107è¡Œï¼‰**ï¼š
- `PROXY_LIST`ï¼šä»£ç†åˆ—è¡¨å¸¸é‡å
- `:`ï¼šç±»å‹æ³¨è§£ç¬¦å·
- `List[Optional[Dict[str, str]]]`ï¼šå¤æ‚çš„ç±»å‹æ³¨è§£
  - æœ€å¤–å±‚ï¼š`List` - åˆ—è¡¨ç±»å‹
  - ä¸­é—´å±‚ï¼š`Optional[...]` - å¯é€‰ç±»å‹ï¼Œå¯ä»¥æ˜¯å…·ä½“å€¼æˆ–None
  - æœ€å†…å±‚ï¼š`Dict[str, str]` - å­—ç¬¦ä¸²é”®å€¼å¯¹å­—å…¸
  - å®Œæ•´å«ä¹‰ï¼šè¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…ƒç´ å¯ä»¥æ˜¯Noneæˆ–è€…å­—ç¬¦ä¸²å­—å…¸

**ç±»å‹æ³¨è§£è§£æ**ï¼š
```python
# ç±»å‹æ³¨è§£çš„å±‚æ¬¡ç»“æ„
List[Optional[Dict[str, str]]]
â”‚    â”‚        â”‚    â”‚    â”‚
â”‚    â”‚        â”‚    â”‚    â””â”€ å€¼ç±»å‹ï¼šå­—ç¬¦ä¸²
â”‚    â”‚        â”‚    â””â”€â”€â”€â”€â”€â”€ é”®ç±»å‹ï¼šå­—ç¬¦ä¸²
â”‚    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å­—å…¸ç±»å‹
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¯é€‰ç±»å‹ï¼ˆå¯ä»¥æ˜¯Noneï¼‰
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆ—è¡¨ç±»å‹

# å®é™…æ•°æ®ç¤ºä¾‹
PROXY_LIST = [
    None,  # Optional - å¯ä»¥æ˜¯None
    {"http": "http://localhost:7890", "https": "http://localhost:7890"},  # Optional[Dict[str, str]]
    {"http": "http://127.0.0.1:1080", "https": "http://127.0.0.1:1080"}   # Optional[Dict[str, str]]
]

# ä¸ºä»€ä¹ˆéœ€è¦Optionalï¼Ÿ
# å› ä¸ºä»£ç†å¯ä»¥"ä¸ä½¿ç”¨"ï¼ˆNoneï¼‰æˆ–"ä½¿ç”¨å…·ä½“ä»£ç†"ï¼ˆDictï¼‰
```

**ç¬¬108è¡Œè¯¦è§£**ï¼š`None,  # ç›´æ¥è¿æ¥ï¼Œä¸ä½¿ç”¨ä»£ç†`

**Noneå€¼çš„å«ä¹‰ï¼ˆç¬¬108è¡Œï¼‰**ï¼š
- `None`ï¼šPythonå†…ç½®å¸¸é‡ï¼Œè¡¨ç¤º"ç©º"æˆ–"æ— "
- `#`ï¼šæ³¨é‡Šç¬¦å·
- `ç›´æ¥è¿æ¥ï¼Œä¸ä½¿ç”¨ä»£ç†`ï¼šæ³¨é‡Šå†…å®¹ï¼Œè¯´æ˜Noneçš„å«ä¹‰
- `,`ï¼šåˆ—è¡¨å…ƒç´ åˆ†éš”ç¬¦

**ä»£ç†é…ç½®è¯¦è§£**ï¼š
```python
# ä»£ç†é…ç½®çš„ç»“æ„
proxy_config = {
    "http": "http://localhost:7890",   # HTTPåè®®çš„ä»£ç†åœ°å€
    "https": "http://localhost:7890"   # HTTPSåè®®çš„ä»£ç†åœ°å€
}

# ä½¿ç”¨requestså‘é€ä»£ç†è¯·æ±‚
import requests

# æ–¹å¼1: ä¸ä½¿ç”¨ä»£ç†
response = requests.get('https://example.com')

# æ–¹å¼2: ä½¿ç”¨ä»£ç†
proxies = {
    "http": "http://127.0.0.1:7890",
    "https": "http://127.0.0.1:7890"
}
response = requests.get('https://example.com', proxies=proxies)

# æ–¹å¼3: ä»åˆ—è¡¨ä¸­éšæœºé€‰æ‹©
import random
proxy = random.choice(PROXY_LIST)  # å¯èƒ½æ˜¯Noneæˆ–ä»£ç†å­—å…¸
response = requests.get('https://example.com', proxies=proxy)

# ä»£ç†çš„å·¥ä½œåŸç†ï¼š
# ä¸ä½¿ç”¨ä»£ç†ï¼š
#   ä½ çš„ç”µè„‘ â†’ ç›´æ¥è¿æ¥ â†’ ç›®æ ‡ç½‘ç«™
#   IPåœ°å€ï¼šä½ çš„çœŸå®IP
#
# ä½¿ç”¨ä»£ç†ï¼š
#   ä½ çš„ç”µè„‘ â†’ ä»£ç†æœåŠ¡å™¨ â†’ ç›®æ ‡ç½‘ç«™
#   IPåœ°å€ï¼šä»£ç†æœåŠ¡å™¨çš„IPï¼ˆéšè—äº†ä½ çš„çœŸå®IPï¼‰

# å¸¸è§ä»£ç†ç±»å‹ï¼š
# 1. HTTPä»£ç† - åªä»£ç†HTTPæµé‡
# 2. HTTPSä»£ç† - ä»£ç†HTTPSæµé‡
# 3. SOCKSä»£ç† - æ›´åº•å±‚ï¼Œæ”¯æŒæ‰€æœ‰åè®®
# 4. é€æ˜ä»£ç† - ä¸ä¿®æ”¹è¯·æ±‚
# 5. åŒ¿åä»£ç† - éšè—çœŸå®IP
# 6. é«˜åŒ¿ä»£ç† - å®Œå…¨éšè—ä»£ç†ç—•è¿¹

# å¸¸è§ä»£ç†ç«¯å£ï¼š
# 7890 - Clashä»£ç†é»˜è®¤ç«¯å£
# 1080 - SOCKSä»£ç†å¸¸ç”¨ç«¯å£
# 8080 - HTTPä»£ç†å¸¸ç”¨ç«¯å£
# 8118 - Privoxyä»£ç†ç«¯å£
# 10809 - æŸäº›VPNè½¯ä»¶ç«¯å£

# ä¸ºä»€ä¹ˆéœ€è¦ä»£ç†ï¼Ÿ
# 1. ç»•è¿‡IPé™åˆ¶ï¼ˆç½‘ç«™é™åˆ¶å•ä¸ªIPçš„è®¿é—®é¢‘ç‡ï¼‰
# 2. è®¿é—®è¢«å°é”çš„ç½‘ç«™
# 3. éšè—çœŸå®IPåœ°å€
# 4. è´Ÿè½½å‡è¡¡ï¼ˆåˆ†æ•£è¯·æ±‚ï¼‰
# 5. æé«˜æˆåŠŸç‡ï¼ˆæŸäº›IPå¯èƒ½è¢«å°ç¦ï¼‰

# ä»£ç†å¤±è´¥å¤„ç†ï¼š
try:
    response = requests.get(url, proxies=proxy, timeout=10)
except requests.exceptions.ProxyError:
    print("ä»£ç†è¿æ¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥è¿æ¥")
    response = requests.get(url, timeout=10)
```

### åŸŸåè§„åˆ™æ˜ å°„ï¼ˆç¬¬153-161è¡Œï¼‰

```python
# ç½‘ç«™åŸŸåä¸å¤„ç†è§„åˆ™çš„æ˜ å°„
DOMAIN_RULES: Dict[str, Callable[[str], Optional[str]]] = {  # ç¬¬154è¡Œ
    "ieeexplore.ieee.org": Rule.ieee_rule,         # ç¬¬155è¡Œ
    "dl.acm.org": Rule.acm_rule,                   # ç¬¬156è¡Œ
    "arxiv.org": Rule.arxiv_rule,                  # ç¬¬157è¡Œ
    "link.springer.com": Rule.springer_rule,       # ç¬¬158è¡Œ
    "www.sciencedirect.com": Rule.elsevier_rule,   # ç¬¬159è¡Œ
    "rfc-editor.org": Rule.rfc_editor_rule         # ç¬¬160è¡Œ
}
```

**ç¬¬154è¡Œè¯¦è§£**ï¼š`DOMAIN_RULES: Dict[str, Callable[[str], Optional[str]]] = {`

**å‡½æ•°ç±»å‹å­—å…¸è¯¦è§£ï¼ˆç¬¬154è¡Œï¼‰**ï¼š
- `DOMAIN_RULES`ï¼šåŸŸåè§„åˆ™å­—å…¸å¸¸é‡å
- `:`ï¼šç±»å‹æ³¨è§£ç¬¦å·
- `Dict[str, Callable[[str], Optional[str]]]`ï¼šå¤æ‚çš„ç±»å‹æ³¨è§£
  - å¤–å±‚ï¼š`Dict` - å­—å…¸ç±»å‹
  - é”®ç±»å‹ï¼š`str` - å­—ç¬¦ä¸²ï¼ˆåŸŸåï¼‰
  - å€¼ç±»å‹ï¼š`Callable[[str], Optional[str]]` - å¯è°ƒç”¨å¯¹è±¡ï¼ˆå‡½æ•°ï¼‰
    - `Callable`ï¼šå¯è°ƒç”¨ç±»å‹ï¼ˆå‡½æ•°ã€æ–¹æ³•ã€ç±»ç­‰ï¼‰
    - `[str]`ï¼šå‡½æ•°å‚æ•°ç±»å‹åˆ—è¡¨ï¼ˆæ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²å‚æ•°ï¼‰
    - `Optional[str]`ï¼šå‡½æ•°è¿”å›å€¼ç±»å‹ï¼ˆè¿”å›å­—ç¬¦ä¸²æˆ–Noneï¼‰

**Callableç±»å‹è¯¦è§£**ï¼š
```python
# Callableç±»å‹æ³¨è§£çš„ç»“æ„
Callable[[å‚æ•°ç±»å‹åˆ—è¡¨], è¿”å›å€¼ç±»å‹]

# ç¤ºä¾‹1: æ¥å—å­—ç¬¦ä¸²ï¼Œè¿”å›å­—ç¬¦ä¸²
Callable[[str], str]
def process(text: str) -> str:
    return text.upper()

# ç¤ºä¾‹2: æ¥å—ä¸¤ä¸ªæ•´æ•°ï¼Œè¿”å›æ•´æ•°
Callable[[int, int], int]
def add(a: int, b: int) -> int:
    return a + b

# ç¤ºä¾‹3: æ¥å—å­—ç¬¦ä¸²ï¼Œè¿”å›å¯é€‰å­—ç¬¦ä¸²ï¼ˆæœ¬é¡¹ç›®ä½¿ç”¨çš„ï¼‰
Callable[[str], Optional[str]]
def extract_abstract(html: str) -> Optional[str]:
    if html:
        return "Abstract text"
    return None

# ç¤ºä¾‹4: æ— å‚æ•°ï¼Œè¿”å›å¸ƒå°”å€¼
Callable[[], bool]
def is_ready() -> bool:
    return True

# Dict[str, Callable[[str], Optional[str]]] çš„å®é™…æ•°æ®
DOMAIN_RULES = {
    "ieee.org": ieee_extraction_function,  # å‡½æ•°å¯¹è±¡
    "acm.org": acm_extraction_function,    # å‡½æ•°å¯¹è±¡
}

# ä½¿ç”¨ç¤ºä¾‹
domain = "ieee.org"
html_content = "<html>...</html>"

# è·å–å¯¹åº”çš„å¤„ç†å‡½æ•°
handler_function = DOMAIN_RULES.get(domain)

# è°ƒç”¨å‡½æ•°å¤„ç†HTML
if handler_function:
    abstract = handler_function(html_content)  # Callableè¢«è°ƒç”¨
```

**ç¬¬155-160è¡Œè¯¦è§£**ï¼šåŸŸåä¸è§„åˆ™å‡½æ•°çš„æ˜ å°„

```python
"ieeexplore.ieee.org": Rule.ieee_rule,  # IEEEè§„åˆ™
```

**é”®å€¼å¯¹ç»“æ„è¯¦è§£**ï¼š
- `"ieeexplore.ieee.org"`ï¼šå­—å…¸çš„é”®ï¼ˆåŸŸåå­—ç¬¦ä¸²ï¼‰
- `:`ï¼šé”®å€¼åˆ†éš”ç¬¦
- `Rule.ieee_rule`ï¼šå­—å…¸çš„å€¼ï¼ˆå‡½æ•°å¯¹è±¡ï¼Œæ³¨æ„æ²¡æœ‰æ‹¬å·ï¼‰
- `,`ï¼šå­—å…¸å…ƒç´ åˆ†éš”ç¬¦

**é‡è¦æ¦‚å¿µ - å‡½æ•°å¯¹è±¡ vs å‡½æ•°è°ƒç”¨**ï¼š
```python
# 1. å‡½æ•°å¯¹è±¡ï¼ˆä¸å¸¦æ‹¬å·ï¼‰- ä¼ é€’å‡½æ•°æœ¬èº«
function_object = Rule.ieee_rule  # è¿™æ˜¯ä¸€ä¸ªå‡½æ•°å¯¹è±¡
# å¯ä»¥åƒå˜é‡ä¸€æ ·ä¼ é€’ã€å­˜å‚¨ã€åç»­è°ƒç”¨

# 2. å‡½æ•°è°ƒç”¨ï¼ˆå¸¦æ‹¬å·ï¼‰- æ‰§è¡Œå‡½æ•°å¹¶è¿”å›ç»“æœ
result = Rule.ieee_rule(html_content)  # è¿™æ˜¯å‡½æ•°è°ƒç”¨
# ç«‹å³æ‰§è¡Œå‡½æ•°ï¼Œresultå­˜å‚¨è¿”å›å€¼

# å­—å…¸ä¸­å­˜å‚¨å‡½æ•°å¯¹è±¡çš„ä¼˜åŠ¿
DOMAIN_RULES = {
    "ieee.org": Rule.ieee_rule,  # å­˜å‚¨å‡½æ•°å¯¹è±¡ï¼Œä¸æ˜¯æ‰§è¡Œç»“æœ
    "acm.org": Rule.acm_rule
}

# ä½¿ç”¨æ—¶åŠ¨æ€è°ƒç”¨
domain = get_domain(url)  # è·å–åŸŸå
handler = DOMAIN_RULES.get(domain)  # è·å–å¯¹åº”çš„å¤„ç†å‡½æ•°
if handler:
    result = handler(html)  # æ­¤æ—¶æ‰è°ƒç”¨å‡½æ•°

# è¿™ç§è®¾è®¡æ¨¡å¼å«"ç­–ç•¥æ¨¡å¼"
# ä¼˜åŠ¿ï¼š
# 1. çµæ´»ï¼šå¯ä»¥æ ¹æ®åŸŸååŠ¨æ€é€‰æ‹©å¤„ç†æ–¹æ³•
# 2. å¯æ‰©å±•ï¼šæ·»åŠ æ–°ç½‘ç«™åªéœ€æ·»åŠ æ–°è§„åˆ™
# 3. ç»´æŠ¤æ€§å¥½ï¼šæ¯ä¸ªç½‘ç«™çš„è§„åˆ™ç‹¬ç«‹
```

**åŸŸåè§„åˆ™æ˜ å°„çš„åº”ç”¨ç¤ºä¾‹**ï¼š
```python
# å®Œæ•´çš„ä½¿ç”¨æµç¨‹
def extract_abstract(url: str, html: str) -> Optional[str]:
    """æ ¹æ®URLé€‰æ‹©åˆé€‚çš„æå–è§„åˆ™"""
    
    # æ­¥éª¤1: ä»URLæå–åŸŸå
    from urllib.parse import urlparse
    parsed = urlparse(url)
    domain = parsed.netloc.lower().lstrip("www.")
    
    # æ­¥éª¤2: æŸ¥æ‰¾å¯¹åº”çš„è§„åˆ™å‡½æ•°
    rule_function = DOMAIN_RULES.get(domain)
    
    # æ­¥éª¤3: å¦‚æœæ‰¾åˆ°è§„åˆ™ï¼Œä½¿ç”¨è¯¥è§„åˆ™æå–
    if rule_function:
        abstract = rule_function(html)  # è°ƒç”¨å¯¹åº”çš„è§„åˆ™å‡½æ•°
        if abstract:
            return abstract
    
    # æ­¥éª¤4: å¦‚æœæ²¡æœ‰æ‰¾åˆ°è§„åˆ™ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
    return generic_extraction(html)

# å®é™…ä½¿ç”¨ç¤ºä¾‹
url1 = "https://ieeexplore.ieee.org/document/12345"
html1 = "<html>IEEEæ ¼å¼çš„HTML...</html>"
abstract1 = extract_abstract(url1, html1)
# åŸŸåæ˜¯"ieeexplore.ieee.org"ï¼Œä½¿ç”¨Rule.ieee_ruleå¤„ç†

url2 = "https://dl.acm.org/doi/10.1145/3517077"
html2 = "<html>ACMæ ¼å¼çš„HTML...</html>"
abstract2 = extract_abstract(url2, html2)
# åŸŸåæ˜¯"dl.acm.org"ï¼Œä½¿ç”¨Rule.acm_ruleå¤„ç†

url3 = "https://unknown-journal.com/paper/123"
html3 = "<html>æœªçŸ¥æ ¼å¼çš„HTML...</html>"
abstract3 = extract_abstract(url3, html3)
# åŸŸåä¸åœ¨DOMAIN_RULESä¸­ï¼Œä½¿ç”¨generic_extractionå¤„ç†

# ä¸ºä»€ä¹ˆéœ€è¦ä¸åŒçš„è§„åˆ™ï¼Ÿ
# å› ä¸ºæ¯ä¸ªå­¦æœ¯ç½‘ç«™çš„HTMLç»“æ„ä¸åŒï¼š

# IEEEç½‘ç«™ç»“æ„ï¼š
# <div class="abstract-text">æ‘˜è¦å†…å®¹</div>

# ACMç½‘ç«™ç»“æ„ï¼š
# <div class="abstractSection"><p>æ‘˜è¦å†…å®¹</p></div>

# Springerç½‘ç«™ç»“æ„ï¼š
# <section class="Abstract"><p>æ‘˜è¦å†…å®¹</p></section>

# æ¯ä¸ªç½‘ç«™éœ€è¦ä¸åŒçš„CSSé€‰æ‹©å™¨å’Œæå–é€»è¾‘
```

---

## ğŸ’¾ Cacheç¼“å­˜ç±»

### ç±»å®šä¹‰å’Œåˆå§‹åŒ–ï¼ˆç¬¬249-254è¡Œï¼‰

```python
class Cache:                                          # ç¬¬249è¡Œ
    """ç®€å•çš„æœ¬åœ°ç¼“å­˜å®ç°"""                             # ç¬¬250è¡Œ
    def __init__(self, cache_dir="paper_cache"):     # ç¬¬251è¡Œ
        self.cache_dir = Path(cache_dir)             # ç¬¬252è¡Œ
        self.cache_dir.mkdir(exist_ok=True)          # ç¬¬253è¡Œ
        self.cache_duration = timedelta(days=7)      # ç¬¬254è¡Œ
```

**ç¬¬249è¡Œè¯¦è§£**ï¼š`class Cache:`

**ç±»å®šä¹‰è¯¦è§£ï¼ˆç¬¬249è¡Œï¼‰**ï¼š
- `class`ï¼šPythonå…³é”®å­—ï¼Œç”¨äºå®šä¹‰ç±»
- `Cache`ï¼šç±»åï¼Œéµå¾ªå¤§å†™å¼€å¤´çš„å‘½åè§„èŒƒï¼ˆCapWords/PascalCaseï¼‰
- `:`ï¼šç±»å®šä¹‰è¯­å¥ç»“æŸç¬¦ï¼Œåé¢æ˜¯ç±»ä½“

**ç±»çš„æ¦‚å¿µ**ï¼š
```python
# ç±»æ˜¯ä»€ä¹ˆï¼Ÿ
# ç±»æ˜¯å¯¹è±¡çš„æ¨¡æ¿/è“å›¾ï¼Œå®šä¹‰äº†å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•

# æ¯”å–»ï¼š
# ç±» = å»ºç­‘å›¾çº¸
# å¯¹è±¡ = æ ¹æ®å›¾çº¸å»ºé€ çš„å®é™…æˆ¿å­

# ç¤ºä¾‹
class Car:  # æ±½è½¦ç±»ï¼ˆå›¾çº¸ï¼‰
    def __init__(self, brand, color):
        self.brand = brand
        self.color = color
    
    def drive(self):
        print(f"{self.brand}æ±½è½¦æ­£åœ¨è¡Œé©¶")

# åˆ›å»ºå¯¹è±¡ï¼ˆå®ä¾‹åŒ–ï¼‰
car1 = Car("Tesla", "red")    # ç¬¬ä¸€è¾†è½¦ï¼ˆå®é™…çš„è½¦ï¼‰
car2 = Car("BMW", "black")    # ç¬¬äºŒè¾†è½¦ï¼ˆå®é™…çš„è½¦ï¼‰

# æ¯ä¸ªå¯¹è±¡éƒ½æ˜¯ç‹¬ç«‹çš„
car1.drive()  # "Teslaæ±½è½¦æ­£åœ¨è¡Œé©¶"
car2.drive()  # "BMWæ±½è½¦æ­£åœ¨è¡Œé©¶"
```

**ç¬¬250è¡Œè¯¦è§£**ï¼š`"""ç®€å•çš„æœ¬åœ°ç¼“å­˜å®ç°"""`

**æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆdocstringï¼‰è¯¦è§£ï¼ˆç¬¬250è¡Œï¼‰**ï¼š
- `"""`ï¼šä¸‰å¼•å·ï¼Œè¡¨ç¤ºå¤šè¡Œå­—ç¬¦ä¸²
- `ç®€å•çš„æœ¬åœ°ç¼“å­˜å®ç°`ï¼šç±»çš„è¯´æ˜æ–‡æ¡£
- `"""`ï¼šä¸‰å¼•å·ç»“æŸ
- ç±»çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²è‡ªåŠ¨æˆä¸ºç±»çš„æ–‡æ¡£

**æ–‡æ¡£å­—ç¬¦ä¸²çš„ä½œç”¨**ï¼š
```python
# æŸ¥çœ‹ç±»çš„æ–‡æ¡£
print(Cache.__doc__)  # è¾“å‡ºï¼šç®€å•çš„æœ¬åœ°ç¼“å­˜å®ç°

# help()å‡½æ•°ä¼šæ˜¾ç¤ºæ–‡æ¡£
help(Cache)
# è¾“å‡ºï¼š
# class Cache(builtins.object)
#  |  ç®€å•çš„æœ¬åœ°ç¼“å­˜å®ç°
#  |  
#  |  Methods defined here:
#  |  __init__(self, cache_dir='paper_cache')
#  |      ...

# è‰¯å¥½çš„æ–‡æ¡£ä¹ æƒ¯
class Cache:
    """
    æœ¬åœ°æ–‡ä»¶ç¼“å­˜ç³»ç»Ÿ
    
    ç”¨äºç¼“å­˜è®ºæ–‡æ‘˜è¦ï¼Œé¿å…é‡å¤è¯·æ±‚åŒä¸€ç¯‡è®ºæ–‡çš„æ‘˜è¦ã€‚
    ç¼“å­˜æ–‡ä»¶ä½¿ç”¨pickleæ ¼å¼å­˜å‚¨ï¼Œé»˜è®¤ä¿å­˜7å¤©ã€‚
    
    å±æ€§:
        cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        cache_duration: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆé»˜è®¤7å¤©ï¼‰
    
    ç¤ºä¾‹:
        >>> cache = Cache("my_cache")
        >>> cache.set("url", "title", "abstract text")
        >>> abstract = cache.get("url", "title")
    """
    pass
```

**ç¬¬251è¡Œè¯¦è§£**ï¼š`def __init__(self, cache_dir="paper_cache"):`

**æ„é€ å‡½æ•°è¯¦è§£ï¼ˆç¬¬251è¡Œï¼‰**ï¼š
- `def`ï¼šPythonå…³é”®å­—ï¼Œå®šä¹‰å‡½æ•°
- `__init__`ï¼šç‰¹æ®Šæ–¹æ³•åï¼ˆåŒä¸‹åˆ’çº¿å¼€å¤´å’Œç»“å°¾ï¼‰ï¼Œæ„é€ å‡½æ•°
- `self`ï¼šç¬¬ä¸€ä¸ªå‚æ•°ï¼Œè¡¨ç¤ºå¯¹è±¡è‡ªèº«ï¼ˆå¿…é¡»æœ‰ï¼‰
- `cache_dir="paper_cache"`ï¼šç¬¬äºŒä¸ªå‚æ•°ï¼Œé»˜è®¤å€¼ä¸º"paper_cache"
- `:`ï¼šå‡½æ•°å®šä¹‰ç»“æŸç¬¦

**__init__æ„é€ å‡½æ•°è¯¦è§£**ï¼š
```python
# __init__æ˜¯ä»€ä¹ˆï¼Ÿ
# 1. ç±»çš„åˆå§‹åŒ–æ–¹æ³•ï¼ˆæ„é€ å‡½æ•°ï¼‰
# 2. åˆ›å»ºå¯¹è±¡æ—¶è‡ªåŠ¨è°ƒç”¨
# 3. ç”¨äºåˆå§‹åŒ–å¯¹è±¡çš„å±æ€§

# ç¤ºä¾‹
class Person:
    def __init__(self, name, age):  # æ„é€ å‡½æ•°
        self.name = name  # åˆå§‹åŒ–nameå±æ€§
        self.age = age    # åˆå§‹åŒ–ageå±æ€§
        print(f"åˆ›å»ºäº†ä¸€ä¸ªäººï¼š{name}")

# åˆ›å»ºå¯¹è±¡æ—¶ï¼Œ__init__è‡ªåŠ¨æ‰§è¡Œ
person = Person("å¼ ä¸‰", 25)
# è¾“å‡ºï¼šåˆ›å»ºäº†ä¸€ä¸ªäººï¼šå¼ ä¸‰
# person.name å·²ç»è¢«åˆå§‹åŒ–ä¸º"å¼ ä¸‰"
# person.age å·²ç»è¢«åˆå§‹åŒ–ä¸º25

# selfå‚æ•°è¯¦è§£
class Cache:
    def __init__(self, cache_dir):
        # selfä»£è¡¨å½“å‰åˆ›å»ºçš„å¯¹è±¡
        # self.cache_dir æ˜¯å¯¹è±¡çš„å±æ€§
        self.cache_dir = cache_dir

cache1 = Cache("cache1")  # selfæŒ‡å‘cache1å¯¹è±¡
cache2 = Cache("cache2")  # selfæŒ‡å‘cache2å¯¹è±¡

print(cache1.cache_dir)  # "cache1"
print(cache2.cache_dir)  # "cache2"

# é»˜è®¤å‚æ•°
class Cache:
    def __init__(self, cache_dir="paper_cache"):  # é»˜è®¤å€¼
        self.cache_dir = cache_dir

# ä½¿ç”¨é»˜è®¤å€¼
cache1 = Cache()                   # cache_dir="paper_cache"
# æŒ‡å®šå€¼ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
cache2 = Cache("custom_cache")     # cache_dir="custom_cache"
```

**ç¬¬252è¡Œè¯¦è§£**ï¼š`self.cache_dir = Path(cache_dir)`

**å±æ€§åˆå§‹åŒ–è¯¦è§£ï¼ˆç¬¬252è¡Œï¼‰**ï¼š
- `self.cache_dir`ï¼šå¯¹è±¡çš„å±æ€§ï¼ˆå®ä¾‹å˜é‡ï¼‰
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `Path(cache_dir)`ï¼šè°ƒç”¨Pathæ„é€ å‡½æ•°ï¼Œåˆ›å»ºPathå¯¹è±¡
- `cache_dir`ï¼šå‚æ•°ï¼Œä¼ å…¥çš„å­—ç¬¦ä¸²è·¯å¾„

**ä»£ç æ‰§è¡Œæµç¨‹**ï¼š
```python
# æ­¥éª¤åˆ†è§£
cache_dir = "paper_cache"           # 1. å‚æ•°å€¼ï¼ˆå­—ç¬¦ä¸²ï¼‰
path_object = Path(cache_dir)       # 2. åˆ›å»ºPathå¯¹è±¡
self.cache_dir = path_object        # 3. å­˜å‚¨åˆ°å¯¹è±¡å±æ€§

# ä¸ºä»€ä¹ˆè¦è½¬æ¢ä¸ºPathå¯¹è±¡ï¼Ÿ
# å­—ç¬¦ä¸²è·¯å¾„çš„é—®é¢˜ï¼š
cache_dir_str = "paper_cache"
file_path_str = cache_dir_str + "/" + "file.pkl"  # æ‰‹åŠ¨æ‹¼æ¥ï¼Œå®¹æ˜“å‡ºé”™

# Pathå¯¹è±¡çš„ä¼˜åŠ¿ï¼š
cache_dir_path = Path("paper_cache")
file_path = cache_dir_path / "file.pkl"  # ä½¿ç”¨/è¿ç®—ç¬¦ï¼Œè·¨å¹³å°å…¼å®¹
file_path.mkdir(exist_ok=True)           # ä¸°å¯Œçš„æ–¹æ³•
exists = file_path.exists()              # æ–¹ä¾¿çš„æ£€æŸ¥

# å¯¹æ¯”
# å­—ç¬¦ä¸²æ–¹å¼ï¼š
import os
cache_dir = "paper_cache"
if not os.path.exists(cache_dir):
    os.makedirs(cache_dir)
file_path = os.path.join(cache_dir, "file.pkl")

# Pathæ–¹å¼ï¼ˆæ¨èï¼‰ï¼š
cache_dir = Path("paper_cache")
cache_dir.mkdir(exist_ok=True)
file_path = cache_dir / "file.pkl"
```

**ç¬¬253è¡Œè¯¦è§£**ï¼š`self.cache_dir.mkdir(exist_ok=True)`

**åˆ›å»ºç›®å½•è¯¦è§£ï¼ˆç¬¬253è¡Œï¼‰**ï¼š
- `self.cache_dir`ï¼šPathå¯¹è±¡ï¼ˆä¸Šä¸€è¡Œåˆ›å»ºçš„ï¼‰
- `.mkdir()`ï¼šPathå¯¹è±¡çš„æ–¹æ³•ï¼Œåˆ›å»ºç›®å½•
- `exist_ok=True`ï¼šå…³é”®å­—å‚æ•°ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨ä¸æŠ¥é”™

**mkdiræ–¹æ³•è¯¦è§£**ï¼š
```python
from pathlib import Path

# mkdiræ–¹æ³•çš„å‚æ•°
cache_dir = Path("paper_cache")
cache_dir.mkdir(
    exist_ok=True,   # ç›®å½•å·²å­˜åœ¨æ—¶ä¸æŠ›å‡ºå¼‚å¸¸
    parents=True     # åˆ›å»ºçˆ¶ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
)

# exist_okå‚æ•°çš„ä½œç”¨
# exist_ok=Falseï¼ˆé»˜è®¤ï¼‰ï¼š
try:
    cache_dir.mkdir(exist_ok=False)
    cache_dir.mkdir(exist_ok=False)  # ç¬¬äºŒæ¬¡ä¼šæŠ›å‡ºFileExistsError
except FileExistsError:
    print("ç›®å½•å·²å­˜åœ¨")

# exist_ok=Trueï¼š
cache_dir.mkdir(exist_ok=True)
cache_dir.mkdir(exist_ok=True)  # ç¬¬äºŒæ¬¡ä¸æŠ¥é”™ï¼Œå®‰å…¨

# parentså‚æ•°çš„ä½œç”¨
# parents=Falseï¼ˆé»˜è®¤ï¼‰ï¼š
deep_path = Path("a/b/c")
deep_path.mkdir()  # FileNotFoundErrorï¼Œå› ä¸ºaå’Œa/bä¸å­˜åœ¨

# parents=Trueï¼š
deep_path = Path("a/b/c")
deep_path.mkdir(parents=True)  # è‡ªåŠ¨åˆ›å»ºaå’Œa/bï¼Œç„¶ååˆ›å»ºa/b/c

# å®Œæ•´ç¤ºä¾‹
cache_dir = Path("paper_cache")
cache_dir.mkdir(exist_ok=True, parents=True)
# 1. å¦‚æœpaper_cacheå·²å­˜åœ¨ï¼Œä¸æŠ¥é”™
# 2. å¦‚æœçˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º

# ç­‰ä»·çš„os.makedirså†™æ³•
import os
os.makedirs("paper_cache", exist_ok=True)
```

**ç¬¬254è¡Œè¯¦è§£**ï¼š`self.cache_duration = timedelta(days=7)`

**ç¼“å­˜æœ‰æ•ˆæœŸè®¾ç½®è¯¦è§£ï¼ˆç¬¬254è¡Œï¼‰**ï¼š
- `self.cache_duration`ï¼šå¯¹è±¡å±æ€§ï¼Œå­˜å‚¨ç¼“å­˜æœ‰æ•ˆæœŸ
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `timedelta(days=7)`ï¼šåˆ›å»ºtimedeltaå¯¹è±¡ï¼Œè¡¨ç¤º7å¤©çš„æ—¶é—´é—´éš”
- `days=7`ï¼šå…³é”®å­—å‚æ•°ï¼ŒæŒ‡å®šå¤©æ•°

**timedeltaæ—¶é—´é—´éš”è¯¦è§£**ï¼š
```python
from datetime import datetime, timedelta

# timedeltaåˆ›å»ºæ—¶é—´é—´éš”
one_day = timedelta(days=1)              # 1å¤©
one_week = timedelta(days=7)             # 7å¤©
one_hour = timedelta(hours=1)            # 1å°æ—¶
one_minute = timedelta(minutes=1)        # 1åˆ†é’Ÿ
one_second = timedelta(seconds=1)        # 1ç§’
mixed = timedelta(days=1, hours=2, minutes=30)  # 1å¤©2å°æ—¶30åˆ†é’Ÿ

# æ—¶é—´è®¡ç®—
now = datetime.now()                     # å½“å‰æ—¶é—´
tomorrow = now + timedelta(days=1)       # æ˜å¤©
last_week = now - timedelta(days=7)      # ä¸€å‘¨å‰
deadline = now + timedelta(hours=24)     # 24å°æ—¶å

# æ—¶é—´æ¯”è¾ƒï¼ˆç¼“å­˜æœ‰æ•ˆæœŸåˆ¤æ–­ï¼‰
cache_time = datetime.now() - timedelta(days=3)   # ç¼“å­˜åˆ›å»ºæ—¶é—´ï¼ˆ3å¤©å‰ï¼‰
cache_duration = timedelta(days=7)                 # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆ7å¤©ï¼‰
time_passed = datetime.now() - cache_time        # å·²è¿‡æ—¶é—´ï¼ˆ3å¤©ï¼‰

if time_passed <= cache_duration:
    print("ç¼“å­˜ä»ç„¶æœ‰æ•ˆ")  # 3å¤© <= 7å¤©
else:
    print("ç¼“å­˜å·²è¿‡æœŸ")

# æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨
class Cache:
    def __init__(self):
        self.cache_duration = timedelta(days=7)  # ç¼“å­˜7å¤©
    
    def is_cache_valid(self, timestamp):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦åœ¨æœ‰æ•ˆæœŸå†…"""
        time_passed = datetime.now() - timestamp
        return time_passed <= self.cache_duration

# ä½¿ç”¨ç¤ºä¾‹
cache = Cache()
paper_cached_at = datetime.now() - timedelta(days=5)  # 5å¤©å‰ç¼“å­˜çš„
is_valid = cache.is_cache_valid(paper_cached_at)
print(is_valid)  # Trueï¼Œå› ä¸º5å¤© < 7å¤©

paper_cached_at = datetime.now() - timedelta(days=10)  # 10å¤©å‰ç¼“å­˜çš„
is_valid = cache.is_cache_valid(paper_cached_at)
print(is_valid)  # Falseï¼Œå› ä¸º10å¤© > 7å¤©
```

**Cacheç±»åˆå§‹åŒ–æ€»ç»“**ï¼š
```python
# å®Œæ•´çš„åˆå§‹åŒ–è¿‡ç¨‹
cache = Cache("my_cache")

# ç­‰ä»·äºï¼š
# 1. åˆ›å»ºç©ºå¯¹è±¡
# 2. è°ƒç”¨__init__(self, cache_dir="my_cache")
#    a. self.cache_dir = Path("my_cache")  # è®¾ç½®ç¼“å­˜ç›®å½•
#    b. self.cache_dir.mkdir(exist_ok=True)  # åˆ›å»ºç›®å½•
#    c. self.cache_duration = timedelta(days=7)  # è®¾ç½®æœ‰æ•ˆæœŸ7å¤©

# æœ€ç»ˆå¯¹è±¡çš„çŠ¶æ€ï¼š
# cache.cache_dir = Pathå¯¹è±¡ï¼ŒæŒ‡å‘"my_cache"ç›®å½•
# cache.cache_duration = timedeltaå¯¹è±¡ï¼Œè¡¨ç¤º7å¤©
# "my_cache"ç›®å½•å·²åˆ›å»ºï¼ˆå¦‚æœä¸å­˜åœ¨çš„è¯ï¼‰
```

---

### Cacheç±»çš„æ ¸å¿ƒæ–¹æ³•

#### ç”Ÿæˆç¼“å­˜é”®æ–¹æ³•ï¼ˆç¬¬256-259è¡Œï¼‰

```python
def _get_cache_key(self, url: str, title: str) -> str:  # ç¬¬256è¡Œ
    """ç”Ÿæˆç¼“å­˜é”®"""                                      # ç¬¬257è¡Œ
    key = f"{url}_{title}"                              # ç¬¬258è¡Œ
    return hashlib.md5(key.encode()).hexdigest()        # ç¬¬259è¡Œ
```

**ç¬¬256è¡Œè¯¦è§£**ï¼š`def _get_cache_key(self, url: str, title: str) -> str:`

**ç§æœ‰æ–¹æ³•å®šä¹‰è¯¦è§£ï¼ˆç¬¬256è¡Œï¼‰**ï¼š
- `def`ï¼šå®šä¹‰å‡½æ•°çš„å…³é”®å­—
- `_get_cache_key`ï¼šæ–¹æ³•åï¼Œä»¥å•ä¸‹åˆ’çº¿å¼€å¤´è¡¨ç¤º**ç§æœ‰æ–¹æ³•**ï¼ˆçº¦å®šï¼‰
- `self`ï¼šç¬¬ä¸€ä¸ªå‚æ•°ï¼Œä»£è¡¨å¯¹è±¡è‡ªèº«
- `url: str`ï¼šç¬¬äºŒä¸ªå‚æ•°ï¼ŒURLå­—ç¬¦ä¸²ï¼Œå¸¦ç±»å‹æ³¨è§£
- `title: str`ï¼šç¬¬ä¸‰ä¸ªå‚æ•°ï¼Œæ ‡é¢˜å­—ç¬¦ä¸²ï¼Œå¸¦ç±»å‹æ³¨è§£
- `-> str`ï¼šè¿”å›å€¼ç±»å‹æ³¨è§£ï¼Œè¡¨ç¤ºè¿”å›å­—ç¬¦ä¸²

**Pythonå‘½åçº¦å®š**ï¼š
```python
# 1. å…¬æœ‰æ–¹æ³•ï¼ˆpublicï¼‰- ä»»ä½•äººéƒ½å¯ä»¥è°ƒç”¨
class Cache:
    def get(self, url, title):  # å…¬æœ‰æ–¹æ³•ï¼Œæ— ä¸‹åˆ’çº¿å‰ç¼€
        """å¤–éƒ¨å¯ä»¥ç›´æ¥è°ƒç”¨"""
        pass

cache = Cache()
cache.get("url", "title")  # âœ… æ¨èä½¿ç”¨

# 2. ç§æœ‰æ–¹æ³•ï¼ˆprivateï¼‰- åªåœ¨ç±»å†…éƒ¨ä½¿ç”¨
class Cache:
    def _get_cache_key(self, url, title):  # å•ä¸‹åˆ’çº¿ï¼Œè¡¨ç¤ºç§æœ‰
        """å†…éƒ¨è¾…åŠ©æ–¹æ³•ï¼Œä¸åº”è¯¥è¢«å¤–éƒ¨ç›´æ¥è°ƒç”¨"""
        return hashlib.md5(f"{url}_{title}".encode()).hexdigest()
    
    def get(self, url, title):
        key = self._get_cache_key(url, title)  # âœ… ç±»å†…éƒ¨è°ƒç”¨
        # ...

cache = Cache()
cache._get_cache_key("url", "title")  # âš ï¸ æŠ€æœ¯ä¸Šå¯ä»¥ï¼Œä½†ä¸æ¨è

# 3. å¼ºç§æœ‰æ–¹æ³•ï¼ˆname manglingï¼‰- åŒä¸‹åˆ’çº¿
class Cache:
    def __internal_method(self):  # åŒä¸‹åˆ’çº¿ï¼Œåç§°æ”¹ç¼–
        pass

# Pythonä¼šå°†æ–¹æ³•åæ”¹ä¸º _Cache__internal_method
# æ›´éš¾ä»å¤–éƒ¨è®¿é—®ï¼Œä½†ä»ç„¶å¯ä»¥

# å‘½åçº¦å®šæ€»ç»“ï¼š
# public_method    - å…¬æœ‰ï¼Œä¾›å¤–éƒ¨ä½¿ç”¨
# _private_method  - ç§æœ‰ï¼Œä»…å†…éƒ¨ä½¿ç”¨ï¼ˆçº¦å®šï¼‰
# __name_mangling  - å¼ºç§æœ‰ï¼Œåç§°æ”¹ç¼–
# __special__      - ç‰¹æ®Šæ–¹æ³•ï¼ŒPythonå†…ç½®ï¼ˆå¦‚__init__ï¼‰
```

**ç¬¬258è¡Œè¯¦è§£**ï¼š`key = f"{url}_{title}"`

**f-stringæ ¼å¼åŒ–å­—ç¬¦ä¸²è¯¦è§£ï¼ˆç¬¬258è¡Œï¼‰**ï¼š
- `key`ï¼šå˜é‡åï¼Œå­˜å‚¨ç»„åˆåçš„å­—ç¬¦ä¸²
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `f"..."`ï¼šf-stringï¼ŒPython 3.6+çš„å­—ç¬¦ä¸²æ ¼å¼åŒ–è¯­æ³•
- `{url}`ï¼šå ä½ç¬¦ï¼Œæ’å…¥urlå˜é‡çš„å€¼
- `_`ï¼šä¸‹åˆ’çº¿åˆ†éš”ç¬¦
- `{title}`ï¼šå ä½ç¬¦ï¼Œæ’å…¥titleå˜é‡çš„å€¼

**f-stringè¯¦ç»†è®²è§£**ï¼š
```python
# f-stringçš„å¤šç§ç”¨æ³•
url = "https://ieee.org/paper/123"
title = "Deep Learning"

# æ–¹å¼1: f-stringï¼ˆæ¨èï¼ŒPython 3.6+ï¼‰
key = f"{url}_{title}"
# ç»“æœ: "https://ieee.org/paper/123_Deep Learning"

# æ–¹å¼2: formatæ–¹æ³•ï¼ˆPython 2.7+ï¼‰
key = "{}_{}" .format(url, title)
# ç»“æœç›¸åŒ

# æ–¹å¼3: %æ ¼å¼åŒ–ï¼ˆæ—§å¼ï¼Œä¸æ¨èï¼‰
key = "%s_%s" % (url, title)
# ç»“æœç›¸åŒ

# æ–¹å¼4: å­—ç¬¦ä¸²æ‹¼æ¥ï¼ˆæœ€åŸå§‹ï¼‰
key = url + "_" + title
# ç»“æœç›¸åŒ

# f-stringçš„é«˜çº§ç”¨æ³•
year = 2024
pi = 3.14159

# 1. è¡¨è¾¾å¼è®¡ç®—
result = f"2 + 2 = {2 + 2}"  # "2 + 2 = 4"

# 2. è°ƒç”¨å‡½æ•°
result = f"å¤§å†™: {title.upper()}"  # "å¤§å†™: DEEP LEARNING"

# 3. æ ¼å¼åŒ–æ•°å­—
result = f"åœ†å‘¨ç‡: {pi:.2f}"  # "åœ†å‘¨ç‡: 3.14"ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰

# 4. å¯¹é½å’Œå¡«å……
result = f"{year:>10}"  # "      2024"ï¼ˆå³å¯¹é½ï¼Œæ€»å®½åº¦10ï¼‰
result = f"{year:0>10}"  # "0000002024"ï¼ˆå³å¯¹é½ï¼Œç”¨0å¡«å……ï¼‰

# 5. å¤šè¡Œf-string
result = f"""
URL: {url}
æ ‡é¢˜: {title}
å¹´ä»½: {year}
"""

# ä¸ºä»€ä¹ˆç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼Ÿ
# 1. ç®€å•æ˜äº†çš„åˆ†éš”ç¬¦
# 2. ç¡®ä¿URLå’Œæ ‡é¢˜ä¸ä¼šæ··æ·†
# ç¤ºä¾‹ï¼š
#   URL: "https://a.com"  Title: "xyz"  â†’ "https://a.com_xyz"
#   URL: "https://a.comx" Title: "yz"   â†’ "https://a.comx_yz"
#   ä¸¤ä¸ªä¸åŒçš„ç»„åˆï¼Œäº§ç”Ÿä¸åŒçš„key
```

**ç¬¬259è¡Œè¯¦è§£**ï¼š`return hashlib.md5(key.encode()).hexdigest()`

**MD5å“ˆå¸Œç”Ÿæˆè¯¦è§£ï¼ˆç¬¬259è¡Œï¼‰**ï¼š
- `return`ï¼šè¿”å›è¯­å¥
- `hashlib.md5()`ï¼šMD5å“ˆå¸Œå‡½æ•°
- `key.encode()`ï¼šå°†å­—ç¬¦ä¸²ç¼–ç ä¸ºå­—èŠ‚
- `.hexdigest()`ï¼šè¿”å›åå…­è¿›åˆ¶æ ¼å¼çš„å“ˆå¸Œå€¼

**å®Œæ•´æ‰§è¡Œæµç¨‹åˆ†è§£**ï¼š
```python
# æ­¥éª¤1: å‡†å¤‡æ•°æ®
url = "https://ieeexplore.ieee.org/document/12345"
title = "Deep Learning for Computer Vision"
key = f"{url}_{title}"
# key = "https://ieeexplore.ieee.org/document/12345_Deep Learning for Computer Vision"

# æ­¥éª¤2: ç¼–ç ä¸ºå­—èŠ‚ - encode()
key_bytes = key.encode()
# key_bytes = b'https://ieeexplore.ieee.org/document/12345_Deep Learning for Computer Vision'

# ä¸ºä»€ä¹ˆè¦encode()ï¼Ÿ
# MD5ç®—æ³•å¤„ç†çš„æ˜¯å­—èŠ‚æ•°æ®ï¼Œä¸æ˜¯å­—ç¬¦ä¸²
# å­—ç¬¦ä¸² â†’ encode() â†’ å­—èŠ‚

# æ­¥éª¤3: è®¡ç®—MD5å“ˆå¸Œ - hashlib.md5()
md5_object = hashlib.md5(key_bytes)
# md5_object = <md5 HASH object>

# æ­¥éª¤4: è·å–åå…­è¿›åˆ¶å­—ç¬¦ä¸² - hexdigest()
cache_key = md5_object.hexdigest()
# cache_key = "a1b2c3d4e5f6789abcdef0123456789"ï¼ˆ32ä¸ªåå…­è¿›åˆ¶å­—ç¬¦ï¼‰

# ä¸€è¡Œä»£ç å®Œæˆæ‰€æœ‰æ­¥éª¤
cache_key = hashlib.md5(key.encode()).hexdigest()

# MD5å“ˆå¸Œçš„ç‰¹æ€§æ¼”ç¤º
import hashlib

# 1. ç¡®å®šæ€§ï¼šç›¸åŒè¾“å…¥æ€»æ˜¯äº§ç”Ÿç›¸åŒè¾“å‡º
hash1 = hashlib.md5("hello".encode()).hexdigest()
hash2 = hashlib.md5("hello".encode()).hexdigest()
print(hash1 == hash2)  # True

# 2. é›ªå´©æ•ˆåº”ï¼šå¾®å°æ”¹å˜å¯¼è‡´å®Œå…¨ä¸åŒçš„è¾“å‡º
hash_hello = hashlib.md5("hello".encode()).hexdigest()
hash_Hello = hashlib.md5("Hello".encode()).hexdigest()  # åªæ”¹äº†å¤§å°å†™
print(hash_hello)  # "5d41402abc4b2a76b9719d911017c592"
print(hash_Hello)  # "8b1a9953c4611296a827abf8c47804d7"
# å®Œå…¨ä¸åŒï¼

# 3. å›ºå®šé•¿åº¦ï¼šæ— è®ºè¾“å…¥å¤šé•¿ï¼Œè¾“å‡ºéƒ½æ˜¯32ä¸ªå­—ç¬¦
short = hashlib.md5("a".encode()).hexdigest()
long = hashlib.md5("a" * 1000000).encode()).hexdigest()
print(len(short))  # 32
print(len(long))   # 32

# 4. å•å‘æ€§ï¼šæ— æ³•ä»å“ˆå¸Œå€¼åæ¨åŸå§‹æ•°æ®
# å³ä½¿çŸ¥é“hash = "5d41402abc4b2a76b9719d911017c592"
# ä¹Ÿæ— æ³•çŸ¥é“åŸå§‹æ•°æ®æ˜¯"hello"ï¼ˆé™¤éæš´åŠ›ç ´è§£ï¼‰

# ä¸ºä»€ä¹ˆä½¿ç”¨MD5ä½œä¸ºç¼“å­˜é”®ï¼Ÿ
# 1. å›ºå®šé•¿åº¦ï¼šæ— è®ºURLå’Œæ ‡é¢˜å¤šé•¿ï¼Œç¼“å­˜æ–‡ä»¶åéƒ½æ˜¯32å­—ç¬¦
# 2. åˆæ³•æ–‡ä»¶åï¼šåªåŒ…å«[0-9a-f]ï¼Œä¸ä¼šæœ‰ç‰¹æ®Šå­—ç¬¦
# 3. å”¯ä¸€æ ‡è¯†ï¼šä¸åŒçš„URL/æ ‡é¢˜ç»„åˆäº§ç”Ÿä¸åŒçš„é”®
# 4. å¿«é€Ÿè®¡ç®—ï¼šMD5ç®—æ³•å¾ˆå¿«

# å®é™…åº”ç”¨ç¤ºä¾‹
def get_cache_filename(url, title):
    key = f"{url}_{title}"
    hash_key = hashlib.md5(key.encode()).hexdigest()
    return f"{hash_key}.pkl"

# ç¤ºä¾‹1
filename1 = get_cache_filename(
    "https://ieeexplore.ieee.org/document/12345",
    "Deep Learning"
)
print(filename1)  # "7d8f9a2b3c4d5e6f1a2b3c4d5e6f7a8b.pkl"

# ç¤ºä¾‹2ï¼šç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒæ–‡ä»¶å
filename2 = get_cache_filename(
    "https://ieeexplore.ieee.org/document/12345",
    "Deep Learning"
)
print(filename1 == filename2)  # True

# ç¤ºä¾‹3ï¼šä¸åŒè¾“å…¥äº§ç”Ÿä¸åŒæ–‡ä»¶å
filename3 = get_cache_filename(
    "https://ieeexplore.ieee.org/document/67890",
    "Machine Learning"
)
print(filename1 == filename3)  # False
```

#### è·å–ç¼“å­˜æ–¹æ³•ï¼ˆç¬¬261-274è¡Œï¼‰

```python
def get(self, url: str, title: str) -> Optional[str]:  # ç¬¬261è¡Œ
    """è·å–ç¼“å­˜çš„æ‘˜è¦"""                                  # ç¬¬262è¡Œ
    cache_key = self._get_cache_key(url, title)        # ç¬¬263è¡Œ
    cache_file = self.cache_dir / f"{cache_key}.pkl"   # ç¬¬264è¡Œ
    
    if cache_file.exists():                            # ç¬¬266è¡Œ
        try:                                           # ç¬¬267è¡Œ
            with open(cache_file, 'rb') as f:          # ç¬¬268è¡Œ
                data = pickle.load(f)                  # ç¬¬269è¡Œ
                if datetime.now() - data['timestamp'] <= self.cache_duration:  # ç¬¬270è¡Œ
                    return data['abstract']            # ç¬¬271è¡Œ
        except Exception:                              # ç¬¬272è¡Œ
            pass                                       # ç¬¬273è¡Œ
    return None                                        # ç¬¬274è¡Œ
```

**ç¬¬263è¡Œè¯¦è§£**ï¼š`cache_key = self._get_cache_key(url, title)`

**è°ƒç”¨ç§æœ‰æ–¹æ³•è¯¦è§£ï¼ˆç¬¬263è¡Œï¼‰**ï¼š
- `cache_key`ï¼šå˜é‡åï¼Œå­˜å‚¨ç”Ÿæˆçš„ç¼“å­˜é”®
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `self._get_cache_key(url, title)`ï¼šè°ƒç”¨å¯¹è±¡è‡ªå·±çš„ç§æœ‰æ–¹æ³•
- `self`ï¼šè®¿é—®å¯¹è±¡è‡ªå·±çš„æ–¹æ³•
- `_get_cache_key`ï¼šç§æœ‰æ–¹æ³•å
- `(url, title)`ï¼šä¼ é€’å‚æ•°

**selfçš„ä½¿ç”¨è¯¦è§£**ï¼š
```python
class Cache:
    def _get_cache_key(self, url, title):
        """ç§æœ‰æ–¹æ³•ï¼šç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(f"{url}_{title}".encode()).hexdigest()
    
    def get(self, url, title):
        """å…¬æœ‰æ–¹æ³•ï¼šè·å–ç¼“å­˜"""
        # é€šè¿‡selfè°ƒç”¨åŒä¸€ä¸ªå¯¹è±¡çš„å…¶ä»–æ–¹æ³•
        cache_key = self._get_cache_key(url, title)
        # ç­‰ä»·äºï¼š
        # cache_key = Cache._get_cache_key(self, url, title)
        
        print(f"ç¼“å­˜é”®: {cache_key}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
cache = Cache()
result = cache.get("https://example.com", "Title")
# æ‰§è¡Œæµç¨‹ï¼š
# 1. cache.get()è¢«è°ƒç”¨ï¼ŒselfæŒ‡å‘cacheå¯¹è±¡
# 2. åœ¨get()å†…éƒ¨ï¼Œself._get_cache_key()è¢«è°ƒç”¨
# 3. _get_cache_key()æ‰§è¡Œï¼Œselfä»ç„¶æŒ‡å‘cacheå¯¹è±¡
# 4. è¿”å›å“ˆå¸Œå€¼

# ä¸ºä»€ä¹ˆéœ€è¦selfï¼Ÿ
# å› ä¸ºå¯¹è±¡å¯èƒ½æœ‰å¤šä¸ªå®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹éƒ½éœ€è¦è®¿é—®è‡ªå·±çš„æ–¹æ³•
cache1 = Cache("cache1")
cache2 = Cache("cache2")

# cache1.get()ä¸­çš„selfæŒ‡å‘cache1
# cache2.get()ä¸­çš„selfæŒ‡å‘cache2
```

**ç¬¬264è¡Œè¯¦è§£**ï¼š`cache_file = self.cache_dir / f"{cache_key}.pkl"`

**è·¯å¾„æ‹¼æ¥è¯¦è§£ï¼ˆç¬¬264è¡Œï¼‰**ï¼š
- `cache_file`ï¼šå˜é‡åï¼Œå­˜å‚¨å®Œæ•´çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `self.cache_dir`ï¼šPathå¯¹è±¡ï¼Œç¼“å­˜ç›®å½•
- `/`ï¼šè·¯å¾„æ‹¼æ¥è¿ç®—ç¬¦ï¼ˆPathå¯¹è±¡ç‰¹æœ‰ï¼‰
- `f"{cache_key}.pkl"`ï¼šf-stringï¼Œç”Ÿæˆæ–‡ä»¶å

**Pathè·¯å¾„æ‹¼æ¥çš„é­”æ³•**ï¼š
```python
from pathlib import Path

# Pathå¯¹è±¡æ”¯æŒç”¨/è¿ç®—ç¬¦æ‹¼æ¥è·¯å¾„
cache_dir = Path("paper_cache")
cache_key = "a1b2c3d4e5f6"

# æ–¹å¼1: ä½¿ç”¨/è¿ç®—ç¬¦ï¼ˆæ¨èï¼‰
cache_file = cache_dir / f"{cache_key}.pkl"
# ç»“æœ: Path('paper_cache/a1b2c3d4e5f6.pkl')

# æ–¹å¼2: ä½¿ç”¨joinpathæ–¹æ³•
cache_file = cache_dir.joinpath(f"{cache_key}.pkl")
# ç»“æœç›¸åŒ

# æ–¹å¼3: å­—ç¬¦ä¸²æ‹¼æ¥ï¼ˆä¸æ¨èï¼‰
cache_file = str(cache_dir) + "/" + cache_key + ".pkl"
# Windowsä¸‹ä¼šå‡ºé”™ï¼Œå› ä¸ºåº”è¯¥ç”¨åæ–œæ \

# /è¿ç®—ç¬¦çš„ä¼˜åŠ¿
# 1. è‡ªåŠ¨å¤„ç†è·¯å¾„åˆ†éš”ç¬¦
#    Linux/Mac: cache_dir / "file.pkl" â†’ "cache_dir/file.pkl"
#    Windows:   cache_dir / "file.pkl" â†’ "cache_dir\\file.pkl"

# 2. æ”¯æŒè¿ç»­æ‹¼æ¥
path = Path("a") / "b" / "c" / "file.txt"
# ç»“æœ: Path('a/b/c/file.txt')

# 3. æ··åˆå­—ç¬¦ä¸²å’ŒPathå¯¹è±¡
path = Path("cache") / "subdir" / Path("file.pkl")
# å®Œå…¨å¯ä»¥

# å®Œæ•´ç¤ºä¾‹
cache_dir = Path("paper_cache")
cache_key = "7d8f9a2b3c4d5e6f"

# ç”Ÿæˆæ–‡ä»¶è·¯å¾„
cache_file = cache_dir / f"{cache_key}.pkl"
print(cache_file)  # paper_cache/7d8f9a2b3c4d5e6f.pkl

# å¯ä»¥ç›´æ¥ç”¨äºæ–‡ä»¶æ“ä½œ
cache_file.exists()           # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
with open(cache_file, 'rb'):  # æ‰“å¼€æ–‡ä»¶
    pass
```

**ç¬¬266è¡Œè¯¦è§£**ï¼š`if cache_file.exists():`

**æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥è¯¦è§£ï¼ˆç¬¬266è¡Œï¼‰**ï¼š
- `if`ï¼šæ¡ä»¶è¯­å¥å…³é”®å­—
- `cache_file.exists()`ï¼šPathå¯¹è±¡çš„æ–¹æ³•ï¼Œæ£€æŸ¥æ–‡ä»¶/ç›®å½•æ˜¯å¦å­˜åœ¨
- `:`ï¼šæ¡ä»¶è¯­å¥ç»“æŸç¬¦
- è¿”å›å¸ƒå°”å€¼ï¼šTrueï¼ˆå­˜åœ¨ï¼‰æˆ–Falseï¼ˆä¸å­˜åœ¨ï¼‰

**exists()æ–¹æ³•è¯¦è§£**ï¼š
```python
from pathlib import Path

cache_file = Path("paper_cache/abc123.pkl")

# exists() - æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
if cache_file.exists():
    print("æ–‡ä»¶å­˜åœ¨")
else:
    print("æ–‡ä»¶ä¸å­˜åœ¨")

# ç›¸å…³çš„æ£€æŸ¥æ–¹æ³•
cache_file.is_file()     # æ˜¯å¦æ˜¯æ–‡ä»¶
cache_file.is_dir()      # æ˜¯å¦æ˜¯ç›®å½•
cache_file.is_symlink()  # æ˜¯å¦æ˜¯ç¬¦å·é“¾æ¥

# ä¸ºä»€ä¹ˆè¦å…ˆæ£€æŸ¥exists()ï¼Ÿ
# 1. é¿å…FileNotFoundErrorå¼‚å¸¸
if cache_file.exists():
    data = cache_file.read_bytes()  # å®‰å…¨
else:
    print("ç¼“å­˜ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°è·å–")

# 2. é€»è¾‘æ¸…æ™°
# å¦‚æœç¼“å­˜å­˜åœ¨ â†’ è¯»å–ç¼“å­˜
# å¦‚æœç¼“å­˜ä¸å­˜åœ¨ â†’ é‡æ–°è·å–å¹¶ç¼“å­˜

# exists() vs try-except å¯¹æ¯”
# æ–¹å¼1: å…ˆæ£€æŸ¥exists()ï¼ˆæ¨èï¼‰
if cache_file.exists():
    with open(cache_file, 'rb') as f:
        data = pickle.load(f)
else:
    data = None

# æ–¹å¼2: try-exceptï¼ˆä¹Ÿå¯ä»¥ï¼‰
try:
    with open(cache_file, 'rb') as f:
        data = pickle.load(f)
except FileNotFoundError:
    data = None

# åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œä½¿ç”¨exists()æ›´æ¸…æ™°åœ°è¡¨è¾¾æ„å›¾
```

**ç¬¬267-273è¡Œè¯¦è§£**ï¼štry-exceptå¼‚å¸¸å¤„ç†

```python
try:                                           # ç¬¬267è¡Œ
    with open(cache_file, 'rb') as f:          # ç¬¬268è¡Œ
        data = pickle.load(f)                  # ç¬¬269è¡Œ
        if datetime.now() - data['timestamp'] <= self.cache_duration:  # ç¬¬270è¡Œ
            return data['abstract']            # ç¬¬271è¡Œ
except Exception:                              # ç¬¬272è¡Œ
    pass                                       # ç¬¬273è¡Œ
```

**ç¬¬267è¡Œè¯¦è§£**ï¼š`try:`

**tryè¯­å¥è¯¦è§£ï¼ˆç¬¬267è¡Œï¼‰**ï¼š
- `try`ï¼šPythonå…³é”®å­—ï¼Œå¼€å§‹å¼‚å¸¸å¤„ç†å—
- `:`ï¼šè¯­å¥ç»“æŸç¬¦
- tryå—ä¸­çš„ä»£ç å¯èƒ½æŠ›å‡ºå¼‚å¸¸

**å¼‚å¸¸å¤„ç†æœºåˆ¶è¯¦è§£**ï¼š
```python
# å¼‚å¸¸å¤„ç†çš„åŸºæœ¬ç»“æ„
try:
    # å¯èƒ½å‡ºé”™çš„ä»£ç 
    risky_operation()
except SomeException:
    # å¦‚æœå‡ºé”™ï¼Œæ‰§è¡Œè¿™é‡Œçš„ä»£ç 
    handle_error()

# ä¸ºä»€ä¹ˆéœ€è¦try-exceptï¼Ÿ
# ç¨‹åºè¿è¡Œæ—¶å¯èƒ½é‡åˆ°å„ç§é”™è¯¯ï¼š
# - æ–‡ä»¶ä¸å­˜åœ¨
# - ç½‘ç»œè¿æ¥å¤±è´¥
# - æ•°æ®æ ¼å¼é”™è¯¯
# - æƒé™ä¸è¶³
# ç­‰ç­‰...

# ä¸ä½¿ç”¨try-exceptçš„åæœ
file = open('nonexistent.txt', 'r')  # FileNotFoundErrorï¼Œç¨‹åºå´©æºƒï¼
# ç¨‹åºåˆ°è¿™é‡Œå°±åœæ­¢äº†ï¼Œåé¢çš„ä»£ç ä¸ä¼šæ‰§è¡Œ

# ä½¿ç”¨try-except
try:
    file = open('nonexistent.txt', 'r')
except FileNotFoundError:
    print("æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼")
    file = None
# ç¨‹åºç»§ç»­æ‰§è¡Œï¼Œä¸ä¼šå´©æºƒ

# å¤šä¸ªexceptå­å¥
try:
    risky_code()
except FileNotFoundError:
    print("æ–‡ä»¶ä¸å­˜åœ¨")
except PermissionError:
    print("æƒé™ä¸è¶³")
except Exception as e:  # æ•è·æ‰€æœ‰å…¶ä»–å¼‚å¸¸
    print(f"æœªçŸ¥é”™è¯¯: {e}")

# try-except-else-finallyå®Œæ•´ç»“æ„
try:
    file = open('data.txt', 'r')
except FileNotFoundError:
    print("æ–‡ä»¶ä¸å­˜åœ¨")
else:
    # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œæ‰§è¡Œè¿™é‡Œ
    data = file.read()
finally:
    # æ— è®ºæ˜¯å¦å¼‚å¸¸ï¼Œéƒ½æ‰§è¡Œè¿™é‡Œï¼ˆç”¨äºæ¸…ç†èµ„æºï¼‰
    if 'file' in locals():
        file.close()
```

**ç¬¬268è¡Œè¯¦è§£**ï¼š`with open(cache_file, 'rb') as f:`

**withè¯­å¥å’Œæ–‡ä»¶æ‰“å¼€è¯¦è§£ï¼ˆç¬¬268è¡Œï¼‰**ï¼š
- `with`ï¼šPythonå…³é”®å­—ï¼Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
- `open(cache_file, 'rb')`ï¼šæ‰“å¼€æ–‡ä»¶å‡½æ•°
- `'rb'`ï¼šæ–‡ä»¶æ‰“å¼€æ¨¡å¼ï¼ˆread binaryï¼ŒäºŒè¿›åˆ¶è¯»å–ï¼‰
- `as f`ï¼šå°†æ–‡ä»¶å¯¹è±¡èµ‹å€¼ç»™å˜é‡f
- `:`ï¼šwithè¯­å¥ç»“æŸç¬¦

**withè¯­å¥è¯¦ç»†è®²è§£**ï¼š
```python
# withè¯­å¥æ˜¯ä»€ä¹ˆï¼Ÿ
# è‡ªåŠ¨ç®¡ç†èµ„æºï¼ˆæ–‡ä»¶ã€ç½‘ç»œè¿æ¥ç­‰ï¼‰çš„æ‰“å¼€å’Œå…³é—­

# ä¸ä½¿ç”¨withï¼ˆéœ€è¦æ‰‹åŠ¨å…³é—­ï¼‰
f = open('file.txt', 'r')
try:
    data = f.read()
finally:
    f.close()  # å¿…é¡»æ‰‹åŠ¨å…³é—­

# ä½¿ç”¨withï¼ˆè‡ªåŠ¨å…³é—­ï¼‰
with open('file.txt', 'r') as f:
    data = f.read()
# withå—ç»“æŸåï¼Œæ–‡ä»¶è‡ªåŠ¨å…³é—­ï¼Œå³ä½¿å‘ç”Ÿå¼‚å¸¸ä¹Ÿä¼šå…³é—­

# withçš„å·¥ä½œåŸç†
class FileOpener:
    def __enter__(self):
        print("æ‰“å¼€æ–‡ä»¶")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        print("å…³é—­æ–‡ä»¶")
        return False

with FileOpener() as f:
    print("ä½¿ç”¨æ–‡ä»¶")
# è¾“å‡ºï¼š
# æ‰“å¼€æ–‡ä»¶
# ä½¿ç”¨æ–‡ä»¶
# å…³é—­æ–‡ä»¶

# æ–‡ä»¶æ‰“å¼€æ¨¡å¼è¯¦è§£
# 'r'  - åªè¯»æ¨¡å¼ï¼ˆæ–‡æœ¬ï¼‰
# 'w'  - å†™å…¥æ¨¡å¼ï¼ˆæ–‡æœ¬ï¼Œè¦†ç›–ï¼‰
# 'a'  - è¿½åŠ æ¨¡å¼ï¼ˆæ–‡æœ¬ï¼‰
# 'rb' - åªè¯»æ¨¡å¼ï¼ˆäºŒè¿›åˆ¶ï¼‰â† æœ¬è¡Œä½¿ç”¨çš„æ¨¡å¼
# 'wb' - å†™å…¥æ¨¡å¼ï¼ˆäºŒè¿›åˆ¶ï¼Œè¦†ç›–ï¼‰
# 'ab' - è¿½åŠ æ¨¡å¼ï¼ˆäºŒè¿›åˆ¶ï¼‰

# ä¸ºä»€ä¹ˆä½¿ç”¨'rb'ï¼ˆäºŒè¿›åˆ¶æ¨¡å¼ï¼‰ï¼Ÿ
# å› ä¸ºpickleåºåˆ—åŒ–çš„æ•°æ®æ˜¯äºŒè¿›åˆ¶æ ¼å¼ï¼Œä¸æ˜¯æ–‡æœ¬

# æ–‡æœ¬æ¨¡å¼ vs äºŒè¿›åˆ¶æ¨¡å¼
# æ–‡æœ¬æ¨¡å¼('r')ï¼š
with open('text.txt', 'r') as f:
    content = f.read()  # è¿”å›å­—ç¬¦ä¸²str
    print(type(content))  # <class 'str'>

# äºŒè¿›åˆ¶æ¨¡å¼('rb')ï¼š
with open('data.pkl', 'rb') as f:
    content = f.read()  # è¿”å›å­—èŠ‚bytes
    print(type(content))  # <class 'bytes'>

# pickleå¿…é¡»ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼
with open('cache.pkl', 'rb') as f:
    data = pickle.load(f)  # æ­£ç¡®

with open('cache.pkl', 'r') as f:
    data = pickle.load(f)  # é”™è¯¯ï¼ä¼šæŠ¥UnicodeDecodeError
```

**ç¬¬269è¡Œè¯¦è§£**ï¼š`data = pickle.load(f)`

**pickleååºåˆ—åŒ–è¯¦è§£ï¼ˆç¬¬269è¡Œï¼‰**ï¼š
- `data`ï¼šå˜é‡åï¼Œå­˜å‚¨åŠ è½½çš„Pythonå¯¹è±¡
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `pickle.load(f)`ï¼šä»æ–‡ä»¶åŠ è½½Pythonå¯¹è±¡
- `f`ï¼šæ–‡ä»¶å¯¹è±¡ï¼ˆä¸Šä¸€è¡Œwithè¯­å¥åˆ›å»ºçš„ï¼‰

**pickle.load()è¯¦è§£**ï¼š
```python
import pickle

# pickleåºåˆ—åŒ–å’Œååºåˆ—åŒ–çš„å®Œæ•´æµç¨‹

# æ­¥éª¤1: åˆ›å»ºPythonå¯¹è±¡
original_data = {
    'abstract': 'è¿™æ˜¯è®ºæ–‡æ‘˜è¦...',
    'timestamp': datetime.now(),
    'meta': {
        'title': 'è®ºæ–‡æ ‡é¢˜',
        'year': 2024
    }
}

# æ­¥éª¤2: åºåˆ—åŒ–ï¼ˆä¿å­˜ï¼‰- pickle.dump()
with open('cache.pkl', 'wb') as f:  # æ³¨æ„ï¼šwbï¼ˆå†™å…¥äºŒè¿›åˆ¶ï¼‰
    pickle.dump(original_data, f)
# ç°åœ¨cache.pklæ–‡ä»¶åŒ…å«äº†åºåˆ—åŒ–çš„äºŒè¿›åˆ¶æ•°æ®

# æ­¥éª¤3: ååºåˆ—åŒ–ï¼ˆåŠ è½½ï¼‰- pickle.load()
with open('cache.pkl', 'rb') as f:  # æ³¨æ„ï¼šrbï¼ˆè¯»å–äºŒè¿›åˆ¶ï¼‰
    loaded_data = pickle.load(f)

# éªŒè¯ï¼šloaded_dataä¸original_dataå®Œå…¨ç›¸åŒ
print(loaded_data['abstract'])  # 'è¿™æ˜¯è®ºæ–‡æ‘˜è¦...'
print(loaded_data['timestamp'])  # datetimeå¯¹è±¡
print(loaded_data['meta']['year'])  # 2024

# pickle.load()è¿”å›çš„å¯¹è±¡ä¿æŒåŸå§‹ç±»å‹
print(type(loaded_data))  # <class 'dict'>
print(type(loaded_data['timestamp']))  # <class 'datetime.datetime'>

# æœ¬é¡¹ç›®ä¸­çš„å®é™…æ•°æ®ç»“æ„
# ä¿å­˜æ—¶ï¼š
cache_data = {
    'abstract': 'This paper presents a novel approach...',
    'timestamp': datetime(2024, 1, 15, 10, 30, 0)
}
with open('abc123.pkl', 'wb') as f:
    pickle.dump(cache_data, f)

# è¯»å–æ—¶ï¼š
with open('abc123.pkl', 'rb') as f:
    data = pickle.load(f)
    # data = {
    #     'abstract': 'This paper presents...',
    #     'timestamp': datetime(2024, 1, 15, 10, 30, 0)
    # }
```

**ç¬¬270è¡Œè¯¦è§£**ï¼š`if datetime.now() - data['timestamp'] <= self.cache_duration:`

**ç¼“å­˜æœ‰æ•ˆæœŸåˆ¤æ–­è¯¦è§£ï¼ˆç¬¬270è¡Œï¼‰**ï¼š
- `if`ï¼šæ¡ä»¶åˆ¤æ–­å…³é”®å­—
- `datetime.now()`ï¼šè·å–å½“å‰æ—¶é—´
- `-`ï¼šæ—¶é—´å‡æ³•è¿ç®—ç¬¦
- `data['timestamp']`ï¼šç¼“å­˜åˆ›å»ºæ—¶é—´ï¼ˆä»å­—å…¸è·å–ï¼‰
- `<=`ï¼šå°äºç­‰äºæ¯”è¾ƒè¿ç®—ç¬¦
- `self.cache_duration`ï¼šç¼“å­˜æœ‰æ•ˆæœŸï¼ˆtimedeltaå¯¹è±¡ï¼‰

**æ—¶é—´åˆ¤æ–­é€»è¾‘å®Œå…¨æ‹†è§£**ï¼š
```python
from datetime import datetime, timedelta

# å‡è®¾æ•°æ®
cache_created_at = datetime(2024, 1, 10, 10, 0, 0)  # ç¼“å­˜åˆ›å»ºæ—¶é—´
current_time = datetime(2024, 1, 15, 10, 0, 0)      # å½“å‰æ—¶é—´
cache_duration = timedelta(days=7)                  # æœ‰æ•ˆæœŸ7å¤©

# è®¡ç®—æ­¥éª¤åˆ†è§£
# æ­¥éª¤1: è®¡ç®—å·²è¿‡æ—¶é—´
time_passed = current_time - cache_created_at
# time_passed = timedelta(days=5)  # è¿‡äº†5å¤©

# æ­¥éª¤2: æ¯”è¾ƒæ˜¯å¦è¶…è¿‡æœ‰æ•ˆæœŸ
is_valid = time_passed <= cache_duration
# is_valid = timedelta(days=5) <= timedelta(days=7)
# is_valid = True  # 5å¤© â‰¤ 7å¤©ï¼Œç¼“å­˜ä»ç„¶æœ‰æ•ˆ

# æ­¥éª¤3: æ ¹æ®ç»“æœå†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜
if is_valid:
    print("ä½¿ç”¨ç¼“å­˜")
    return data['abstract']
else:
    print("ç¼“å­˜è¿‡æœŸï¼Œé‡æ–°è·å–")
    return None

# å®Œæ•´ç¤ºä¾‹
data = {
    'abstract': 'è®ºæ–‡æ‘˜è¦å†…å®¹...',
    'timestamp': datetime.now() - timedelta(days=3)  # 3å¤©å‰ç¼“å­˜çš„
}
cache_duration = timedelta(days=7)  # æœ‰æ•ˆæœŸ7å¤©

# åˆ¤æ–­
if datetime.now() - data['timestamp'] <= cache_duration:
    print("ç¼“å­˜æœ‰æ•ˆï¼Œä½¿ç”¨ç¼“å­˜")
    abstract = data['abstract']
else:
    print("ç¼“å­˜è¿‡æœŸï¼Œéœ€è¦é‡æ–°è·å–")
    abstract = None

# ä¸åŒæƒ…å†µçš„ç¤ºä¾‹
# æƒ…å†µ1: ç¼“å­˜3å¤©å‰åˆ›å»ºï¼Œæœ‰æ•ˆæœŸ7å¤©
cached_3_days_ago = datetime.now() - timedelta(days=3)
print(datetime.now() - cached_3_days_ago <= timedelta(days=7))  # True

# æƒ…å†µ2: ç¼“å­˜10å¤©å‰åˆ›å»ºï¼Œæœ‰æ•ˆæœŸ7å¤©
cached_10_days_ago = datetime.now() - timedelta(days=10)
print(datetime.now() - cached_10_days_ago <= timedelta(days=7))  # False

# æƒ…å†µ3: ç¼“å­˜åˆšåˆ›å»ºï¼Œæœ‰æ•ˆæœŸ7å¤©
cached_just_now = datetime.now()
print(datetime.now() - cached_just_now <= timedelta(days=7))  # True

# æ—¶é—´æ¯”è¾ƒçš„å¸¸è§æ¨¡å¼
# æ¨¡å¼1: æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
start_time = datetime(2024, 1, 1)
end_time = datetime(2024, 12, 31)
current = datetime.now()
if start_time <= current <= end_time:
    print("åœ¨æ—¶é—´èŒƒå›´å†…")

# æ¨¡å¼2: æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
deadline = datetime(2024, 12, 31)
if datetime.now() > deadline:
    print("å·²è¿‡æœŸ")

# æ¨¡å¼3: æ£€æŸ¥è·ç¦»æŸæ—¶é—´ç‚¹çš„é—´éš”
last_update = datetime(2024, 1, 1)
if datetime.now() - last_update > timedelta(days=30):
    print("è¶…è¿‡30å¤©æœªæ›´æ–°")
```

**ç¬¬271è¡Œè¯¦è§£**ï¼š`return data['abstract']`

**è¿”å›ç¼“å­˜æ•°æ®è¯¦è§£ï¼ˆç¬¬271è¡Œï¼‰**ï¼š
- `return`ï¼šè¿”å›è¯­å¥å…³é”®å­—
- `data['abstract']`ï¼šä»å­—å…¸ä¸­è·å–æ‘˜è¦å†…å®¹
- `data`ï¼špickleåŠ è½½çš„å­—å…¸å¯¹è±¡
- `['abstract']`ï¼šå­—å…¸çš„é”®ï¼Œè®¿é—®æ‘˜è¦å€¼

**å­—å…¸è®¿é—®è¯¦è§£**ï¼š
```python
# å­—å…¸çš„ä¸¤ç§è®¿é—®æ–¹å¼

# æ–¹å¼1: æ–¹æ‹¬å·è®¿é—®ï¼ˆæœ¬è¡Œä½¿ç”¨çš„ï¼‰
data = {'abstract': 'è®ºæ–‡æ‘˜è¦...', 'timestamp': datetime.now()}
abstract = data['abstract']  # 'è®ºæ–‡æ‘˜è¦...'

# å¦‚æœé”®ä¸å­˜åœ¨ï¼Œä¼šæŠ›å‡ºKeyError
# title = data['title']  # KeyError: 'title'

# æ–¹å¼2: get()æ–¹æ³•ï¼ˆæ›´å®‰å…¨ï¼‰
abstract = data.get('abstract')  # 'è®ºæ–‡æ‘˜è¦...'
title = data.get('title')  # Noneï¼ˆé”®ä¸å­˜åœ¨è¿”å›Noneï¼‰
title = data.get('title', 'é»˜è®¤æ ‡é¢˜')  # 'é»˜è®¤æ ‡é¢˜'ï¼ˆæŒ‡å®šé»˜è®¤å€¼ï¼‰

# ä¸ºä»€ä¹ˆè¿™é‡Œç”¨æ–¹æ‹¬å·è€Œä¸æ˜¯get()ï¼Ÿ
# å› ä¸ºæˆ‘ä»¬ç¡®å®š'abstract'é”®ä¸€å®šå­˜åœ¨ï¼ˆæ˜¯æˆ‘ä»¬è‡ªå·±ä¿å­˜çš„ï¼‰
# å¦‚æœä¸å­˜åœ¨ï¼Œè¯´æ˜æ•°æ®æŸåï¼Œåº”è¯¥æŠ›å‡ºå¼‚å¸¸

# returnè¯­å¥çš„æ‰§è¡Œ
def get_cache(self, url, title):
    # ... å‰é¢çš„ä»£ç  ...
    if datetime.now() - data['timestamp'] <= self.cache_duration:
        return data['abstract']  # è¿”å›æ‘˜è¦ï¼Œå‡½æ•°ç»“æŸ
    # returnåé¢çš„ä»£ç ä¸ä¼šæ‰§è¡Œ
    print("è¿™è¡Œæ°¸è¿œä¸ä¼šæ‰§è¡Œ")
    
    return None  # å¦‚æœæ²¡æœ‰ä¸Šé¢çš„returnï¼Œæ‰ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ

# å‡½æ•°å¯ä»¥æœ‰å¤šä¸ªreturn
def check_cache(cache_file):
    if not cache_file.exists():
        return None  # ç¬¬ä¸€ä¸ªreturnï¼šæ–‡ä»¶ä¸å­˜åœ¨
    
    try:
        with open(cache_file, 'rb') as f:
            data = pickle.load(f)
            if is_valid(data):
                return data['abstract']  # ç¬¬äºŒä¸ªreturnï¼šç¼“å­˜æœ‰æ•ˆ
    except:
        pass
    
    return None  # ç¬¬ä¸‰ä¸ªreturnï¼šå…¶ä»–æƒ…å†µ

# æ‰§è¡Œåˆ°ä»»ä½•ä¸€ä¸ªreturnï¼Œå‡½æ•°å°±ç«‹å³ç»“æŸ
```

**ç¬¬272-273è¡Œè¯¦è§£**ï¼š`except Exception:` å’Œ `pass`

**å¼‚å¸¸æ•è·å’Œå¿½ç•¥è¯¦è§£ï¼ˆç¬¬272-273è¡Œï¼‰**ï¼š
- `except Exception`ï¼šæ•è·æ‰€æœ‰å¼‚å¸¸ç±»å‹
- `:`ï¼šexceptè¯­å¥ç»“æŸç¬¦
- `pass`ï¼šPythonå…³é”®å­—ï¼Œä»€ä¹ˆéƒ½ä¸åš

**Exceptionå¼‚å¸¸å±‚æ¬¡**ï¼š
```python
# Pythonå¼‚å¸¸çš„ç»§æ‰¿å±‚æ¬¡
BaseException  # æ‰€æœ‰å¼‚å¸¸çš„åŸºç±»
â”œâ”€â”€ SystemExit  # ç³»ç»Ÿé€€å‡º
â”œâ”€â”€ KeyboardInterrupt  # Ctrl+Cä¸­æ–­
â””â”€â”€ Exception  # å¸¸è§„å¼‚å¸¸çš„åŸºç±» â† except Exceptionæ•è·è¿™ä¸€å±‚åŠä»¥ä¸‹
    â”œâ”€â”€ FileNotFoundError  # æ–‡ä»¶æœªæ‰¾åˆ°
    â”œâ”€â”€ PermissionError  # æƒé™é”™è¯¯
    â”œâ”€â”€ ValueError  # å€¼é”™è¯¯
    â”œâ”€â”€ TypeError  # ç±»å‹é”™è¯¯
    â”œâ”€â”€ KeyError  # å­—å…¸é”®é”™è¯¯
    â””â”€â”€ ... è¿˜æœ‰å¾ˆå¤š

# æ•è·ä¸åŒçº§åˆ«çš„å¼‚å¸¸

# 1. æ•è·ç‰¹å®šå¼‚å¸¸ï¼ˆæœ€ç²¾ç¡®ï¼‰
try:
    file = open('nonexistent.txt')
except FileNotFoundError:
    print("æ–‡ä»¶ä¸å­˜åœ¨")

# 2. æ•è·å¤šä¸ªç‰¹å®šå¼‚å¸¸
try:
    risky_operation()
except (FileNotFoundError, PermissionError):
    print("æ–‡ä»¶ç›¸å…³é”™è¯¯")

# 3. æ•è·æ‰€æœ‰Exceptionï¼ˆæœ¬è¡Œä½¿ç”¨çš„ï¼‰
try:
    anything_could_go_wrong()
except Exception:
    pass  # å¿½ç•¥æ‰€æœ‰é”™è¯¯

# 4. è·å–å¼‚å¸¸å¯¹è±¡
try:
    risky_code()
except Exception as e:
    print(f"é”™è¯¯: {e}")
    print(f"é”™è¯¯ç±»å‹: {type(e)}")

# passè¯­å¥è¯¦è§£
# pass = "ä»€ä¹ˆéƒ½ä¸åš"

# ç¤ºä¾‹1: å ä½ç¬¦
def function_not_implemented_yet():
    pass  # æš‚æ—¶ä»€ä¹ˆéƒ½ä¸åšï¼Œä»¥åå†å®ç°

# ç¤ºä¾‹2: ç©ºçš„exceptå—
try:
    risky_code()
except Exception:
    pass  # å¿½ç•¥é”™è¯¯ï¼Œç»§ç»­æ‰§è¡Œ

# ç¤ºä¾‹3: ç©ºçš„ç±»
class EmptyClass:
    pass  # å…ˆå®šä¹‰ç±»ç»“æ„ï¼Œä»¥åæ·»åŠ æ–¹æ³•

# ä¸ºä»€ä¹ˆè¿™é‡Œç”¨except Exception: passï¼Ÿ
# 1. ç¼“å­˜è¯»å–å¤±è´¥ä¸æ˜¯è‡´å‘½é”™è¯¯
# 2. å³ä½¿ç¼“å­˜æŸåï¼Œç¨‹åºä¹Ÿåº”è¯¥ç»§ç»­è¿è¡Œ
# 3. æœ€åæƒ…å†µï¼šå¿½ç•¥ç¼“å­˜ï¼Œé‡æ–°è·å–æ‘˜è¦

# å¯èƒ½çš„å¼‚å¸¸æƒ…å†µ
# - cache.pklæ–‡ä»¶æŸåï¼ˆpickle.load()å¤±è´¥ï¼‰
# - ç£ç›˜è¯»å–é”™è¯¯
# - æƒé™ä¸è¶³
# - æ•°æ®æ ¼å¼æ”¹å˜ï¼ˆæ—§ç‰ˆæœ¬çš„ç¼“å­˜ï¼‰

# æ›´è¯¦ç»†çš„å¼‚å¸¸å¤„ç†ï¼ˆå¯¹æ¯”ï¼‰
try:
    with open(cache_file, 'rb') as f:
        data = pickle.load(f)
        if datetime.now() - data['timestamp'] <= self.cache_duration:
            return data['abstract']
except FileNotFoundError:
    print("ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨")
except pickle.UnpicklingError:
    print("ç¼“å­˜æ–‡ä»¶æŸå")
except KeyError:
    print("ç¼“å­˜æ•°æ®ç»“æ„é”™è¯¯")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")

# æœ¬é¡¹ç›®é€‰æ‹©ç®€å•å¤„ç†ï¼š
# except Exception: pass
# åŸå› ï¼šç¼“å­˜å¤±è´¥ä¸é‡è¦ï¼Œé‡æ–°è·å–å°±å¥½
```

**ç¬¬274è¡Œè¯¦è§£**ï¼š`return None`

**è¿”å›Noneè¯¦è§£ï¼ˆç¬¬274è¡Œï¼‰**ï¼š
- `return`ï¼šè¿”å›è¯­å¥
- `None`ï¼šPythonå†…ç½®å¸¸é‡ï¼Œè¡¨ç¤º"ç©º"æˆ–"æ— å€¼"

**Noneçš„å«ä¹‰å’Œç”¨æ³•**ï¼š
```python
# Noneæ˜¯ä»€ä¹ˆï¼Ÿ
# Pythonçš„ç‰¹æ®Šå€¼ï¼Œè¡¨ç¤º"æ²¡æœ‰å€¼"ã€"ç©º"ã€"ä¸å­˜åœ¨"

# Noneçš„ç±»å‹
print(type(None))  # <class 'NoneType'>

# Noneçš„ä½¿ç”¨åœºæ™¯

# 1. å‡½æ•°æ²¡æœ‰è¿”å›å€¼
def no_return():
    print("Hello")
# éšå¼è¿”å›None

result = no_return()
print(result)  # None

# 2. æ˜ç¡®è¡¨ç¤º"æ‰¾ä¸åˆ°"æˆ–"ä¸å­˜åœ¨"
def find_user(user_id):
    if user_id == 1:
        return {"name": "Alice"}
    else:
        return None  # æ‰¾ä¸åˆ°ç”¨æˆ·

user = find_user(999)
if user is None:
    print("ç”¨æˆ·ä¸å­˜åœ¨")

# 3. å˜é‡åˆå§‹åŒ–
result = None  # å…ˆåˆå§‹åŒ–ä¸ºNone
if some_condition:
    result = get_value()

# Noneçš„åˆ¤æ–­ï¼ˆé‡è¦ï¼ï¼‰

# æ­£ç¡®çš„æ–¹å¼ï¼šä½¿ç”¨is
if result is None:
    print("ç»“æœä¸ºNone")

if result is not None:
    print("ç»“æœä¸ä¸ºNone")

# é”™è¯¯çš„æ–¹å¼ï¼šä½¿ç”¨==ï¼ˆè™½ç„¶ä¹Ÿèƒ½workï¼‰
if result == None:  # ä¸æ¨è
    print("ç»“æœä¸ºNone")

# ä¸ºä»€ä¹ˆè¦ç”¨isè€Œä¸æ˜¯==ï¼Ÿ
# isï¼šæ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªå¯¹è±¡ï¼ˆèº«ä»½æ¯”è¾ƒï¼‰
# ==ï¼šæ£€æŸ¥å€¼æ˜¯å¦ç›¸ç­‰ï¼ˆå€¼æ¯”è¾ƒï¼‰
# Noneæ˜¯å•ä¾‹ï¼Œæ°¸è¿œåªæœ‰ä¸€ä¸ªNoneå¯¹è±¡ï¼Œæ‰€ä»¥ç”¨isæ›´å‡†ç¡®

# Noneåœ¨å¸ƒå°”ä¸Šä¸‹æ–‡ä¸­
# Noneè¢«è§†ä¸ºFalse
if None:
    print("ä¸ä¼šæ‰§è¡Œ")

if not None:
    print("ä¼šæ‰§è¡Œ")

# ä½†æ˜¯ï¼Noneä¸ç­‰äºFalse
print(None == False)  # False
print(None is False)  # False

# æœ¬é¡¹ç›®ä¸­return Noneçš„å«ä¹‰
def get(self, url, title):
    # å°è¯•ä»ç¼“å­˜è·å–
    if cache_exists and cache_valid:
        return data['abstract']  # æ‰¾åˆ°äº†ï¼Œè¿”å›æ‘˜è¦
    else:
        return None  # æ²¡æ‰¾åˆ°æˆ–è¿‡æœŸï¼Œè¿”å›None

# è°ƒç”¨è€…çš„å¤„ç†
abstract = cache.get(url, title)
if abstract is None:
    # ç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°è·å–
    abstract = fetch_from_website(url)
else:
    # ä½¿ç”¨ç¼“å­˜çš„æ‘˜è¦
    print(f"ä»ç¼“å­˜è·å–: {abstract}")

# Optional[str]ç±»å‹æ³¨è§£çš„å«ä¹‰
def get(self, url: str, title: str) -> Optional[str]:
    # Optional[str]è¡¨ç¤ºè¿”å›å€¼å¯èƒ½æ˜¯ï¼š
    # 1. strç±»å‹çš„å­—ç¬¦ä¸²
    # 2. None
    pass

# ç­‰ä»·äº
from typing import Union
def get(self, url: str, title: str) -> Union[str, None]:
    pass
```

#### è®¾ç½®ç¼“å­˜æ–¹æ³•ï¼ˆç¬¬276-288è¡Œï¼‰

```python
def set(self, url: str, title: str, abstract: str):  # ç¬¬276è¡Œ
    """è®¾ç½®ç¼“å­˜"""                                     # ç¬¬277è¡Œ
    cache_key = self._get_cache_key(url, title)      # ç¬¬278è¡Œ
    cache_file = self.cache_dir / f"{cache_key}.pkl" # ç¬¬279è¡Œ
    
    try:                                             # ç¬¬281è¡Œ
        with open(cache_file, 'wb') as f:            # ç¬¬282è¡Œ
            pickle.dump({                            # ç¬¬283è¡Œ
                'abstract': abstract,                # ç¬¬284è¡Œ
                'timestamp': datetime.now()          # ç¬¬285è¡Œ
            }, f)                                    # ç¬¬286è¡Œ
    except Exception:                                # ç¬¬287è¡Œ
        pass                                         # ç¬¬288è¡Œ
```

**ç¬¬282è¡Œè¯¦è§£**ï¼š`with open(cache_file, 'wb') as f:`

**äºŒè¿›åˆ¶å†™å…¥æ¨¡å¼è¯¦è§£ï¼ˆç¬¬282è¡Œï¼‰**ï¼š
- `with open()`ï¼šwithè¯­å¥æ‰“å¼€æ–‡ä»¶
- `cache_file`ï¼šPathå¯¹è±¡ï¼Œæ–‡ä»¶è·¯å¾„
- `'wb'`ï¼šæ–‡ä»¶æ‰“å¼€æ¨¡å¼ï¼ˆwrite binaryï¼ŒäºŒè¿›åˆ¶å†™å…¥ï¼‰
- `as f`ï¼šæ–‡ä»¶å¯¹è±¡èµ‹å€¼ç»™å˜é‡f

**'wb'æ¨¡å¼è¯¦è§£**ï¼š
```python
# æ–‡ä»¶æ‰“å¼€æ¨¡å¼å¯¹æ¯”
# 'r'  - åªè¯»ï¼ˆæ–‡æœ¬ï¼‰read
# 'w'  - å†™å…¥ï¼ˆæ–‡æœ¬ï¼‰writeï¼Œæ–‡ä»¶å·²å­˜åœ¨åˆ™è¦†ç›–
# 'a'  - è¿½åŠ ï¼ˆæ–‡æœ¬ï¼‰append
# 'rb' - åªè¯»ï¼ˆäºŒè¿›åˆ¶ï¼‰read binary
# 'wb' - å†™å…¥ï¼ˆäºŒè¿›åˆ¶ï¼‰write binary â† æœ¬è¡Œä½¿ç”¨
# 'ab' - è¿½åŠ ï¼ˆäºŒè¿›åˆ¶ï¼‰append binary

# ä¸ºä»€ä¹ˆä½¿ç”¨'wb'ï¼Ÿ
# 1. pickleéœ€è¦äºŒè¿›åˆ¶æ¨¡å¼
# 2. 'w'è¡¨ç¤ºå†™å…¥ï¼Œä¼šè¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼‰

# 'w'æ¨¡å¼çš„è¡Œä¸º
# æƒ…å†µ1: æ–‡ä»¶ä¸å­˜åœ¨
with open('new_file.pkl', 'wb') as f:
    f.write(b'data')
# ç»“æœï¼šåˆ›å»ºæ–°æ–‡ä»¶

# æƒ…å†µ2: æ–‡ä»¶å·²å­˜åœ¨
with open('existing_file.pkl', 'wb') as f:
    f.write(b'new data')
# ç»“æœï¼šè¦†ç›–æ—§æ–‡ä»¶ï¼Œæ—§å†…å®¹ä¸¢å¤±

# å¦‚æœæƒ³ä¿ç•™æ—§å†…å®¹ï¼Œä½¿ç”¨'ab'ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
with open('file.pkl', 'ab') as f:
    f.write(b'appended data')
# ç»“æœï¼šåœ¨æ–‡ä»¶æœ«å°¾è¿½åŠ ï¼Œä¿ç•™æ—§å†…å®¹

# æ–‡æœ¬æ¨¡å¼vsäºŒè¿›åˆ¶æ¨¡å¼çš„åŒºåˆ«

# æ–‡æœ¬æ¨¡å¼('w')ï¼š
with open('text.txt', 'w') as f:
    f.write("Hello")  # å†™å…¥å­—ç¬¦ä¸²

# äºŒè¿›åˆ¶æ¨¡å¼('wb')ï¼š
with open('binary.pkl', 'wb') as f:
    f.write(b"Hello")  # å†™å…¥å­—èŠ‚ï¼ˆæ³¨æ„bå‰ç¼€ï¼‰
    # f.write("Hello")  # é”™è¯¯ï¼å¿…é¡»æ˜¯å­—èŠ‚

# pickleä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼
data = {'key': 'value'}
with open('cache.pkl', 'wb') as f:
    pickle.dump(data, f)  # pickle.dumpè‡ªåŠ¨å¤„ç†äºŒè¿›åˆ¶è½¬æ¢
```

**ç¬¬283-286è¡Œè¯¦è§£**ï¼špickle.dump()åºåˆ—åŒ–

```python
pickle.dump({                            # ç¬¬283è¡Œ
    'abstract': abstract,                # ç¬¬284è¡Œ
    'timestamp': datetime.now()          # ç¬¬285è¡Œ
}, f)                                    # ç¬¬286è¡Œ
```

**pickle.dump()è¯¦ç»†è®²è§£**ï¼š
```python
import pickle
from datetime import datetime

# pickle.dump()çš„è¯­æ³•
# pickle.dump(obj, file)
# obj: è¦åºåˆ—åŒ–çš„Pythonå¯¹è±¡
# file: æ–‡ä»¶å¯¹è±¡

# æœ¬ä»£ç åˆ†è§£
abstract = "This paper presents a novel approach..."

# æ­¥éª¤1: åˆ›å»ºè¦ä¿å­˜çš„å­—å…¸
cache_data = {
    'abstract': abstract,
    'timestamp': datetime.now()
}

# æ­¥éª¤2: æ‰“å¼€æ–‡ä»¶ï¼ˆäºŒè¿›åˆ¶å†™å…¥æ¨¡å¼ï¼‰
cache_file = Path("paper_cache/abc123.pkl")
with open(cache_file, 'wb') as f:
    # æ­¥éª¤3: åºåˆ—åŒ–å¹¶ä¿å­˜
    pickle.dump(cache_data, f)

# å®Œæ•´æµç¨‹ç¤ºä¾‹
# ä¿å­˜
data_to_save = {
    'abstract': 'Deep learning is...',
    'timestamp': datetime(2024, 1, 15, 10, 30, 0),
    'metadata': {
        'title': 'AI Research',
        'year': 2024,
        'authors': ['Alice', 'Bob']
    }
}

with open('cache.pkl', 'wb') as f:
    pickle.dump(data_to_save, f)

# è¯»å–
with open('cache.pkl', 'rb') as f:
    loaded_data = pickle.load(f)

# éªŒè¯ï¼šå®Œå…¨æ¢å¤
print(loaded_data['abstract'])  # 'Deep learning is...'
print(loaded_data['timestamp'])  # datetime(2024, 1, 15, 10, 30, 0)
print(loaded_data['metadata']['authors'])  # ['Alice', 'Bob']

# pickleæ”¯æŒçš„æ•°æ®ç±»å‹
# âœ“ åŸºæœ¬ç±»å‹: int, float, str, bool, None
# âœ“ å®¹å™¨ç±»å‹: list, tuple, dict, set
# âœ“ æ—¥æœŸæ—¶é—´: datetime, timedelta
# âœ“ è‡ªå®šä¹‰ç±»çš„å®ä¾‹
# âœ— æ–‡ä»¶å¯¹è±¡ã€ç½‘ç»œè¿æ¥ï¼ˆæ— æ³•åºåˆ—åŒ–ï¼‰

# æœ¬é¡¹ç›®çš„æ•°æ®ç»“æ„
cache_structure = {
    'abstract': str,        # æ‘˜è¦æ–‡æœ¬
    'timestamp': datetime   # ç¼“å­˜åˆ›å»ºæ—¶é—´
}

# ä¸ºä»€ä¹ˆè¦ä¿å­˜timestampï¼Ÿ
# ç”¨äºåˆ¤æ–­ç¼“å­˜æ˜¯å¦è¿‡æœŸ
# è¯»å–æ—¶æ£€æŸ¥ï¼šdatetime.now() - timestamp <= cache_duration
```

**Cacheç±»å®Œæ•´ä½¿ç”¨æµç¨‹**ï¼š
```python
from pathlib import Path
from datetime import datetime, timedelta
import pickle
import hashlib

# åˆ›å»ºç¼“å­˜å¯¹è±¡
cache = Cache("paper_cache")

# åœºæ™¯1: è®¾ç½®ç¼“å­˜
url = "https://ieeexplore.ieee.org/document/12345"
title = "Deep Learning for Computer Vision"
abstract = "This paper presents a novel deep learning approach..."

cache.set(url, title, abstract)
# æ‰§è¡Œæµç¨‹ï¼š
# 1. ç”Ÿæˆç¼“å­˜é”®ï¼š_get_cache_key(url, title) â†’ "7d8f9a2b..."
# 2. ç”Ÿæˆæ–‡ä»¶è·¯å¾„ï¼špaper_cache/7d8f9a2b....pkl
# 3. ä¿å­˜æ•°æ®ï¼š
#    {
#        'abstract': abstract,
#        'timestamp': datetime.now()
#    }

# åœºæ™¯2: è·å–ç¼“å­˜ï¼ˆç¼“å­˜æœ‰æ•ˆï¼‰
cached_abstract = cache.get(url, title)
if cached_abstract:
    print(f"ä»ç¼“å­˜è·å–: {cached_abstract}")
else:
    print("ç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

# åœºæ™¯3: å®Œæ•´çš„ä½¿ç”¨æ¨¡å¼
def get_paper_abstract(url, title):
    """è·å–è®ºæ–‡æ‘˜è¦ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    # æ­¥éª¤1: å°è¯•ä»ç¼“å­˜è·å–
    cached = cache.get(url, title)
    if cached:
        print("âœ“ ä½¿ç”¨ç¼“å­˜")
        return cached
    
    # æ­¥éª¤2: ç¼“å­˜ä¸å­˜åœ¨ï¼Œä»ç½‘ç«™è·å–
    print("âœ— ç¼“å­˜ä¸å­˜åœ¨ï¼Œä»ç½‘ç«™è·å–...")
    abstract = fetch_from_website(url)
    
    # æ­¥éª¤3: ä¿å­˜åˆ°ç¼“å­˜
    if abstract:
        cache.set(url, title, abstract)
        print("âœ“ å·²ç¼“å­˜")
    
    return abstract

# ä½¿ç”¨ç¤ºä¾‹
abstract1 = get_paper_abstract(url1, title1)  # ç¬¬ä¸€æ¬¡ï¼šä»ç½‘ç«™è·å–å¹¶ç¼“å­˜
abstract2 = get_paper_abstract(url1, title1)  # ç¬¬äºŒæ¬¡ï¼šä»ç¼“å­˜è·å–ï¼ˆå¿«ï¼ï¼‰

# ç¼“å­˜çš„ä¼˜åŠ¿
# 1. é€Ÿåº¦ï¼šä»ç£ç›˜è¯»å–ï¼ˆå‡ æ¯«ç§’ï¼‰vs ç½‘ç»œè¯·æ±‚ï¼ˆå‡ ç§’ï¼‰
# 2. å¯é ï¼šç½‘ç»œå¯èƒ½å¤±è´¥ï¼Œç¼“å­˜æ€»æ˜¯å¯ç”¨
# 3. ç¤¼è²Œï¼šå‡å°‘å¯¹ç½‘ç«™æœåŠ¡å™¨çš„è¯·æ±‚å‹åŠ›
# 4. èŠ‚çœï¼šå‡å°‘å¸¦å®½æ¶ˆè€—

# ç¼“å­˜çš„è®¾è®¡å†³ç­–
# Q: ä¸ºä»€ä¹ˆç¼“å­˜æœ‰æ•ˆæœŸæ˜¯7å¤©ï¼Ÿ
# A: å¹³è¡¡æ–°é²œåº¦å’Œæ€§èƒ½ã€‚è®ºæ–‡æ‘˜è¦é€šå¸¸ä¸ä¼šå˜åŒ–ã€‚

# Q: ä¸ºä»€ä¹ˆç”¨MD5è€Œä¸æ˜¯ç›´æ¥ç”¨URLä½œä¸ºæ–‡ä»¶åï¼Ÿ
# A: URLå¯èƒ½åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œä¸é€‚åˆåšæ–‡ä»¶åã€‚
#    MD5ä¿è¯æ–‡ä»¶ååˆæ³•ä¸”é•¿åº¦å›ºå®šã€‚

# Q: ä¸ºä»€ä¹ˆç”¨pickleè€Œä¸æ˜¯JSONï¼Ÿ
# A: pickleå¯ä»¥ç›´æ¥ä¿å­˜datetimeå¯¹è±¡ï¼ŒJSONä¸è¡Œã€‚
#    JSONéœ€è¦æ‰‹åŠ¨è½¬æ¢ï¼šdatetime â†’ å­—ç¬¦ä¸² â†’ datetime

# Q: ä¸ºä»€ä¹ˆå¼‚å¸¸æ—¶ç”¨passè€Œä¸æ˜¯è®°å½•æ—¥å¿—ï¼Ÿ
# A: ç¼“å­˜å¤±è´¥ä¸æ˜¯å…³é”®é”™è¯¯ï¼Œé‡æ–°è·å–å³å¯ã€‚
#    è®°å½•æ—¥å¿—ä¼šå¢åŠ ä»£ç å¤æ‚åº¦ã€‚
```

---

## ğŸš€ Analyseræ ¸å¿ƒåˆ†æç±»

### ç±»å˜é‡å’Œåˆå§‹åŒ–ï¼ˆç¬¬305-314è¡Œï¼‰

```python
class Analyser:                                      # ç¬¬305è¡Œ
    # åˆå§‹åŒ–ç¼“å­˜                                        # ç¬¬306è¡Œ
    _cache = Cache()                                 # ç¬¬307è¡Œ
    _access_counts = defaultdict(int)                # ç¬¬308è¡Œ
    _max_attempts = 3                                # ç¬¬309è¡Œ
    _last_access_time = defaultdict(float)           # ç¬¬310è¡Œ
    _proxy_success_rate = {}                         # ç¬¬311è¡Œ
    _proxy_attempts = defaultdict(int)               # ç¬¬312è¡Œ
    _proxy_successes = defaultdict(int)              # ç¬¬313è¡Œ
    _failed_proxies = set()                          # ç¬¬314è¡Œ
```

**ç±»å˜é‡ vs å®ä¾‹å˜é‡è¯¦è§£**ï¼š
```python
classç¤ºä¾‹:
    # ç±»å˜é‡ï¼ˆæ‰€æœ‰å®ä¾‹å…±äº«ï¼‰
    class_var = "æˆ‘æ˜¯ç±»å˜é‡"
    
    def __init__(self):
        # å®ä¾‹å˜é‡ï¼ˆæ¯ä¸ªå®ä¾‹ç‹¬ç«‹ï¼‰
        self.instance_var = "æˆ‘æ˜¯å®ä¾‹å˜é‡"

# ç±»å˜é‡çš„ç‰¹ç‚¹
obj1 = ç¤ºä¾‹()
obj2 = ç¤ºä¾‹()

# æ‰€æœ‰å®ä¾‹å…±äº«åŒä¸€ä¸ªç±»å˜é‡
print(obj1.class_var)  # "æˆ‘æ˜¯ç±»å˜é‡"
print(obj2.class_var)  # "æˆ‘æ˜¯ç±»å˜é‡"

ç¤ºä¾‹.class_var = "ä¿®æ”¹äº†"
print(obj1.class_var)  # "ä¿®æ”¹äº†"ï¼ˆæ‰€æœ‰å®ä¾‹éƒ½æ”¹å˜ï¼‰
print(obj2.class_var)  # "ä¿®æ”¹äº†"

# å®ä¾‹å˜é‡æ˜¯ç‹¬ç«‹çš„
obj1.instance_var = "obj1çš„å€¼"
obj2.instance_var = "obj2çš„å€¼"
print(obj1.instance_var)  # "obj1çš„å€¼"
print(obj2.instance_var)  # "obj2çš„å€¼"ï¼ˆäº’ä¸å½±å“ï¼‰

# æœ¬é¡¹ç›®ä¸ºä»€ä¹ˆä½¿ç”¨ç±»å˜é‡ï¼Ÿ
# å› ä¸ºæ‰€æœ‰Analyserçš„ä½¿ç”¨éƒ½åº”è¯¥å…±äº«åŒä¸€ä¸ªï¼š
# - ç¼“å­˜ç³»ç»Ÿï¼ˆé¿å…é‡å¤ç¼“å­˜ï¼‰
# - è®¿é—®è®¡æ•°ï¼ˆå…¨å±€é™æµï¼‰
# - ä»£ç†çŠ¶æ€ï¼ˆå…¨å±€ä»£ç†ç®¡ç†ï¼‰
```

**ç¬¬307è¡Œè¯¦è§£**ï¼š`_cache = Cache()`

**å•ä¾‹ç¼“å­˜è¯¦è§£ï¼ˆç¬¬307è¡Œï¼‰**ï¼š
- `_cache`ï¼šç±»å˜é‡åï¼Œä»¥å•ä¸‹åˆ’çº¿å¼€å¤´è¡¨ç¤ºç§æœ‰
- `=`ï¼šèµ‹å€¼è¿ç®—ç¬¦
- `Cache()`ï¼šåˆ›å»ºCacheå¯¹è±¡å®ä¾‹

```python
# å•ä¾‹æ¨¡å¼çš„åº”ç”¨
class Analyser:
    _cache = Cache()  # ç±»å˜é‡ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«åŒä¸€ä¸ªCacheå¯¹è±¡

# æ•ˆæœ
analyser1 = Analyser()
analyser2 = Analyser()

# ä¸¤ä¸ªanalyserä½¿ç”¨åŒä¸€ä¸ªç¼“å­˜
analyser1.run(url1, title1)  # ç¼“å­˜æ‘˜è¦
analyser2.run(url1, title1)  # ä½¿ç”¨ç›¸åŒçš„ç¼“å­˜ï¼ˆç«‹å³è¿”å›ï¼‰

# è¿™ç§è®¾è®¡çš„ä¼˜åŠ¿
# 1. èŠ‚çœå†…å­˜ï¼šåªæœ‰ä¸€ä¸ªCacheå¯¹è±¡
# 2. ç¼“å­˜å…±äº«ï¼šä¸åŒçš„Analyserå®ä¾‹å¯ä»¥å¤ç”¨ç¼“å­˜
# 3. ä¸€è‡´æ€§ï¼šæ‰€æœ‰æ“ä½œä½¿ç”¨åŒä¸€ä¸ªç¼“å­˜ç›®å½•
```

**ç¬¬308è¡Œè¯¦è§£**ï¼š`_access_counts = defaultdict(int)`

**è®¿é—®è®¡æ•°å™¨è¯¦è§£ï¼ˆç¬¬308è¡Œï¼‰**ï¼š
- `_access_counts`ï¼šç±»å˜é‡ï¼Œç»Ÿè®¡å„æ•°æ®æºçš„è®¿é—®æ¬¡æ•°
- `defaultdict(int)`ï¼šé»˜è®¤å­—å…¸ï¼Œé»˜è®¤å€¼ä¸º0

```python
# è®¿é—®è®¡æ•°çš„åº”ç”¨
_access_counts = defaultdict(int)

# è®°å½•è®¿é—®
_access_counts["Aminer"] += 1  # ç¬¬ä¸€æ¬¡è®¿é—®Aminer
_access_counts["Aminer"] += 1  # ç¬¬äºŒæ¬¡è®¿é—®
_access_counts["CrossRef"] += 1  # è®¿é—®CrossRef

print(_access_counts)
# defaultdict(<class 'int'>, {'Aminer': 2, 'CrossRef': 1})

# æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å°è¯•æ¬¡æ•°
max_attempts = 3
if _access_counts["Aminer"] < max_attempts:
    # è¿˜å¯ä»¥ç»§ç»­è®¿é—®
    try_aminer_api()
else:
    # å·²è¾¾ä¸Šé™ï¼Œè·³è¿‡
    try_next_source()

# ä¸ºä»€ä¹ˆéœ€è¦é™åˆ¶è®¿é—®æ¬¡æ•°ï¼Ÿ
# 1. APIé…é¢ï¼šè®¸å¤šAPIæœ‰è°ƒç”¨æ¬¡æ•°é™åˆ¶
# 2. é¿å…æµªè´¹ï¼šå¦‚æœä¸€ä¸ªæ•°æ®æºæ€»æ˜¯å¤±è´¥ï¼Œä¸è¦ä¸€ç›´å°è¯•
# 3. è´Ÿè½½å‡è¡¡ï¼šè½®æ¢ä½¿ç”¨ä¸åŒçš„æ•°æ®æº
```

**ç¬¬310è¡Œè¯¦è§£**ï¼š`_last_access_time = defaultdict(float)`

**è®¿é—®æ—¶é—´è®°å½•è¯¦è§£ï¼ˆç¬¬310è¡Œï¼‰**ï¼š
- `_last_access_time`ï¼šè®°å½•æ¯ä¸ªæ•°æ®æºçš„æœ€åè®¿é—®æ—¶é—´
- `defaultdict(float)`ï¼šé»˜è®¤å­—å…¸ï¼Œé»˜è®¤å€¼ä¸º0.0ï¼ˆæµ®ç‚¹æ•°ï¼‰

```python
import time

_last_access_time = defaultdict(float)

# è®°å½•è®¿é—®æ—¶é—´
_last_access_time["Aminer"] = time.time()
# å­˜å‚¨å½“å‰æ—¶é—´æˆ³ï¼Œå¦‚ï¼š1699999999.123456

# æ£€æŸ¥è®¿é—®é—´éš”
retry_delay = 5  # ä¸¤æ¬¡è®¿é—®è‡³å°‘é—´éš”5ç§’
current_time = time.time()
last_access = _last_access_time["Aminer"]

if current_time - last_access < retry_delay:
    # è·ç¦»ä¸Šæ¬¡è®¿é—®ä¸è¶³5ç§’ï¼Œéœ€è¦ç­‰å¾…
    wait_time = retry_delay - (current_time - last_access)
    print(f"ç­‰å¾…{wait_time}ç§’...")
    time.sleep(wait_time)

# ç„¶åè¿›è¡Œè®¿é—®
make_request_to_aminer()
_last_access_time["Aminer"] = time.time()  # æ›´æ–°æ—¶é—´

# ä¸ºä»€ä¹ˆéœ€è¦æ§åˆ¶è®¿é—®é—´éš”ï¼Ÿ
# 1. é€Ÿç‡é™åˆ¶ï¼šAPIé€šå¸¸é™åˆ¶æ¯ç§’è¯·æ±‚æ•°
# 2. ç¤¼è²Œï¼šé¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
# 3. é¿å…å°ç¦ï¼šé¢‘ç¹è¯·æ±‚å¯èƒ½è¢«è§†ä¸ºæ”»å‡»
```

**ç¬¬311-313è¡Œè¯¦è§£**ï¼šä»£ç†æˆåŠŸç‡è·Ÿè¸ª

```python
_proxy_success_rate = {}                 # ç¬¬311è¡Œ
_proxy_attempts = defaultdict(int)       # ç¬¬312è¡Œ
_proxy_successes = defaultdict(int)      # ç¬¬313è¡Œ
```

**ä»£ç†ç»Ÿè®¡ç³»ç»Ÿè¯¦è§£**ï¼š
```python
# ä»£ç†ç»Ÿè®¡æ•°æ®ç»“æ„
_proxy_attempts = defaultdict(int)    # ä»£ç†å°è¯•æ¬¡æ•°
_proxy_successes = defaultdict(int)   # ä»£ç†æˆåŠŸæ¬¡æ•°
_proxy_success_rate = {}              # ä»£ç†æˆåŠŸç‡

# ä½¿ç”¨æµç¨‹
proxy = {"http": "http://127.0.0.1:7890"}
proxy_key = str(proxy)

# 1. å°è¯•ä½¿ç”¨ä»£ç†
_proxy_attempts[proxy_key] += 1

# 2. è¯·æ±‚ç»“æœ
if request_successful:
    _proxy_successes[proxy_key] += 1

# 3. è®¡ç®—æˆåŠŸç‡
success_rate = _proxy_successes[proxy_key] / _proxy_attempts[proxy_key]
_proxy_success_rate[proxy_key] = success_rate

# ç¤ºä¾‹æ•°æ®
_proxy_attempts = {
    'None': 10,                      # ç›´æ¥è¿æ¥å°è¯•10æ¬¡
    '{"http": "...7890"}': 5,        # ä»£ç†Aå°è¯•5æ¬¡
    '{"http": "...1080"}': 8         # ä»£ç†Bå°è¯•8æ¬¡
}

_proxy_successes = {
    'None': 8,                       # ç›´æ¥è¿æ¥æˆåŠŸ8æ¬¡
    '{"http": "...7890"}': 4,        # ä»£ç†AæˆåŠŸ4æ¬¡
    '{"http": "...1080"}': 2         # ä»£ç†BæˆåŠŸ2æ¬¡
}

_proxy_success_rate = {
    'None': 0.8,                     # 80%æˆåŠŸç‡
    '{"http": "...7890"}': 0.8,      # 80%æˆåŠŸç‡
    '{"http": "...1080"}': 0.25      # 25%æˆåŠŸç‡ï¼ˆä¸å¥½ï¼‰
}

# æ™ºèƒ½é€‰æ‹©ä»£ç†ï¼šä¼˜å…ˆé€‰æ‹©æˆåŠŸç‡é«˜çš„
best_proxy = max(_proxy_success_rate.items(), key=lambda x: x[1])
# è¿”å›æˆåŠŸç‡æœ€é«˜çš„ä»£ç†
```

**ç¬¬314è¡Œè¯¦è§£**ï¼š`_failed_proxies = set()`

**å¤±è´¥ä»£ç†é›†åˆè¯¦è§£ï¼ˆç¬¬314è¡Œï¼‰**ï¼š
- `_failed_proxies`ï¼šè®°å½•å·²å¤±è´¥çš„ä»£ç†
- `set()`ï¼šé›†åˆæ•°æ®ç»“æ„ï¼Œè‡ªåŠ¨å»é‡

```python
# é›†åˆçš„ç‰¹ç‚¹
_failed_proxies = set()

# æ·»åŠ å¤±è´¥çš„ä»£ç†
_failed_proxies.add('{"http": "...7890"}')
_failed_proxies.add('{"http": "...1080"}')
_failed_proxies.add('{"http": "...7890"}')  # é‡å¤æ·»åŠ ï¼Œè‡ªåŠ¨å¿½ç•¥

print(_failed_proxies)
# {'{"http": "...7890"}', '{"http": "...1080"}'}  # åªæœ‰ä¸¤ä¸ª

# æ£€æŸ¥ä»£ç†æ˜¯å¦å¤±è´¥è¿‡
proxy_str = '{"http": "...7890"}'
if proxy_str in _failed_proxies:
    print("è¿™ä¸ªä»£ç†å¤±è´¥è¿‡ï¼Œä¸ä½¿ç”¨")
else:
    print("è¿™ä¸ªä»£ç†å¯ä»¥å°è¯•")

# set vs list å¯¹æ¯”
# setï¼š
if item in my_set:  # O(1) - å¸¸æ•°æ—¶é—´ï¼Œéå¸¸å¿«
    pass

# listï¼š
if item in my_list:  # O(n) - çº¿æ€§æ—¶é—´ï¼Œåˆ—è¡¨è¶Šé•¿è¶Šæ…¢
    pass

# ä¸ºä»€ä¹ˆä½¿ç”¨setï¼Ÿ
# 1. å¿«é€ŸæŸ¥æ‰¾ï¼šæ£€æŸ¥ä»£ç†æ˜¯å¦å¤±è´¥è¿‡ï¼ŒO(1)æ—¶é—´
# 2. è‡ªåŠ¨å»é‡ï¼šä¸ä¼šé‡å¤è®°å½•åŒä¸€ä¸ªä»£ç†
# 3. å†…å­˜æ•ˆç‡ï¼šåªå­˜å‚¨å”¯ä¸€å€¼
```

### runæ ¸å¿ƒæ–¹æ³•ï¼ˆç¬¬432-516è¡Œï¼‰

```python
@classmethod                                                    # ç¬¬432è¡Œ
def run(cls, url: str, title: str = "", info: dict = None) -> Optional[str]:  # ç¬¬433è¡Œ
    """è·å–æ‘˜è¦ä¿¡æ¯"""                                             # ç¬¬434è¡Œ
    # é¦–å…ˆæ£€æŸ¥ç¼“å­˜                                                 # ç¬¬435è¡Œ
    cached_abstract = cls._cache.get(url, title)               # ç¬¬436è¡Œ
    if cached_abstract:                                        # ç¬¬437è¡Œ
        return cached_abstract                                 # ç¬¬438è¡Œ
```

**@classmethodè£…é¥°å™¨è¯¦è§£**ï¼š
```python
# @classmethodæ˜¯ä»€ä¹ˆï¼Ÿ
# ç±»æ–¹æ³•è£…é¥°å™¨ï¼Œä½¿æ–¹æ³•æˆä¸ºç±»æ–¹æ³•è€Œä¸æ˜¯å®ä¾‹æ–¹æ³•

# å®ä¾‹æ–¹æ³•vsç±»æ–¹æ³•
class MyClass:
    # å®ä¾‹æ–¹æ³•ï¼ˆéœ€è¦åˆ›å»ºå¯¹è±¡ï¼‰
    def instance_method(self):
        print(f"å®ä¾‹æ–¹æ³•ï¼Œself={self}")
    
    # ç±»æ–¹æ³•ï¼ˆä¸éœ€è¦åˆ›å»ºå¯¹è±¡ï¼‰
    @classmethod
    def class_method(cls):
        print(f"ç±»æ–¹æ³•ï¼Œcls={cls}")

# è°ƒç”¨å®ä¾‹æ–¹æ³•
obj = MyClass()
obj.instance_method()  # å¿…é¡»å…ˆåˆ›å»ºå¯¹è±¡

# è°ƒç”¨ç±»æ–¹æ³•
MyClass.class_method()  # ç›´æ¥ç”¨ç±»åè°ƒç”¨ï¼Œæ— éœ€åˆ›å»ºå¯¹è±¡

# cls vs self
# self: æŒ‡å‘å®ä¾‹å¯¹è±¡
# cls: æŒ‡å‘ç±»æœ¬èº«

# æœ¬é¡¹ç›®ä¸ºä»€ä¹ˆç”¨@classmethodï¼Ÿ
# å› ä¸ºAnalyserä¸éœ€è¦åˆ›å»ºå®ä¾‹
# ç›´æ¥Analyser.run(url, title)å³å¯
abstract = Analyser.run("https://...", "Title")

# å¦‚æœç”¨å®ä¾‹æ–¹æ³•
analyser = Analyser()  # éœ€è¦å…ˆåˆ›å»ºå¯¹è±¡
abstract = analyser.run("https://...", "Title")
```

**å®Œæ•´çš„æ‘˜è¦è·å–æµç¨‹**ï¼š
```python
def run(cls, url, title="", info=None):
    """
    å¤šå±‚æ¬¡æ‘˜è¦è·å–ç­–ç•¥ï¼š
    1. ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
    2. åŸå§‹URLï¼ˆæ¬¡å¿«ï¼‰
    3. DOIï¼ˆå¤‡é€‰ï¼‰
    4. å¤‡é€‰APIï¼ˆå¤šä¸ªï¼‰
    5. é€šç”¨æå–ï¼ˆå…œåº•ï¼‰
    """
    
    # ç­–ç•¥1: ç¼“å­˜ï¼ˆç¬¬436-438è¡Œï¼‰
    cached = cls._cache.get(url, title)
    if cached:
        return cached  # ç«‹å³è¿”å›ï¼Œæœ€å¿«
    
    # ç­–ç•¥2: åŸå§‹URLï¼ˆç¬¬448-453è¡Œï¼‰
    try:
        abstract = cls._try_source_url(url, title)
        if abstract:
            cls._cache.set(url, title, abstract)  # ç¼“å­˜ç»“æœ
            return abstract
    except:
        pass
    
    # ç­–ç•¥3: DOIï¼ˆç¬¬458-463è¡Œï¼‰
    if info and "doi" in info:
        abstract = cls._try_doi(info["doi"], title)
        if abstract:
            cls._cache.set(url, title, abstract)
            return abstract
    
    # ç­–ç•¥4: å¤‡é€‰APIï¼ˆç¬¬468-503è¡Œï¼‰
    for attempt in range(3):  # æœ€å¤š3è½®
        for source in ALTERNATE_SOURCES:
            abstract = cls._try_alternate_source(title, source, info)
            if abstract:
                cls._cache.set(url, title, abstract)
                return abstract
    
    # ç­–ç•¥5: é€šç”¨æå–ï¼ˆç¬¬506-511è¡Œï¼‰
    abstract = cls._extract_generic_content(url, title, info)
    if abstract:
        cls._cache.set(url, title, abstract)
        return abstract
    
    # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
    return None

# ä¸ºä»€ä¹ˆæœ‰è¿™ä¹ˆå¤šç­–ç•¥ï¼Ÿ
# 1. æé«˜æˆåŠŸç‡ï¼šä¸€ä¸ªæ–¹æ³•å¤±è´¥ï¼Œè¿˜æœ‰å…¶ä»–æ–¹æ³•
# 2. é€‚åº”ä¸åŒç½‘ç«™ï¼šä¸åŒç½‘ç«™éœ€è¦ä¸åŒçš„æå–æ–¹æ³•
# 3. é™çº§ç­–ç•¥ï¼šä»æœ€å‡†ç¡®åˆ°æœ€é€šç”¨
# 4. å®¹é”™æ€§ï¼šç½‘ç»œé—®é¢˜ä¸ä¼šå¯¼è‡´å®Œå…¨å¤±è´¥
```

---

## ğŸ¯ æŠ€æœ¯æ€»ç»“

### Analyser.pyçš„æ ¸å¿ƒæŠ€æœ¯æ ˆ

| æŠ€æœ¯ç±»åˆ« | å…·ä½“æŠ€æœ¯ | åº”ç”¨åœºæ™¯ |
|---------|---------|---------|
| **ç½‘ç»œè¯·æ±‚** | requests, HTTPAdapter, Retry | HTTPè¯·æ±‚å’Œé‡è¯•æœºåˆ¶ |
| **HTMLè§£æ** | BeautifulSoup, lxml | ç½‘é¡µå†…å®¹æå– |
| **ç¼“å­˜ç³»ç»Ÿ** | pickle, hashlib, Path | æœ¬åœ°æ–‡ä»¶ç¼“å­˜ |
| **æ—¶é—´ç®¡ç†** | datetime, timedelta, time | ç¼“å­˜è¿‡æœŸå’Œè®¿é—®æ§åˆ¶ |
| **æ­£åˆ™è¡¨è¾¾å¼** | re | æ–‡æœ¬åŒ¹é…å’Œæå– |
| **ç±»å‹æç¤º** | typing | ä»£ç å¯è¯»æ€§å’ŒIDEæ”¯æŒ |
| **æ•°æ®ç»“æ„** | defaultdict, set | é«˜æ•ˆçš„æ•°æ®ç»Ÿè®¡ |
| **å¼‚å¸¸å¤„ç†** | try-except | å®¹é”™å’Œç¨³å®šæ€§ |

### è®¾è®¡æ¨¡å¼åº”ç”¨

1. **ç­–ç•¥æ¨¡å¼**: ä¸åŒç½‘ç«™ä½¿ç”¨ä¸åŒçš„è§£æè§„åˆ™
2. **å•ä¾‹æ¨¡å¼**: å…¨å±€å…±äº«çš„ç¼“å­˜ç³»ç»Ÿ
3. **å·¥å‚æ¨¡å¼**: åŠ¨æ€åˆ›å»ºä¼šè¯å’Œä»£ç†
4. **è£…é¥°å™¨æ¨¡å¼**: @classmethodä¿®é¥°æ–¹æ³•

### æ ¸å¿ƒè®¾è®¡æ€æƒ³

1. **å¤šå±‚æ¬¡é™çº§**: ç¼“å­˜â†’åŸå§‹URLâ†’DOIâ†’å¤‡é€‰APIâ†’é€šç”¨æå–
2. **æ™ºèƒ½é‡è¯•**: æŒ‡æ•°é€€é¿ã€çŠ¶æ€ç åˆ¤æ–­ã€ä»£ç†è½®æ¢
3. **å…¨å±€é™æµ**: è®¿é—®æ¬¡æ•°é™åˆ¶ã€æ—¶é—´é—´éš”æ§åˆ¶
4. **å®¹é”™è®¾è®¡**: å¼‚å¸¸æ•è·ã€é»˜è®¤å€¼ã€graceful degradation

### æ€§èƒ½ä¼˜åŒ–æŠ€å·§

1. **ç¼“å­˜ä¼˜å…ˆ**: é¿å…é‡å¤ç½‘ç»œè¯·æ±‚
2. **è¿æ¥å¤ç”¨**: keep-aliveä¿æŒè¿æ¥
3. **å¹¶å‘æ§åˆ¶**: é¿å…è¿‡å¿«è¯·æ±‚å¯¼è‡´å°ç¦
4. **ä»£ç†æ™ºèƒ½é€‰æ‹©**: æ ¹æ®æˆåŠŸç‡é€‰æ‹©æœ€ä½³ä»£ç†

### å®‰å…¨æ€§è€ƒè™‘

1. **User-Agentä¼ªè£…**: æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
2. **è¯·æ±‚å¤´è½®æ¢**: é¿å…ç‰¹å¾è¯†åˆ«
3. **è®¿é—®é—´éš”**: é¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
4. **å¼‚å¸¸éš”ç¦»**: ä¸€ä¸ªå¤±è´¥ä¸å½±å“æ•´ä½“

è¿™ä¸ªç³»ç»Ÿå±•ç¤ºäº†ä¸€ä¸ª**ç”Ÿäº§çº§çˆ¬è™«ç³»ç»Ÿ**åº”è¯¥å…·å¤‡çš„æ‰€æœ‰è¦ç´ ï¼

